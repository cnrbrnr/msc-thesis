@article{Badel_Lefort_Brette_Petersen_Gerstner_Richardson_2008, title={Dynamic I-V curves are reliable predictors of naturalistic pyramidal-neuron voltage traces}, volume={99}, ISSN={0022-3077}, DOI={10.1152/jn.01107.2007}, abstractNote={Neuronal response properties are typically probed by intracellular measurements of current-voltage (I-V) relationships during application of current or voltage steps. Here we demonstrate the measurement of a novel I-V curve measured while the neuron exhibits a fluctuating voltage and emits spikes. This dynamic I-V curve requires only a few tens of seconds of experimental time and so lends itself readily to the rapid classification of cell type, quantification of heterogeneities in cell populations, and generation of reduced analytical models. We apply this technique to layer-5 pyramidal cells and show that their dynamic I-V curve comprises linear and exponential components, providing experimental evidence for a recently proposed theoretical model. The approach also allows us to determine the change of neuronal response properties after a spike, millisecond by millisecond, so that postspike refractoriness of pyramidal cells can be quantified. Observations of I-V curves during and in absence of refractoriness are cast into a model that is used to predict both the subthreshold response and spiking activity of the neuron to novel stimuli. The predictions of the resulting model are in excellent agreement with experimental data and close to the intrinsic neuronal reproducibility to repeated stimuli.}, number={2}, journal={Journal of Neurophysiology}, author={Badel, Laurent and Lefort, Sandrine and Brette, Romain and Petersen, Carl C. H. and Gerstner, Wulfram and Richardson, Magnus J. E.}, year={2008}, month=feb, pages={656–666}, language={eng} }
@article{Bauerle_2023, title={Mean Field Markov Decision Processes}, volume={88}, ISSN={1432-0606}, DOI={10.1007/s00245-023-09985-1}, abstractNote={We consider mean-field control problems in discrete time with discounted reward, infinite time horizon and compact state and action space. The existence of optimal policies is shown and the limiting mean-field problem is derived when the number of individuals tends to infinity. Moreover, we consider the average reward problem and show that the optimal policy in this mean-field limit is $$varepsilon $$-optimal for the discounted problem if the number of individuals is large and the discount factor close to one. This result is very helpful, because it turns out that in the special case when the reward does only depend on the distribution of the individuals, we obtain a very interesting subclass of problems where an average reward optimal policy can be obtained by first computing an optimal measure from a static optimization problem and then achieving it with Markov Chain Monte Carlo methods. We give two applications: Avoiding congestion an a graph and optimal positioning on a market place which we solve explicitly.}, number={1}, journal={Applied Mathematics \& Optimization}, author={Bäuerle, Nicole}, year={2023}, month=apr, pages={12}, language={en} }
@article{Barty_Chancelier_Cohen_Lara_Guilbaud_Carpentier_2006, title={Dual effect free stochastic controls}, volume={142}, ISSN={1572-9338}, DOI={10.1007/s10479-006-6160-4}, abstractNote={In stochastic optimal control, a key issue is the fact that “solutions” are searched for in terms of “closed-loop control laws” over available information and, as a consequence, a major potential difficulty is the fact that present control may affect future available information. This is known as the “dual effect” of control. Our main result consists in characterizing the maximal set of closed-loop control laws containing open-loop ones and for which the information provided by observations closed with such a feedback remains fixed. We give more specific results in the two following cases: multi-agent systems and discrete time stochastic input-output systems with dynamic information structure.}, number={1}, journal={Annals of Operations Research}, author={Barty, K. and Chancelier, J.-P. and Cohen, G. and Lara, M. De and Guilbaud, T. and Carpentier, P.}, year={2006}, month=feb, pages={41–62}, language={en} }
@article{Blair_Erlanger_1933, title={A COMPARISON OF THE CHARACTERISTICS OF AXONS THROUGH THEIR INDIVIDUAL ELECTRICAL RESPONSES}, volume={106}, ISSN={0002-9513}, DOI={10.1152/ajplegacy.1933.106.3.524}, number={3}, journal={American Journal of Physiology-Legacy Content}, author={Blair, E. A. and Erlanger, Joseph}, year={1933}, month=nov, pages={524–564}, language={en} }
@article{Borkar_1993, title={White-noise representations in stochastic realization theory}, volume={31}, ISSN={0363-0129}, DOI={10.1137/0331050}, number={5}, journal={SIAM J. Control Optim.}, author={Borkar, Vivek S.}, year={1993}, month=sep, pages={1093–1102} }
@article{Borovkov_Decrouez_Gilson_2014, title={On Stationary Distributions of Stochastic Neural Networks}, volume={51}, ISSN={0021-9002}, abstractNote={The paper deals with nonlinear Poisson neuron network models with bounded memory dynamics, which can include both Hebbian learning mechanisms and refractory periods. The state of the network is described by the times elapsed since its neurons fired within the post-synaptic transfer kernel memory span, and the current strengths of synaptic connections, the state spaces of our models being hierarchies of finitedimensional components. We prove the ergodicity of the stochastic processes describing the behaviour of the networks, establish the existence of continuously differentiable stationary distribution densities (with respect to the Lebesgue measures of corresponding dimensionality) on the components of the state space, and find upper bounds for them. For the density components, we derive a system of differential equations that can be solved in a few simplest cases only. Approaches to approximate computation of the stationary density are discussed. One approach is to reduce the dimensionality of the problem by modifying the network so that each neuron cannot fire if the number of spikes it emitted within the post-synaptic transfer kernel memory span reaches a given threshold. We show that the stationary distribution of this “truncated” network converges to that of the unrestricted network as the threshold increases, and that the convergence is at a superexponential rate. A complementary approach uses discrete Markov chain approximations to the network process.}, number={3}, journal={Journal of Applied Probability}, publisher={Applied Probability Trust}, author={Borovkov, K. and Decrouez, G. and Gilson, M.}, year={2014}, pages={837–857} }
@article{Burghi_Sepulchre_2023, title={Adaptive observers for biophysical neuronal circuits}, ISSN={0018-9286, 1558-2523, 2334-3303}, DOI={10.1109/TAC.2023.3344723}, abstractNote={This paper presents an adaptive observer for online state and parameter estimation of a broad class of biophysical models of neuronal networks. The design closely resembles classical solutions of adaptive control, and the convergence proof is based on contraction analysis. Our results include robustness guarantees with respect to unknown parameter dynamics. We discuss the potential of the approach in neurophysiological applications.}, note={arXiv:2111.02176 [cs, eess, q-bio]}, journal={IEEE Transactions on Automatic Control}, author={Burghi, Thiago B. and Sepulchre, Rodolphe}, year={2023}, pages={1–14}, language={en} }
@article{Bremaud_Massoulié_1996, title={Stability of nonlinear Hawkes processes}, volume={24}, ISSN={0091-1798}, url={https://projecteuclid.org/journals/annals-of-probability/volume-24/issue-3/Stability-of-nonlinear-Hawkes-processes/10.1214/aop/1065725193.full}, DOI={10.1214/aop/1065725193}, abstractNote={We address the problem of the convergence to equilibrium of a general class of point processes, containing, in particular, the nonlinear mutually exciting point processes, an extension of the linear Hawkes processes, and give general conditions guaranteeing the existence of a stationary version and the convergence to equilibrium of a nonstationary version, both in distribution and in variation. We also give a new proof of a result of Kerstan concerning point processes with bounded intensity and general nonlinear dynamics satisfying a Lipschitz condition.}, number={3}, journal={The Annals of Probability}, author={Brémaud, Pierre and Massoulié, Laurent}, year={1996}, month=jul }
@article{Brette_2015, title={Philosophy of the Spike: Rate-Based vs. Spike-Based Theories of the Brain}, volume={9}, ISSN={1662-5137}, url={https://www.frontiersin.org/journals/systems-neuroscience/articles/10.3389/fnsys.2015.00151/full}, DOI={10.3389/fnsys.2015.00151}, abstractNote={<p>Does the brain use a firing rate code or a spike timing code? Considering this controversial question from an epistemological perspective, I argue that progress has been hampered by its problematic phrasing. It takes the perspective of an external observer looking at whether those two observables vary with stimuli, and thereby misses the relevant question: which one has a causal role in neural activity? When rephrased in a more meaningful way, the rate-based view appears as an <italic>ad hoc</italic> methodological postulate, one that is practical but with virtually no empirical or theoretical support.</p>}, journal={Frontiers in Systems Neuroscience}, publisher={Frontiers}, author={Brette, Romain}, year={2015}, month=nov, language={English} }
@article{Brillinger_1988, title={Maximum likelihood analysis of spike trains of interacting nerve cells}, volume={59}, ISSN={1432-0770}, DOI={10.1007/BF00318010}, abstractNote={Suppose that a neuron is firing spontaneously or that it is firing under the influence of other neurons. Suppose that the data available are the firing times of the neurons present. An “integrate several inputs and fire” model is developed and studied empirically. For the model a neuron’s firing occurs when an internal state variable crosses a random threshold. This conceptual model leads to maximum likelihood estimates of internal quantities, such as the postsynaptic potentials of the measured influencing neurons, the membrane potential, the absolute threshold and also estimates of derived quantities such as the strength-duration curve and the recovery process of the threshold. The model’s validity is examined via an estimate of the conditional firing probability. The approach appears useful for estimating biologically meaningful parameters, for examining hypotheses re these parameters, for understanding the connections present in neural networks and for aiding description and classification of neurons and synapses. Analyses are presented for a number of data sets collected for the sea hare,Aplysia californica, by J. P. Segundo. Both excitatory and inhibitory examples are provided. The computations were carried out via the Glim statistical package. An example of a Glim program realizing the work is presented in the Appendix.}, number={3}, journal={Biological Cybernetics}, author={Brillinger, D. R.}, year={1988}, month=aug, pages={189–200}, language={en} }
@article{Brunel_van_Rossum_2007, title={Quantitative investigations of electrical nerve excitation treated as polarization}, volume={97}, ISSN={1432-0770}, DOI={10.1007/s00422-007-0189-6}, number={5}, journal={Biological Cybernetics}, author={Brunel, Nicolas and van Rossum, Mark C. W.}, year={2007}, month=dec, pages={341–349}, language={en} }
@article{Bruti-Liberati_Nikitopoulos-Sklibosios_Platen_2006, title={First Order Strong Approximations of Jump Diffusions}, volume={12}, ISSN={0929-9629, 1569-3961}, url={https://www.degruyter.com/document/doi/10.1515/156939606778705191/html}, DOI={10.1515/156939606778705191}, number={3}, journal={Monte Carlo Methods and Applications}, author={Bruti-Liberati, Nicola and Nikitopoulos-Sklibosios, Christina and Platen, Eckhard}, year={2006}, month=jan }
@article{Bruti-Liberati_Platen_2005, series={Research Paper Series}, title={On the Strong Approximation of Jump-Diffusion Processes}, url={https://ideas.repec.org//p/uts/rpaper/157.html}, abstractNote={In financial modelling, filtering and other areas the underlying dynamics are often specified via stochastic differential equations (SDEs) of jump-diffusion type. The class of jump-diffusion SDEs that admits explicit solutions is rather limited. Consequently, there is a need for the systematic use of discrete time approximations in corresponding simulations. This paper presents a survey and new results on strong numerical schemes for SDEs of jump-diffusion type. These are relevant for scenario analysis, filtering and hedge simulation in finance. It provides a convergence theorem for the construction of strong approximations of any given order of convergence for SDEs driven by Wiener processes and Poisson random measures. The paper covers also derivative free, drift-implicit and jump adapted strong approximations. For the commutative case particular schemes are obtained. Finally, a numerical study on the accuracy of several strong schemes is presented.}, journal={Research Paper Series}, publisher={Quantitative Finance Research Centre, University of Technology, Sydney}, author={Bruti-Liberati, Nicola and Platen, Eckhard}, year={2005}, collection={Research Paper Series}, language={en} }
@article{Buesing_Bill_Nessler_Maass_2011, title={Neural Dynamics as Sampling: A Model for Stochastic Computation in Recurrent Networks of Spiking Neurons}, volume={7}, ISSN={1553-7358}, DOI={10.1371/journal.pcbi.1002211}, abstractNote={The organization of computations in networks of spiking neurons in the brain is still largely unknown, in particular in view of the inherently stochastic features of their firing activity and the experimentally observed trial-to-trial variability of neural systems in the brain. In principle there exists a powerful computational framework for stochastic computations, probabilistic inference by sampling, which can explain a large number of macroscopic experimental data in neuroscience and cognitive science. But it has turned out to be surprisingly difficult to create a link between these abstract models for stochastic computations and more detailed models of the dynamics of networks of spiking neurons. Here we create such a link and show that under some conditions the stochastic firing activity of networks of spiking neurons can be interpreted as probabilistic inference via Markov chain Monte Carlo (MCMC) sampling. Since common methods for MCMC sampling in distributed systems, such as Gibbs sampling, are inconsistent with the dynamics of spiking neurons, we introduce a different approach based on non-reversible Markov chains that is able to reflect inherent temporal processes of spiking neuronal activity through a suitable choice of random variables. We propose a neural network model and show by a rigorous theoretical analysis that its neural activity implements MCMC sampling of a given distribution, both for the case of discrete and continuous time. This provides a step towards closing the gap between abstract functional models of cortical computation and more detailed models of networks of spiking neurons.}, number={11}, journal={PLoS Computational Biology}, author={Buesing, Lars and Bill, Johannes and Nessler, Bernhard and Maass, Wolfgang}, editor={Sporns, Olaf}, year={2011}, month=nov, pages={e1002211}, language={en} }
@article{Buonocore_Caputo_D’Onofrio_Pirozzi_2015, title={Closed-form solutions for the first-passage-time problem and neuronal modeling}, volume={64}, ISSN={1827-3491}, DOI={10.1007/s11587-015-0248-6}, abstractNote={The Gauss–Diffusion processes are here considered and some relations between their infinitesimal moments and mean and covariance functions are remarked. The corresponding linear stochastic differential equations are re-written specifying the coefficient functions and highlighting their meanings in theoretical and application contexts. We resort the Doob-transformation of a Gauss–Markov process as a transformed Wiener process and we represent some time-inhomogeneous processes as transformed Ornstein–Uhlenbeck process. The first passage time problem is considered in order to discuss some neuronal models based on Gauss–Diffusion processes. We recall some different approaches to solve the first passage time problem specifying when a closed-form result exists and numerical evaluations are required when the latter is not available. In the contest of neuronal modeling, relations between firing threshold, mean behavior of the neuronal membrane voltage and input currents are given for the existence of a closed-form result useful to describe the firing activity. Finally, we collect in an unified way some models and the corresponding Gauss–Diffusion processes already considered by us in some previous papers.}, number={2}, journal={Ricerche di Matematica}, author={Buonocore, Aniello and Caputo, Luigia and D’Onofrio, Giuseppe and Pirozzi, Enrica}, year={2015}, month=nov, pages={421–439}, language={en} }
@article{Buzsáki_Anastassiou_Koch_2012, title={The origin of extracellular fields and currents — EEG, ECoG, LFP and spikes}, volume={13}, rights={2012 Springer Nature Limited}, ISSN={1471-0048}, DOI={10.1038/nrn3241}, abstractNote={All currents in the brain superimpose to yield an “electric field” at any given point in space. The current sources and sinks form dipoles or higher-order n-poles.Extracellular currents arise from many sources, including synaptic currents, fast action potentials and their afterpotentials, calcium spikes and voltage-dependent intrinsic currents.The magnitude of extracellular currents depends critically on two factors: the cytoarchitectural organization of a network and the temporal synchrony of the various current sinks and sources.Depending on the recording method, neuroscientists distinguish between electroencephalogram (EEG), electrocorticogram (ECoG) and local field potential (LFP; also known as micro-, depth or intracranial EEG), although all of these measures refer to the same biophysical process.The electric field is the force “felt” by an electric charge, and can be transmitted through brain volume. The extent of volume conduction depends on the relationships between the current dipole and the features of the conductive medium.High-density sampling of the extracellular field with contemporary methods enables the calculation of current source density, and therefore the localization of current sinks and sources.The voltage gradients generated by highly synchronous activity of neuronal groups can affect the transmembrane potential of the member neurons and alter their excitability through ephaptic coupling.Synchronous spiking of nearby neurons is the main source of the high-frequency components of the local field.There is a discernable relationship between the temporal evolution of cell assemblies and the time-dependent changes of the spatially distributed currents. High-density, wide-band recordings of the local field can therefore provide access to both afferent inputs and the spiking output of neurons.}, number={6}, journal={Nature Reviews Neuroscience}, publisher={Nature Publishing Group}, author={Buzsáki, György and Anastassiou, Costas A. and Koch, Christof}, year={2012}, month=jun, pages={407–420}, language={en} }
@article{Cessac_2011, title={Statistics of spike trains in conductance-based neural networks: Rigorous results}, volume={1}, ISSN={2190-8567}, DOI={10.1186/2190-8567-1-8}, number={1}, journal={The Journal of Mathematical Neuroscience}, author={Cessac, Bruno}, year={2011}, pages={8}, language={en} }
@article{Cessac_Viéville_2008, title={On dynamics of integrate-and-fire neural networks with conductance based synapses}, volume={2}, ISSN={1662-5188}, url={https://www.frontiersin.org/journals/computational-neuroscience/articles/10.3389/neuro.10.002.2008/full}, DOI={10.3389/neuro.10.002.2008}, abstractNote={<p>We present a mathematical analysis of networks with integrate-and-fire (<abbrev>IF</abbrev>) neurons with conductance based synapses. Taking into account the realistic fact that the spike time is only known within some <italic>finite</italic> precision, we propose a model where spikes are effective at times multiple of a characteristic time scale δ, where δ can be <italic>arbitrary</italic> small (in particular, well beyond the numerical precision). We make a complete mathematical characterization of the model-dynamics and obtain the following results. The asymptotic dynamics is composed by finitely many stable periodic orbits, whose number and period can be arbitrary large and can diverge in a region of the synaptic weights space, traditionally called the “edge of chaos”, a notion mathematically well defined in the present paper. Furthermore, except at the edge of chaos, there is a one-to-one correspondence between the membrane potential trajectories and the raster plot. This shows that the neural code is entirely “in the spikes” in this case. As a key tool, we introduce an order parameter, easy to compute numerically, and closely related to a natural notion of entropy, providing a relevant characterization of the computational capabilities of the network. This allows us to compare the computational capabilities of leaky and IF models and conductance based models. The present study considers networks with constant input, and without time-dependent plasticity, but the framework has been designed for both extensions.</p>}, journal={Frontiers in Computational Neuroscience}, publisher={Frontiers}, author={Cessac, Bruno and Viéville, Thierry}, year={2008}, month=jul, language={English} }
@article{Chevallier_2017, title={Mean-field limit of generalized Hawkes processes}, volume={127}, ISSN={0304-4149}, DOI={10.1016/j.spa.2017.02.012}, abstractNote={We generalize multivariate Hawkes processes mainly by including a dependence with respect to the age of the process, i.e. the delay since the last point. Within this class, we investigate the limit behaviour, when n goes to infinity, of a system of n mean-field interacting age-dependent Hawkes processes. We prove that such a system can be approximated by independent and identically distributed age dependent point processes interacting with their own mean intensity. This result generalizes the study performed by Delattre et al. (2016). In continuity with Chevallier et al. (2015), the second goal of this paper is to give a proper link between these generalized Hawkes processes as microscopic models of individual neurons and the age-structured system of partial differential equations introduced by Pakdaman et al. (2010) as macroscopic model of neurons.}, number={12}, journal={Stochastic Processes and their Applications}, author={Chevallier, Julien}, year={2017}, month=dec, pages={3870–3912} }
@article{Chevallier_Caceres_Doumic_Reynaud_Bouret_2015, journal={pre-print}, title={Microscopic approach of a time elapsed neural model}, url={http://arxiv.org/abs/1506.02361}, note={arXiv:1506.02361}, publisher={arXiv}, author={Chevallier, Julien and Caceres, Maria J. and Doumic, Marie and Reynaud-Bouret, Patricia}, year={2015}, month=jun, language={en} }
@article{Chornoboy_Schramm_Karr_1988, title={Maximum likelihood identification of neural point process systems}, volume={59}, ISSN={0340-1200}, DOI={10.1007/BF00332915}, abstractNote={Using the theory of random point processes, a method is presented whereby functional relationships between neurons can be detected and modeled. The method is based on a point process characterization involving stochastic intensities and an additive rate function model. Estimates are based on the maximum likelihood (ML) principle and asymptotic properties are examined in the absence of a stationarity assumption. An iterative algorithm that computes the ML estimates is presented. It is based on the expectation/maximization (EM) procedure of Dempster et al. (1977) and makes ML identification accessible to models requiring many parameters. Examples illustrating the use of the method are also presented. These examples are derived from simulations of simple neural systems that cannot be identified using correlation techniques. It is shown that the ML method correctly identifies each of these systems.}, number={4–5}, journal={Biological Cybernetics}, author={Chornoboy, E. S. and Schramm, L. P. and Karr, A. F.}, year={1988}, pages={265–275}, language={eng} }
@article{Daly_Gavaghan_Holmes_Cooper_2015, title={Hodgkin–Huxley revisited: reparametrization and identifiability analysis of the classic action potential model with approximate Bayesian methods}, volume={2}, ISSN={2054-5703}, DOI={10.1098/rsos.150499}, abstractNote={As cardiac cell models become increasingly complex, a correspondingly complex ‘genealogy’ of inherited parameter values has also emerged. The result has been the loss of a direct link between model parameters and experimental data, limiting both reproducibility and the ability to re-fit to new data. We examine the ability of approximate Bayesian computation (ABC) to infer parameter distributions in the seminal action potential model of Hodgkin and Huxley, for which an immediate and documented connection to experimental results exists. The ability of ABC to produce tight posteriors around the reported values for the gating rates of sodium and potassium ion channels validates the precision of this early work, while the highly variable posteriors around certain voltage dependency parameters suggests that voltage clamp experiments alone are insufficient to constrain the full model. Despite this, Hodgkin and Huxley’s estimates are shown to be competitive with those produced by ABC, and the variable behaviour of posterior parametrized models under complex voltage protocols suggests that with additional data the model could be fully constrained. This work will provide the starting point for a full identifiability analysis of commonly used cardiac models, as well as a template for informative, data-driven parametrization of newly proposed models.}, number={12}, journal={Royal Society Open Science}, author={Daly, Aidan C. and Gavaghan, David J. and Holmes, Chris and Cooper, Jonathan}, year={2015}, month=dec, pages={150499} }
@article{Daur_Nadim_Bucher_2016, title={The complexity of small circuits: the stomatogastric nervous system}, volume={41}, ISSN={0959-4388}, DOI={10.1016/j.conb.2016.07.005}, abstractNote={The crustacean stomatogastric nervous system is a long-standing test bed for studies of circuit dynamics and neuromodulation. We give a brief update on the most recent work on this system, with an emphasis on the broader implications for understanding neural circuits. In particular, we focus on new findings underlining that different levels of dynamics taking place at different time scales all interact in multiple ways. Dynamics due to synaptic and intrinsic neuronal properties, neuromodulation, and long-term gene expression-dependent regulation are not independent, but influence each other. Extensive research on the stomatogastric system shows that these dynamic interactions convey robustness to circuit operation, while facilitating the flexibility of producing multiple circuit outputs.}, journal={Current opinion in neurobiology}, author={Daur, Nelly and Nadim, Farzan and Bucher, Dirk}, year={2016}, month=dec, pages={1–7} }
@article{Denève_Machens_2016, title={Efficient codes and balanced networks}, volume={19}, ISSN={1097-6256, 1546-1726}, DOI={10.1038/nn.4243}, abstractNote={Recent years have seen a growing interest in inhibitory interneurons and their circuits. A striking property of cortical inhibition is how tightly it balances excitation. Inhibitory currents not only match excitatory currents on average, but track them on a millisecond time scale, whether they are caused by external stimuli or spontaneous fluctuations. We review, together with experimental evidence, recent theoretical approaches that investigate the advantages of such tight balance for coding and computation. These studies suggest a possible revision of the dominant view that neurons represent information with firing rates corrupted by Poisson noise. Instead, tight excitatory/inhibitory balance may be a signature of a highly cooperative code, orders of magnitude more precise than a Poisson rate code. Moreover, tight balance may provide a template that allows cortical neurons to construct high-dimensional population codes and learn complex functions of their inputs.}, number={3}, journal={Nature Neuroscience}, author={Denève, Sophie and Machens, Christian K}, year={2016}, month=mar, pages={375–382}, language={en} }
@article{Destexhe_Mainen_Sejnowski_1994, title={Synthesis of models for excitable membranes, synaptic transmission and neuromodulation using a common kinetic formalism}, volume={1}, rights={http://www.springer.com/tdm}, ISSN={0929-5313, 1573-6873}, DOI={10.1007/BF00961734}, abstractNote={Markov kinetic models were used to synthesize a complete description of synaptic transmission, including opening of voltage-dependent channels in the presynaptic terminal, release of neurotransmitter, gating of postsynaptic receptors, and activation of second-messenger systems. These kinetic schemes provide a more general framework for modeling ion channels than the Hodgkin-Huxley formalism, supporting a continuous spectrum of descriptions ranging from the very simple and computationally efficient to the highly complex and biophysically precise. Examples are given of simple kinetic schemes based on fits to experimental data that capture the essential properties of voltage-gated, synaptic and neuromodulatory currents. The Markov formalism allows the dynamics of ionic currents to be considered naturally in the larger context of biochemical signal transduction. This frame,work can facilitate the integration of a wide range of experimental data and promote consistent theoretical analysis of neural mechanisms from molecular interactions to network computations.}, number={3}, journal={Journal of Computational Neuroscience}, author={Destexhe, Alain and Mainen, Zachary F. and Sejnowski, Terrence J.}, year={1994}, month=aug, pages={195–230}, language={en} }
@article{Faisal_Selen_Wolpert_2008, title={Noise in the nervous system}, volume={9}, rights={2008 Springer Nature Limited}, ISSN={1471-0048}, DOI={10.1038/nrn2258}, abstractNote={Trial-to-trial variability can result from both deterministic sources, such as complex dynamics or internal states, and randomness — that is, noise. This Review focuses on noise and its impact along the behavioural loop.Sensory noise is noise in sensory signals and sensory receptors. It limits the amount of information that is available to other areas of the CNS.Cellular noise is an underestimated contributor to neuronal variability. The stochastic nature of neuronal mechanisms becomes critical in the many small structures of the CNS.Electrical noise in neurons, especially channel noise from voltage-gated ion channels, limits neuronal reliability and cell size, producing millisecond variability in action-potential initiation and propagation.Synaptic noise results from the noisy biochemical processes that underlie synaptic transmission. Adding up these noise sources can account for the observed postsynaptic-response variability.Noise build-up in neural networks can be contained by appropriate network layouts, homeostatic mechanisms and the threshold-like nature of neurons.Motor noise results when neural signals are converted into forces. The architecture of motor neurons and their muscle fibres makes the conversion noisy. The brain organizes movements to minimize the effects of motor noise on movement variability.Beneficial effects of noise include stochastic resonance in specific cases of sensory processing and forcing neural networks to be more robust and explore more states.Behavioural variability, as observed in sensory estimation and movement tasks, appears to be mainly produced by noise.The principle of averaging is one of two fundamental principles applied by the CNS to compensate for noise by summing over sources of redundant information.The principle of prior knowledge is the other fundamental principle: it exploits the expected nature of signals and noise. The CNS often applies it in combination with averaging, such as in Bayesian cue combination in sensory processing.}, number={4}, journal={Nature Reviews Neuroscience}, publisher={Nature Publishing Group}, author={Faisal, A. Aldo and Selen, Luc P. J. and Wolpert, Daniel M.}, year={2008}, month=apr, pages={292–303}, language={en} }
@article{Gerhard_Deger_Truccolo_2017, title={On the stability and dynamics of stochastic spiking neuron models: Nonlinear Hawkes process and point process GLMs}, volume={13}, ISSN={1553-734X}, DOI={10.1371/journal.pcbi.1005390}, abstractNote={Point process generalized linear models (PP-GLMs) provide an important statistical framework for modeling spiking activity in single-neurons and neuronal networks. Stochastic stability is essential when sampling from these models, as done in computational neuroscience to analyze statistical properties of neuronal dynamics and in neuro-engineering to implement closed-loop applications. Here we show, however, that despite passing common goodness-of-fit tests, PP-GLMs estimated from data are often unstable, leading to divergent firing rates. The inclusion of absolute refractory periods is not a satisfactory solution since the activity then typically settles into unphysiological rates. To address these issues, we derive a framework for determining the existence and stability of fixed points of the expected conditional intensity function (CIF) for general PP-GLMs. Specifically, in nonlinear Hawkes PP-GLMs, the CIF is expressed as a function of the previous spike history and exogenous inputs. We use a mean-field quasi-renewal (QR) approximation that decomposes spike history effects into the contribution of the last spike and an average of the CIF over all spike histories prior to the last spike. Fixed points for stationary rates are derived as self-consistent solutions of integral equations. Bifurcation analysis and the number of fixed points predict that the original models can show stable, divergent, and metastable (fragile) dynamics. For fragile models, fluctuations of the single-neuron dynamics predict expected divergence times after which rates approach unphysiologically high values. This metric can be used to estimate the probability of rates to remain physiological for given time periods, e.g., for simulation purposes. We demonstrate the use of the stability framework using simulated single-neuron examples and neurophysiological recordings. Finally, we show how to adapt PP-GLM estimation procedures to guarantee model stability. Overall, our results provide a stability framework for data-driven PP-GLMs and shed new light on the stochastic dynamics of state-of-the-art statistical models of neuronal spiking activity., Earthquakes, gene regulatory elements, financial transactions, and action potentials produced by nerve cells are examples of sequences of discrete events in space or time. In many cases, such events do not appear independently of each other. Instead, the occurrence of one event changes the rate of upcoming events (e.g, aftershocks following an earthquake). The nonlinear Hawkes process is a statistical model that captures these complex dependencies. Unfortunately, for a given model, it is hard to predict whether stochastic samples will produce an event pattern consistent with observations. In particular, with positive feedback loops, the process might diverge and yield unrealistically high event rates. Here, we show that an approximation to the mathematical model predicts dynamical properties, in particular, whether the model will exhibit stable and finite rates. In the context of neurophysiology, we find that models estimated from experimental data often tend to show metastability or even unstable dynamics. Our framework can be used to add constraints to data-driven estimation procedures to find the optimal model with realistic event rates and help to build more robust models of single-cell spiking dynamics. It is a first step towards studying the stability of large-scale nonlinear spiking neural network models estimated from data.}, number={2}, journal={PLoS Computational Biology}, author={Gerhard, Felipe and Deger, Moritz and Truccolo, Wilson}, year={2017}, month=feb, pages={e1005390} }
@article{Fourcaud-Trocmé_Hansel_Van_Vreeswijk_Brunel_2003, title={How Spike Generation Mechanisms Determine the Neuronal Response to Fluctuating Inputs}, volume={23}, ISSN={0270-6474, 1529-2401}, DOI={10.1523/JNEUROSCI.23-37-11628.2003}, number={37}, journal={The Journal of Neuroscience}, author={Fourcaud-Trocmé, Nicolas and Hansel, David and Van Vreeswijk, Carl and Brunel, Nicolas}, year={2003}, month=dec, pages={11628–11640}, language={en} }
@article{Gerwinn_Macke_Bethge_2009, title={Bayesian Population Decoding of Spiking Neurons}, volume={3}, ISSN={1662-5188}, DOI={10.3389/neuro.10.021.2009}, abstractNote={The timing of action potentials in spiking neurons depends on the temporal dynamics of their inputs and contains information about temporal fluctuations in the stimulus. Leaky integrate-and-fire neurons constitute a popular class of encoding models, in which spike times depend directly on the temporal structure of the inputs. However, optimal decoding rules for these models have only been studied explicitly in the noiseless case. Here, we study decoding rules for probabilistic inference of a continuous stimulus from the spike times of a population of leaky integrate-and-fire neurons with threshold noise. We derive three algorithms for approximating the posterior distribution over stimuli as a function of the observed spike trains. In addition to a reconstruction of the stimulus we thus obtain an estimate of the uncertainty as well. Furthermore, we derive a ‘spike-by-spike’ online decoding scheme that recursively updates the posterior with the arrival of each new spike. We use these decoding rules to reconstruct time-varying stimuli represented by a Gaussian process from spike trains of single neurons as well as neural populations.}, journal={Frontiers in Computational Neuroscience}, author={Gerwinn, Sebastian and Macke, Jakob and Bethge, Matthias}, year={2009}, month=oct, pages={21} }
@article{Giraudo_Sacerdote_1997, series={Neuronal Coding}, title={Jump-diffusion processes as models for neuronal activity}, volume={40}, ISSN={0303-2647}, DOI={10.1016/0303-2647(96)01632-2}, abstractNote={Aiming at an improvement of the existing neuronal models, we consider a mixed process ensuing from the superposition of continuous diffusions and of Poisson time-distributed sequence of impulses and focus our attention on the moments of the firing time. In particular, we consider three different instances: the large jumps model in which each jump causes the neuron firing, the reset model characterized by jumps towards the resting potential and a more general model where constant amplitude excitatory and inhibitory jumps are superimposed on diffusion. By resorting to analytical arguments and to numerical computations, the main behavioral differences of the considered models are outlined.}, number={1}, journal={Biosystems}, author={Giraudo, Maria Teresa and Sacerdote, Laura}, year={1997}, month=jan, pages={75–82}, collection={Neuronal Coding} }
@article{Gjorgjieva_Drion_Marder_2016, series={Neurobiology of cognitive behavior}, title={Computational implications of biophysical diversity and multiple timescales in neurons and synapses for circuit performance}, volume={37}, ISSN={0959-4388}, DOI={10.1016/j.conb.2015.12.008}, abstractNote={Despite advances in experimental and theoretical neuroscience, we are still trying to identify key biophysical details that are important for characterizing the operation of brain circuits. Biological mechanisms at the level of single neurons and synapses can be combined as ‘building blocks’ to generate circuit function. We focus on the importance of capturing multiple timescales when describing these intrinsic and synaptic components. Whether inherent in the ionic currents, the neurons complex morphology, or the neurotransmitter composition of synapses, these multiple timescales prove crucial for capturing the variability and richness of circuit output and enhancing the information-carrying capacity observed across nervous systems.}, journal={Current Opinion in Neurobiology}, author={Gjorgjieva, Julijana and Drion, Guillaume and Marder, Eve}, year={2016}, month=apr, pages={44–52}, collection={Neurobiology of cognitive behavior} }
@article{Glasserman_Merener_2003, title={Numerical solution of jump-diffusion LIBOR market models}, volume={7}, ISSN={0949-2984}, DOI={10.1007/s007800200076}, abstractNote={This paper develops, analyzes, and tests computational procedures for the numerical solution of LIBOR market models with jumps. We consider, in particular, a class of models in which jumps are driven by marked point processes with intensities that depend on the LIBOR rates themselves. While this formulation offers some attractive modeling features, it presents a challenge for computational work. As a first step, we therefore show how to reformulate a term structure model driven by marked point processes with suitably bounded state-dependent intensities into one driven by a Poisson random measure. This facilitates the development of discretization schemes because the Poisson random measure can be simulated without discretization error. Jumps in LIBOR rates are then thinned from the Poisson random measure using state-dependent thinning probabilities. Because of discontinuities inherent to the thinning process, this procedure falls outside the scope of existing convergence results; we provide some theoretical support for our method through a result establishing first and second order convergence of schemes that accommodates thinning but imposes stronger conditions on other problem data. The bias and computational efficiency of various schemes are compared through numerical experiments.}, number={1}, journal={Finance and Stochastics}, author={Glasserman, Paul and Merener, Nicolas}, year={2003}, month=jan, pages={1–27}, language={en} }
@article{Habenschuss_Jonke_Maass_2013, title={Stochastic Computations in Cortical Microcircuit Models}, volume={9}, ISSN={1553-7358}, DOI={10.1371/journal.pcbi.1003311}, abstractNote={Experimental data from neuroscience suggest that a substantial amount of knowledge is stored in the brain in the form of probability distributions over network states and trajectories of network states. We provide a theoretical foundation for this hypothesis by showing that even very detailed models for cortical microcircuits, with data-based diverse nonlinear neurons and synapses, have a stationary distribution of network states and trajectories of network states to which they converge exponentially fast from any initial state. We demonstrate that this convergence holds in spite of the non-reversibility of the stochastic dynamics of cortical microcircuits. We further show that, in the presence of background network oscillations, separate stationary distributions emerge for different phases of the oscillation, in accordance with experimentally reported phase-specific codes. We complement these theoretical results by computer simulations that investigate resulting computation times for typical probabilistic inference tasks on these internally stored distributions, such as marginalization or marginal maximum-a-posteriori estimation. Furthermore, we show that the inherent stochastic dynamics of generic cortical microcircuits enables them to quickly generate approximate solutions to difficult constraint satisfaction problems, where stored knowledge and current inputs jointly constrain possible solutions. This provides a powerful new computing paradigm for networks of spiking neurons, that also throws new light on how networks of neurons in the brain could carry out complex computational tasks such as prediction, imagination, memory recall and problem solving.}, number={11}, journal={PLoS Computational Biology}, author={Habenschuss, Stefan and Jonke, Zeno and Maass, Wolfgang}, editor={Sporns, Olaf}, year={2013}, month=nov, pages={e1003311}, language={en} }
@article{Guillaume_et_al_2019, title={Introductory overview of identifiability analysis: A guide to evaluating whether you have the right type of data for your modeling purpose}, volume={119}, ISSN={1364-8152}, DOI={10.1016/j.envsoft.2019.07.007}, journal={Environmental Modelling and Software}, author={Guillaume, Joseph H. A. and Jakeman, John D. and Marsili-Libelli, Stefano and Asher, Michael and Brunner, Philip and Croke, Barry and Hill, Mary C. and Jakeman, Anthony J. and Keesman, Karel J. and Razavi, Saman and Stigter, Johannes D.}, year={2019}, pages={418–432} }
@article{Haspel_et_al, title={The time is ripe to reverse engineer an entire nervous system: simulating behavior from neural interactions}, journal={pre-print}, author={Gal Haspel and Ben Baker and Isabel Beets and Edward S Boyden and Jeffrey Brown and George Church and Netta Cohen and Daniel Colon-Ramos and Eva Dyer and Christopher Fang-Yen and Steven Flavell and Miriam B Goodman and Anne C Hart and Eduardo J Izquierdo and Konstantinos Kagias and Shawn Lockery and Yangning Lu and Adam Marblestone and Jordan Matelsky and Brett Mensh and Talmo D Pereira and Hanspeter Pfister and Kanaka Rajan and Horacio G Rotstein and Monika Scholz and Joshua W. Shaevitz and Eli Shlizerman and Quilee Simeon and Michael A Skuhersky and Vineet Tiruvadi and Vivek Venkatachalam and Donglai Wei and Brock Wester and Guangyu Robert Yang and Eviatar Yemini and Manuel Zimmer and Konrad P Kording}, year={2024}, eprint={2308.06578}, archivePrefix={arXiv}, primaryClass={q-bio.NC}, url={https://arxiv.org/abs/2308.06578} }
@article{Hawkes_1971, title={Spectra of some self-exciting and mutually exciting point processes}, volume={58}, ISSN={0006-3444, 1464-3510}, DOI={10.1093/biomet/58.1.83}, abstractNote={SUMMARY In recent years methods of data analysis for point processes have received some attention, for example, by Cox & Lewis (1966) and Lewis (1964). In particular Bartlett (1963a,b) has introduced methods of analysis based on the point spectrum. Theoretical models are relatively sparse. In this paper the theoretical properties of a class of processes with particular reference to the point spectrum or corresponding covariance density functions are discussed. A particular result is a self-exciting process with the same second-order properties as a certain doubly stochastic process. These are not distinguishable by methods of data analysis based on these properties.}, number={1}, journal={Biometrika}, author={Hawkes, Alan G.}, year={1971}, pages={83–90}, language={en} }
@article{Hawkes_Oakes_1974, title={A cluster process representation of a self-exciting process}, volume={11}, ISSN={0021-9002, 1475-6072}, DOI={10.2307/3212693}, abstractNote={It is shown that all stationary self-exciting point processes with finite intensity may be represented as Poisson cluster processes which are age-dependent immigration-birth processes, and their existence is established. This result is used to derive some counting and interval properties of these processes using the probability generating functional.}, number={3}, journal={Journal of Applied Probability}, author={Hawkes, Alan G. and Oakes, David}, year={1974}, month=sep, pages={493–503}, language={en} }
@article{Herculano-Houzel_2009, title={The human brain in numbers: a linearly scaled-up primate brain}, volume={3}, ISSN={1662-5161}, DOI={10.3389/neuro.09.031.2009}, abstractNote={The human brain has often been viewed as outstanding among mammalian brains: the most cognitively able, the largest-than-expected from body size, endowed with an overdeveloped cerebral cortex that represents over 80% of brain mass, and purportedly containing 100 billion neurons and 10x more glial cells. Such uniqueness was seemingly necessary to justify the superior cognitive abilities of humans over larger-brained mammals such as elephants and whales. However, our recent studies using a novel method to determine the cellular composition of the brain of humans and other primates as well as of rodents and insectivores show that, since different cellular scaling rules apply to the brains within these orders, brain size can no longer be considered a proxy for the number of neurons in the brain. These studies also showed that the human brain is not exceptional in its cellular composition, as it was found to contain as many neuronal and non-neuronal cells as would be expected of a primate brain of its size. Additionally, the so-called overdeveloped human cerebral cortex holds only 19% of all brain neurons, a fraction that is similar to that found in other mammals. In what regards absolute numbers of neurons, however, the human brain does have two advantages compared to other mammalian brains: compared to rodents, and probably to whales and elephants as well, it is built according to the very economical, space-saving scaling rules that apply to other primates; and, among economically built primate brains, it is the largest, hence containing the most neurons. These findings argue in favor of a view of cognitive abilities that is centered on absolute numbers of neurons, rather than on body size or encephalization, and call for a re-examination of several concepts related to the exceptionality of the human brain.}, journal={Frontiers in Human Neuroscience}, author={Herculano-Houzel, Suzana}, year={2009}, pages={31}, language={eng} }
@article{Hodgkin_1948, title={The local electric changes associated with repetitive action in a non-medullated axon}, volume={107}, ISSN={1469-7793}, DOI={10.1113/jphysiol.1948.sp004260}, number={2}, journal={The Journal of Physiology}, author={Hodgkin, A. L.}, year={1948}, pages={165–181}, language={en} }
@article{Hodgkin_Huxley_1952, title={A quantitative description of membrane current and its application to conduction and excitation in nerve}, volume={117}, ISSN={0022-3751}, DOI={10.1113/jphysiol.1952.sp004764}, number={4}, journal={The Journal of Physiology}, author={Hodgkin, A. L. and Huxley, A. F.}, year={1952}, month=aug, pages={500–544}, language={eng} }
@article{Hubel_Wiesel_1962, title={Receptive fields, binocular interaction and functional architecture in the cat’s visual cortex}, volume={160}, ISSN={0022-3751}, DOI={10.1113/jphysiol.1962.sp006837}, number={1}, journal={The Journal of Physiology}, author={Hubel, D. H. and Wiesel, T. N.}, year={1962}, month=jan, pages={106–154}, language={eng} }
@article{Ionescu-Tulcea_1949, title={Mesures dans les espaces produits}, journal={Atti Accad. Naz. Lincei Rend}, volume={7}, author={Ionescu-Tulcea, CT}, year={1949}, pages={208–211} }
@article{Jabin_Schmutz_Zhou_2024, journal={preprint}, title={Non-exchangeable networks of integrate-and-fire neurons: spatially-extended mean-field limit of the empirical measure}, url={http://arxiv.org/abs/2409.06325}, abstractNote={The dynamics of exchangeable or spatially-structured networks of N interacting stochastic neurons can be described by deterministic population equations in the mean-ﬁeld limit N → ∞, when synaptic weights scale as O(1/N ). This asymptotic behavior has been proven in several works but a general question has remained unanswered: does the O(1/N ) scaling of synaptic weights, by itself, suﬃce to guarantee the convergence of network dynamics to a deterministic population equation, even when networks are not assumed to be exchangeable or spatially structured? In this work, we consider networks of stochastic integrate-and-ﬁre neurons with arbitrary synaptic weights satisfying only a O(1/N ) scaling condition. Borrowing results from the theory of dense graph limits (graphons), we prove that, as N → ∞, and up to the extraction of a subsequence, the empirical measure of the neurons’ membrane potentials converges to the solution of a spatially-extended mean-ﬁeld partial diﬀerential equation (PDE). Our proof requires analytical techniques which go beyond standard propagation of chaos methods. In particular, we introduce a weak metric that depends on the dense graph limit kernel and we show how the weak convergence of the initial data can be obtained by propagating the regularity of the limit kernel along the dual-backward equation associated with the spatially-extended meanﬁeld PDE. Overall, this result invites us to re-interpret spatially-extended population equations as universal mean-ﬁeld limits of networks of neurons with O(1/N ) synaptic weight scaling.}, note={arXiv:2409.06325}, publisher={arXiv}, author={Jabin, Pierre-Emmanuel and Schmutz, Valentin and Zhou, Datong}, year={2024}, month=sep, language={en} }
@article{Jabin_Zhou_2023, title={The mean-field Limit of sparse networks of integrate and fire neurons}, journal={pre-print}, url={http://arxiv.org/abs/2309.04046}, abstractNote={We study the mean-ﬁeld limit of a model of biological neuron networks based on the so-called stochastic integrate-and-ﬁre (IF) dynamics. Our approach allows to derive a continuous limit for the macroscopic behavior of the system, the 1-particle distribution, for a large number of neurons with no structural assumptions on the connection map outside of a generalized mean-ﬁeld scaling. We propose a novel notion of observables that naturally extends the notion of marginals to systems with non-identical or non-exchangeable agents. Our new observables satisfy a complex approximate hierarchy, essentially a tree-indexed extension of the classical BBGKY hierarchy. We are able to pass to the limit in this hierarchy as the number of neurons increases through novel quantitative stability estimates in some adapted weak norm. While we require non-vanishing diﬀusion, this approach notably addresses the challenges of sparse interacting graphs/matrices and singular interactions from Poisson jumps, and requires no additional regularity on the initial distribution.}, note={arXiv:2309.04046 [nlin, q-bio]}, publisher={arXiv}, author={Jabin, Pierre-Emmanuel and Zhou, Datong}, year={2023}, month=sep, language={en} }
@article{Jacod_1975, title={Multivariate point processes: predictable projection, Radon-Nikodym derivatives, representation of martingales}, volume={31}, ISSN={1432-2064}, DOI={10.1007/BF00536010}, number={3}, journal={Zeitschrift für Wahrscheinlichkeitstheorie und Verwandte Gebiete}, author={Jacod, Jean}, year={1975}, month=sep, pages={235–253}, language={en} }
@article{Jahn_Berg_Hounsgaard_Ditlevsen_2011, title={Motoneuron membrane potentials follow a time inhomogeneous jump diffusion process}, volume={31}, ISSN={1573-6873}, DOI={10.1007/s10827-011-0326-z}, abstractNote={Stochastic leaky integrate-and-fire models are popular due to their simplicity and statistical tractability. They have been widely applied to gain understanding of the underlying mechanisms for spike timing in neurons, and have served as building blocks for more elaborate models. Especially the Ornstein–Uhlenbeck process is popular to describe the stochastic fluctuations in the membrane potential of a neuron, but also other models like the square-root model or models with a non-linear drift are sometimes applied. Data that can be described by such models have to be stationary and thus, the simple models can only be applied over short time windows. However, experimental data show varying time constants, state dependent noise, a graded firing threshold and time-inhomogeneous input. In the present study we build a jump diffusion model that incorporates these features, and introduce a firing mechanism with a state dependent intensity. In addition, we suggest statistical methods to estimate all unknown quantities and apply these to analyze turtle motoneuron membrane potentials. Finally, simulated and real data are compared and discussed. We find that a square-root diffusion describes the data much better than an Ornstein–Uhlenbeck process with constant diffusion coefficient. Further, the membrane time constant decreases with increasing depolarization, as expected from the increase in synaptic conductance. The network activity, which the neuron is exposed to, can be reasonably estimated to be a threshold version of the nerve output from the network. Moreover, the spiking characteristics are well described by a Poisson spike train with an intensity depending exponentially on the membrane potential.}, number={3}, journal={Journal of Computational Neuroscience}, author={Jahn, Patrick and Berg, Rune W. and Hounsgaard, Jørn and Ditlevsen, Susanne}, year={2011}, month=nov, pages={563–579}, language={en} }
@article{Jang_2007, title={Jump diffusion processes and their applications in insurance and finance}, volume={41}, ISSN={0167-6687}, DOI={10.1016/j.insmatheco.2006.09.006}, abstractNote={For insurance risks, jump processes such as homogeneous/non-homogeneous compound Poisson processes and compound Cox processes have been used to model aggregate losses. If we consider the economic assumption of a positive interest to aggregate losses, Lévy processes have proven to be useful. Also in financial modelling, it has been observed that diffusion models are not robust enough to capture the appearance of jumps in underlying asset prices and interest rates. As a result, jump diffusion processes, which are, simply speaking, combinations of compound Poisson processes with Brownian motion, have gained popularity for modelling in insurance and finance. In this paper, considering a jump diffusion process, we obtain the explicit expression of the joint Laplace transform of the distribution of a jump diffusion process and its integrated process, assuming that jump size follows the mixture of two exponential distributions, which is a special case of phase-type distributions. Based on this Laplace transform, we derive the moments of the aggregate accumulated claim amounts of insurance risk. For a financial application, we concern non-defaultable zero-coupon bond pricing. We also provide several numerical examples for the moments of aggregate accumulated claims and default-free zero-coupon bond prices.}, number={1}, journal={Insurance: Mathematics and Economics}, author={Jang, Jiwook}, year={2007}, month=jul, pages={62–70} }
@article{Johnson_1996, title={Point process models of single-neuron discharges}, volume={3}, ISSN={1573-6873}, DOI={10.1007/BF00161089}, abstractNote={In most neural systems, neurons communicate via sequences of action potentials. Contemporary models assume that the action potentials’ times of occurrence rather than their waveforms convey information. The mathematical tool for describing sequences of events occurring in time and/or space is the theory of point processes. Using this theory, we show that neural discharge patterns convey time-varying information intermingled with the neuron’s response characteristics. We review the basic techniques for analyzing single-neuron discharge patterns and describe what they reveal about the underlying point process model. By applying information theory and estimation theory to point processes, we describe the fundamental limits on how well information can be represented by and extracted from neural discharges. We illustrate applying these results by considering recordings from the lower auditory pathway.}, number={4}, journal={Journal of Computational Neuroscience}, author={Johnson, Don H.}, year={1996}, month=dec, pages={275–299}, language={en} }
@article{Jolivet_Lewis_Gerstner_2004, title={Generalized Integrate-and-Fire Models of Neuronal Activity Approximate Spike Trains of a Detailed Model to a High Degree of Accuracy}, volume={92}, ISSN={0022-3077}, DOI={10.1152/jn.00190.2004}, abstractNote={We demonstrate that single-variable integrate-and-fire models can quantitatively capture the dynamics of a physiologically detailed model for fast-spiking cortical neurons. Through a systematic set of approximations, we reduce the conductance-based model to 2 variants of integrate-and-fire models. In the first variant (nonlinear integrate-and-fire model), parameters depend on the instantaneous membrane potential, whereas in the second variant, they depend on the time elapsed since the last spike [Spike Response Model (SRM)]. The direct reduction links features of the simple models to biophysical features of the full conductance-based model. To quantitatively test the predictive power of the SRM and of the nonlinear integrate-and-fire model, we compare spike trains in the simple models to those in the full conductance-based model when the models are subjected to identical randomly fluctuating input. For random current input, the simple models reproduce 70–80 percent of the spikes in the full model (with temporal precision of ±2 ms) over a wide range of firing frequencies. For random conductance injection, up to 73 percent of spikes are coincident. We also present a technique for numerically optimizing parameters in the SRM and the nonlinear integrate-and-fire model based on spike trains in the full conductance-based model. This technique can be used to tune simple models to reproduce spike trains of real neurons.}, number={2}, journal={Journal of Neurophysiology}, publisher={American Physiological Society}, author={Jolivet, Renaud and Lewis, Timothy J. and Gerstner, Wulfram}, year={2004}, month=aug, pages={959–976} }
@article{Kass_Amari_Arai_Brown_Diekman_Diesmann_Doiron_Eden_Fairhall_Fiddyment_et_al._2018, title={Computational Neuroscience: Mathematical and Statistical Perspectives}, volume={5}, ISSN={2326-8298, 2326-831X}, DOI={10.1146/annurev-statistics-041715-033733}, abstractNote={Mathematical and statistical models have played important roles in neuroscience, especially by describing the electrical activity of neurons recorded individually, or collectively across large networks. As the field moves forward rapidly, new challenges are emerging. For maximal effectiveness, those working to advance computational neuroscience will need to appreciate and exploit the complementary strengths of mechanistic theory and the statistical paradigm.}, number={Volume 5, 2018}, journal={Annual Review of Statistics and Its Application}, publisher={Annual Reviews}, author={Kass, Robert E. and Amari, Shun-Ichi and Arai, Kensuke and Brown, Emery N. and Diekman, Casey O. and Diesmann, Markus and Doiron, Brent and Eden, Uri T. and Fairhall, Adrienne L. and Fiddyment, Grant M. and Fukai, Tomoki and Grün, Sonja and Harrison, Matthew T. and Helias, Moritz and Nakahara, Hiroyuki and Teramae, Jun-nosuke and Thomas, Peter J. and Reimers, Mark and Rodu, Jordan and Rotstein, Horacio G. and Shea-Brown, Eric and Shimazaki, Hideaki and Shinomoto, Shigeru and Yu, Byron M. and Kramer, Mark A.}, year={2018}, month=mar, pages={183–214}, language={en} }
@article{Kistler_Gerstner_Hemmen_1997, title={Reduction of the Hodgkin-Huxley Equations to a Single-Variable Threshold Model}, volume={9}, ISSN={0899-7667}, DOI={10.1162/neco.1997.9.5.1015}, abstractNote={It is generally believed that a neuron is a threshold element that fires when some variable u reaches a threshold. Here we pursue the question of whether this picture can be justified and study the four-dimensional neuron model of Hodgkin and Huxley as a concrete example. The model is approximated by a response kernel expansion in terms of a single variable, the membrane voltage. The first-order term is linear in the input and its kernel has the typical form of an elementary postsynaptic potential. Higher-order kernels take care of nonlinear interactions between input spikes. In contrast to the standard Volterra expansion, the kernels depend on the firing time of the most recent output spike. In particular, a zero-order kernel that describes the shape of the spike and the typical after-potential is included. Our model neuron fires if the membrane voltage, given by the truncated response kernel expansion, crosses a threshold. The threshold model is tested on a spike train generated by the Hodgkin-Huxley model with a stochastic input current. We find that the threshold model predicts 90 percent of the spikes correctly. Our results show that, to good approximation, the description of a neuron as a threshold element can indeed be justified.}, number={5}, journal={Neural Computation}, author={Kistler, Werner M. and Gerstner, Wulfram and Hemmen, J. Leo van}, year={1997}, month=jul, pages={1015–1045} }
@article{Knight_1972, title={Dynamics of Encoding in a Population of Neurons}, volume={59}, ISSN={0022-1295}, DOI={10.1085/jgp.59.6.734}, abstractNote={A simple encoder model, which is a reasonable idealization from known electrophysiological properties, yields a population in which the variation of the firing rate with time is a perfect replica of the shape of the input stimulus. A population of noise-free encoders which depart even slightly from the simple model yield a very much degraded copy of the input stimulus. The presence of noise improves the performance of such a population. The firing rate of a population of neurons is related to the firing rate of a single member in a subtle way.}, number={6}, journal={Journal of General Physiology}, author={Knight, Bruce W.}, year={1972}, month=jun, pages={734–766} }
@article{Knill_Pouget_2004, title={The Bayesian brain: the role of uncertainty in neural coding and computation}, volume={27}, ISSN={0166-2236, 1878-108X}, DOI={10.1016/j.tins.2004.10.007}, number={12}, journal={Trends in Neurosciences}, publisher={Elsevier}, author={Knill, David C. and Pouget, Alexandre}, year={2004}, month=dec, pages={712–719}, language={English} }
@article{Koch_Bernander_Douglas_1995, title={Do neurons have a voltage or a current threshold for action potential initiation?}, volume={2}, ISSN={1573-6873}, DOI={10.1007/BF00962708}, journal={Journal of Computational Neuroscience}, author={Koch, Christof and Bernander, Ojvind and Douglas, Rodney J.}, year={1995}, month={mar}, pages={63–82}, language={en} }
@article{Kushner_2000, title={Jump-Diffusions with Controlled Jumps: Existence and Numerical Methods}, volume={249}, ISSN={0022-247X}, DOI={10.1006/jmaa.2000.6936}, abstractNote={A comprehensive development of effective numerical methods for stochastic control problems in continuous time, for reflected jump-diffusion models, is given in earlier work by the author. While these methods cover the bulk of models which have been of interest to date, they do not explicitly deal with the case where the jump itself is controlled in the sense that the value of the control just before the jump affects the distribution of the jump. We do not deal explicitly with the numerical algorithms but develop some of the concepts which are needed to provide the background which is necessary to extend the proofs of earlier work to this case. A critical issue is that of closure, i.e., defining the model such that any sequence of (systems, controls) has a convergent subsequence of the same type. One needs to introduce an extension of the Poisson measure (which serves a purpose analogous to that served by relaxed controls), which we call the relaxed Poisson measure, analogously to the use of the martingale measure concept given earlier to deal with controlled variance. The existence of an optimal control is a consequence of the development.}, number={1}, journal={Journal of Mathematical Analysis and Applications}, author={Kushner, Harold J.}, year={2000}, month=sep, pages={179–198} }
@article{Levenstein_Alvarez_Amarasingham_Azab_Chen_Gerkin_Hasenstaub_Iyer_Jolivet_Marzen_et_al._2023, title={On the Role of Theory and Modeling in Neuroscience}, volume={43}, rights={Copyright © 2023 the authors. SfN exclusive license.}, ISSN={0270-6474, 1529-2401}, DOI={10.1523/JNEUROSCI.1179-22.2022}, abstractNote={In recent years, the field of neuroscience has gone through rapid experimental advances and a significant increase in the use of quantitative and computational methods. This growth has created a need for clearer analyses of the theory and modeling approaches used in the field. This issue is particularly complex in neuroscience because the field studies phenomena that cross a wide range of scales and often require consideration at varying degrees of abstraction, from precise biophysical interactions to the computations they implement. We argue that a pragmatic perspective of science, in which descriptive, mechanistic, and normative models and theories each play a distinct role in defining and bridging levels of abstraction, will facilitate neuroscientific practice. This analysis leads to methodological suggestions, including selecting a level of abstraction that is appropriate for a given problem, identifying transfer functions to connect models and data, and the use of models themselves as a form of experiment.}, number={7}, journal={Journal of Neuroscience}, publisher={Society for Neuroscience}, author={Levenstein, Daniel and Alvarez, Veronica A. and Amarasingham, Asohan and Azab, Habiba and Chen, Zhe S. and Gerkin, Richard C. and Hasenstaub, Andrea and Iyer, Ramakrishnan and Jolivet, Renaud B. and Marzen, Sarah and Monaco, Joseph D. and Prinz, Astrid A. and Quraishi, Salma and Santamaria, Fidel and Shivkumar, Sabyasachi and Singh, Matthew F. and Traub, Roger and Nadim, Farzan and Rotstein, Horacio G. and Redish, A. David}, year={2023}, month=feb, pages={1074–1088}, language={en} }
@article{Liewald_Miller_Logothetis_Wagner_Schüz_2014, title={Distribution of axon diameters in cortical white matter: an electron-microscopic study on three human brains and a macaque}, volume={108}, ISSN={1432-0770}, DOI={10.1007/s00422-014-0626-2}, abstractNote={The aim of this study was to obtain information on the axonal diameters of cortico-cortical fibres in the human brain, connecting distant regions of the same hemisphere via the white matter. Samples for electron microscopy were taken from the region of the superior longitudinal fascicle and from the transitional white matter between temporal and frontal lobe where the uncinate and inferior occipitofrontal fascicle merge. We measured the inner diameter of cross sections of myelinated axons. For comparison with data from the literature on the human corpus callosum, we also took samples from that region. For comparison with well-fixed material, we also included samples from corresponding regions of a monkey brain (Macaca mulatta). Fibre diameters in human brains ranged from 0.16 to 9 $$upmu hbox {m}$$. Distributions of diameters were similar in the three systems of cortico-cortical fibres investigated, both in humans and the monkey, with most of the average values below 1 $$upmu $$m diameter and a small population of much thicker fibres. Within individual human brains, the averages were larger in the superior longitudinal fascicle than in the transitional zone between temporal and frontal lobe. An asymmetry between left and right could be found in one of the human brains, as well as in the monkey brain. A correlation was also found between the thickness of the myelin sheath and the inner axon diameter for axons whose calibre was greater than about 0.6 $$upmu hbox {m}$$. The results are compared to white matter data in other mammals and are discussed with respect to conduction velocity, brain size, cognition, as well as diffusion weighted imaging studies.}, number={5}, journal={Biological Cybernetics}, author={Liewald, Daniel and Miller, Robert and Logothetis, Nikos and Wagner, Hans-Joachim and Schüz, Almut}, year={2014}, month=oct, pages={541–557}, language={en} }
@article{Lisman_Buzsaki_2008, title={A Neural Coding Scheme Formed by the Combined Function of Gamma and Theta Oscillations}, volume={34}, ISSN={0586-7614}, DOI={10.1093/schbul/sbn060}, abstractNote={Brain oscillations are important in controlling the timing of neuronal firing. This process has been extensively analyzed in connection with gamma frequency oscillations and more recently with respect to theta frequency oscillations. Here we review evidence that theta and gamma oscillations work together to form a neural code. This coding scheme provides a way for multiple neural ensembles to represent an ordered sequence of items. In the hippocampus, this coding scheme is utilized during the phase precession, a phenomenon that can be interpreted as the recall of sequences of items (places) from long-term memory. The same coding scheme may be used in certain cortical regions to encode multi-item short-term memory. The possibility that abnormalities in theta/gamma could underlie symptoms of schizophrenia is discussed.}, number={5}, journal={Schizophrenia Bulletin}, author={Lisman, John and Buzsáki, György}, year={2008}, month=sep, pages={974–980} }
@article{Ma_Jazayeri_2014, title={Neural Coding of Uncertainty and Probability}, volume={37}, ISSN={0147-006X, 1545-4126}, DOI={10.1146/annurev-neuro-071013-014017}, abstractNote={Organisms must act in the face of sensory, motor, and reward uncertainty stemming from a pandemonium of stochasticity and missing information. In many tasks, organisms can make better decisions if they have at their disposal a representation of the uncertainty associated with task-relevant variables. We formalize this problem using Bayesian decision theory and review recent behavioral and neural evidence that the brain may use knowledge of uncertainty, confidence, and probability.}, number={Volume 37, 2014}, journal={Annual Review of Neuroscience}, publisher={Annual Reviews}, author={Ma, Wei Ji and Jazayeri, Mehrdad}, year={2014}, month=jul, pages={205–220}, language={en} }
@article{Marre_El_Boustani_Frégnac_Destexhe_2009, title={Prediction of Spatiotemporal Patterns of Neural Activity from Pairwise Correlations}, volume={102}, rights={http://link.aps.org/licenses/aps-default-license}, ISSN={0031-9007, 1079-7114}, DOI={10.1103/PhysRevLett.102.138101}, number={13}, journal={Physical Review Letters}, author={Marre, O. and El Boustani, S. and Frégnac, Y. and Destexhe, A.}, year={2009}, month=apr, pages={138101}, language={en} }
@article{Mascart_Muzy_Reynaud-bouret_2021, title={Efficient Simulation of Sparse Graphs of Point Processes}, url={http://arxiv.org/abs/2001.01702}, journal={pre-print}, abstractNote={We derive new discrete event simulation algorithms for marked time point processes. The main idea is to couple a special structure, namely the associated local independence graph, as defined by Didelez arXiv:0710.5874, with the activity tracking algorithm [muzy, 2019] for achieving high performance asynchronous simulations. With respect to classical algorithm, this allows reducing drastically the computational complexity, especially when the graph is sparse. [muzy, 2019] A. Muzy. 2019. Exploiting activity for the modeling and simulation of dynamics and learning processes in hierarchical (neurocognitive) systems. (Submitted to) Magazine of Computing in Science & Engineering (2019)}, note={arXiv:2001.01702}, publisher={arXiv}, author={Mascart, Cyrille and Muzy, Alexandre and Reynaud-bouret, Patricia}, year={2021}, month=mar, language={en} }
@article{Meliza_Kostuk_Huang_Nogaret_Margoliash_Abarbanel_2014, title={Estimating parameters and predicting membrane voltages with conductance-based neuron models}, volume={108}, ISSN={1432-0770}, DOI={10.1007/s00422-014-0615-5}, abstractNote={Recent results demonstrate techniques for fully quantitative, statistical inference of the dynamics of individual neurons under the Hodgkin–Huxley framework of voltage-gated conductances. Using a variational approximation, this approach has been successfully applied to simulated data from model neurons. Here, we use this method to analyze a population of real neurons recorded in a slice preparation of the zebra finch forebrain nucleus HVC. Our results demonstrate that using only 1,500 ms of voltage recorded while injecting a complex current waveform, we can estimate the values of 12 state variables and 72 parameters in a dynamical model, such that the model accurately predicts the responses of the neuron to novel injected currents. A less complex model produced consistently worse predictions, indicating that the additional currents contribute significantly to the dynamics of these neurons. Preliminary results indicate some differences in the channel complement of the models for different classes of HVC neurons, which accords with expectations from the biology. Whereas the model for each cell is incomplete (representing only the somatic compartment, and likely to be missing classes of channels that the real neurons possess), our approach opens the possibility to investigate in modeling the plausibility of additional classes of channels the cell might possess, thus improving the models over time. These results provide an important foundational basis for building biologically realistic network models, such as the one in HVC that contributes to the process of song production and developmental vocal learning in songbirds.}, number={4}, journal={Biological Cybernetics}, author={Meliza, C. Daniel and Kostuk, Mark and Huang, Hao and Nogaret, Alain and Margoliash, Daniel and Abarbanel, Henry D. I.}, year={2014}, month=aug, pages={495–516}, language={en} }
@article{Moroz_2009, title={On the Independent Origins of Complex Brains and Neurons}, volume={74}, ISSN={0006-8977}, DOI={10.1159/000258665}, abstractNote={Analysis of the origin and evolution of neurons is crucial for revealing principles of organization of neural circuits with unexpected implications for genomic sciences, biomedical applications and regenerative medicine. This article presents an overview of some controversial ideas about the origin and evolution of neurons and nervous systems, focusing on the independent origin of complex brains and possible independent origins of neurons. First, earlier hypotheses related to the origin of neurons are summarized. Second, the diversity of nervous systems and convergent evolution of complex brains in relation to current views about animal phylogeny is discussed. Third, the lineages of molluscs and basal metazoans are used as illustrated examples of multiple origins of complex brains and neurons. Finally, a hypothesis about the independent origin of complex brains, centralized nervous systems and neurons is outlined. Injury-associated mechanisms leading to secretion of signal peptides (and related molecules) can be considered as evolutionary predecessors of inter-neuronal signaling and the major factors in the appearance of neurons in the first place.}, number={3}, journal={Brain Behavior and Evolution}, author={Moroz, Leonid L.}, year={2009}, month=dec, pages={177–190} }
@article{Mountcastle_1997, title={The columnar organization of the neocortex}, volume={120 ( Pt 4)}, ISSN={0006-8950}, DOI={10.1093/brain/120.4.701}, abstractNote={The modular organization of nervous systems is a widely documented principle of design for both vertebrate and invertebrate brains of which the columnar organization of the neocortex is an example. The classical cytoarchitectural areas of the neocortex are composed of smaller units, local neural circuits repeated iteratively within each area. Modules may vary in cell type and number, in internal and external connectivity, and in mode of neuronal processing between different large entities; within any single large entity they have a basic similarity of internal design and operation. Modules are most commonly grouped into entities by sets of dominating external connections. This unifying factor is most obvious for the heterotypical sensory and motor areas of the neocortex. Columnar defining factors in homotypical areas are generated, in part, within the cortex itself. The set of all modules composing such an entity may be fractionated into different modular subsets by different extrinsic connections. Linkages between them and subsets in other large entities form distributed systems. The neighborhood relations between connected subsets of modules in different entities result in nested distributed systems that serve distributed functions. A cortical area defined in classical cytoarchitectural terms may belong to more than one and sometimes to several distributed systems. Columns in cytoarchitectural areas located at some distance from one another, but with some common properties, may be linked by long-range, intracortical connections.}, journal={Brain: A Journal of Neurology}, author={Mountcastle, V. B.}, year={1997}, month=apr, pages={701–722}, language={eng} }
@article{Mountcastle_1957, title={Modality and topographic properties of single neurons of cat’s somatic sensory cortex}, volume={20}, ISSN={0022-3077}, DOI={10.1152/jn.1957.20.4.408}, number={4}, journal={Journal of Neurophysiology}, publisher={American Physiological Society}, author={Mountcastle, Vernon B.}, year={1957}, month=jul, pages={408–434} }
@article{Nunez_1974, title={The brain wave equation: a model for the EEG}, volume={21}, ISSN={0025-5564}, DOI={10.1016/0025-5564(74)90020-0}, abstractNote={Both spontaneous and evoked potentials measured on the surface of the head are believed due to postsynaptic potentials in vertically oriented neurons in the cortex. Potential differences between surface locations at any given time are due to the instantaneous difference in synaptic activity between the corresponding vertical regions. Because of the high correlation of activity between regions separated by distances large compared to the radius of influence of single neurons, communication between these locations must be by means of action potentials. In order to quantify the dynamics of interaction of 1010 cortical neurons, use is made of the concept of a neural mass. The neural mass consists of sufficiently large number of neurons so as to exhibit certain average properties which are independent of its exact inner circuitry. An integral wave equation is derived to describe the spatial-temporal variation of cortical potential. Solutions are obtained indicating that electrical oscillations, which are independent of the location and time history of subcortical input, can persist in the cortex. The nature of these oscillations depends on the relative abundance of excitatory and inhibitory connections between neural masses and on the physiological state of the brain. The latter is partially determined by velocity distribution functions for action potential propagation. A dispersion relation for brain waves is shown to exist for certain ranges of the connection parameters. In some limiting cases, weakly damped waves occur with ω=ck, where c refers to a characteristic velocity for the distribution functions. Preliminary experiments indicate qualitative agreement with this result.}, number={3}, journal={Mathematical Biosciences}, author={Nunez, Paul L.}, year={1974}, month=dec, pages={279–297} }
@article{Ogata_1981, title={On Lewis’ simulation method for point processes}, volume={27}, rights={https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html}, ISSN={0018-9448}, DOI={10.1109/TIT.1981.1056305}, abstractNote={A simple and efficient method of simulation is discwsed for point processes that are specified by their conditional intensities. Tbe metbud is based on tbe thinning algorithm which was introduced recently by Lewis and Shedler for the simulation of nonhomogeneous Poisson pmcesses. Algorithms are given for past dependent point processes contain@ multivariate processes. The simulations are performed for some parametric conditional intensity functions, and the accuracy of tbe sbnulated data is demonstrated by the liieliiood ratio test aud the minbuum Akaiie information criterion (AK) procedure.}, number={1}, journal={IEEE Transactions on Information Theory}, author={Ogata, Y.}, year={1981}, month=jan, pages={23–31}, language={en} }
@inbook{Paninski_Brown_Iyengar_Kass_2009, title={Statistical Models of Spike Trains}, ISBN={978-0-19-923507-0}, url={https://academic.oup.com/book/6207/chapter/149837952}, DOI={10.1093/acprof:oso/9780199235070.003.0010}, note={DOI: 10.1093/acprof:oso/9780199235070.003.0010}, booktitle={Stochastic Methods in Neuroscience}, publisher={Oxford University Press}, author={Paninski, Liam and Brown, Emery N. and Iyengar, Satish and Kass, Robert E.}, editor={Laing, Carlo and Lord, Gabriel J}, year={2009}, month=sep, pages={272–296}, language={en} }
@article{Parker_2022, title={Neurobiological reduction: From cellular explanations of behavior to interventions}, volume={13}, ISSN={1664-1078}, DOI={10.3389/fpsyg.2022.987101}, abstractNote={Scientific reductionism, the view that higher level functions can be explained by properties at some lower-level or levels, has been an assumption of nervous system analyses since the acceptance of the neuron doctrine in the late 19th century, and became a dominant experimental approach with the development of intracellular recording techniques in the mid-20th century. Subsequent refinements of electrophysiological approaches and the continual development of molecular and genetic techniques have promoted a focus on molecular and cellular mechanisms in experimental analyses and explanations of sensory, motor, and cognitive functions. Reductionist assumptions have also influenced our views of the etiology and treatment of psychopathologies, and have more recently led to claims that we can, or even should, pharmacologically enhance the normal brain. Reductionism remains an area of active debate in the philosophy of science. In neuroscience and psychology, the debate typically focuses on the mind-brain question and the mechanisms of cognition, and how or if they can be explained in neurobiological terms. However, these debates are affected by the complexity of the phenomena being considered and the difficulty of obtaining the necessary neurobiological detail. We can instead ask whether features identified in neurobiological analyses of simpler aspects in simpler nervous systems support current molecular and cellular approaches to explaining systems or behaviors. While my view is that they do not, this does not invite the opposing view prevalent in dichotomous thinking that molecular and cellular detail is irrelevant and we should focus on computations or representations. We instead need to consider how to address the long-standing dilemma of how a nervous system that ostensibly functions through discrete cell to cell communication can generate population effects across multiple spatial and temporal scales to generate behavior.}, journal={Frontiers in Psychology}, author={Parker, David}, year={2022}, month=dec, pages={987101} }
@article{Pecevski_Buesing_Maass_2011, title={Probabilistic Inference in General Graphical Models through Sampling in Stochastic Networks of Spiking Neurons}, volume={7}, ISSN={1553-7358}, DOI={10.1371/journal.pcbi.1002294}, abstractNote={An important open problem of computational neuroscience is the generic organization of computations in networks of neurons in the brain. We show here through rigorous theoretical analysis that inherent stochastic features of spiking neurons, in combination with simple nonlinear computational operations in specific network motifs and dendritic arbors, enable networks of spiking neurons to carry out probabilistic inference through sampling in general graphical models. In particular, it enables them to carry out probabilistic inference in Bayesian networks with converging arrows (‘“explaining away”’) and with undirected loops, that occur in many real-world tasks. Ubiquitous stochastic features of networks of spiking neurons, such as trial-to-trial variability and spontaneous activity, are necessary ingredients of the underlying computational organization. We demonstrate through computer simulations that this approach can be scaled up to neural emulations of probabilistic inference in fairly large graphical models, yielding some of the most complex computations that have been carried out so far in networks of spiking neurons.}, number={12}, journal={PLoS Computational Biology}, author={Pecevski, Dejan and Buesing, Lars and Maass, Wolfgang}, editor={Sporns, Olaf}, year={2011}, month=dec, pages={e1002294}, language={en} }
@article{Pillow_Paninski_Uzzell_Simoncelli_Chichilnisky_2005, title={Prediction and Decoding of Retinal Ganglion Cell Responses with a Probabilistic Spiking Model}, volume={25}, rights={Copyright © 2005 Society for Neuroscience 0270-6474/05/2511003-11.00/0}, ISSN={0270-6474, 1529-2401}, DOI={10.1523/JNEUROSCI.3305-05.2005}, abstractNote={Sensory encoding in spiking neurons depends on both the integration of sensory inputs and the intrinsic dynamics and variability of spike generation. We show that the stimulus selectivity, reliability, and timing precision of primate retinal ganglion cell (RGC) light responses can be reproduced accurately with a simple model consisting of a leaky integrate-and-fire spike generator driven by a linearly filtered stimulus, a postspike current, and a Gaussian noise current. We fit model parameters for individual RGCs by maximizing the likelihood of observed spike responses to a stochastic visual stimulus. Although compact, the fitted model predicts the detailed time structure of responses to novel stimuli, accurately capturing the interaction between the spiking history and sensory stimulus selectivity. The model also accounts for the variability in responses to repeated stimuli, even when fit to data from a single (nonrepeating) stimulus sequence. Finally, the model can be used to derive an explicit, maximum-likelihood decoding rule for neural spike trains, thus providing a tool for assessing the limitations that spiking variability imposes on sensory performance.}, number={47}, journal={Journal of Neuroscience}, publisher={Society for Neuroscience}, author={Pillow, Jonathan W. and Paninski, Liam and Uzzell, Valerie J. and Simoncelli, Eero P. and Chichilnisky, E. J.}, year={2005}, month=nov, pages={11003–11013}, language={en} }
@article{Pillow_Shlens_Paninski_Sher_Litke_Chichilnisky_Simoncelli_2008, title={Spatio-temporal correlations and visual signalling in a complete neuronal population}, volume={454}, rights={2008 Macmillan Publishers Limited. All rights reserved}, ISSN={1476-4687}, DOI={10.1038/nature07140}, abstractNote={Correlated activity between sensory neurons governs both the stimulus information conveyed by a neural population and how downstream neurons can extract it. Although previous studies looking at pairs of cells have examined correlations, their functional origin and impact on the neural code are still not understood. Pillow et al. address the question in a complete population of primate retinal ganglion cells. Fitting the physiological data to a model of multi-neuron spike responses, the authors find that a significant fraction of what is usually considered single-cell noise in trial-to-trial response variability can be explained by correlations, and that a significant amount of sensory information can be decoded from the correlation structure.}, number={7207}, journal={Nature}, publisher={Nature Publishing Group}, author={Pillow, Jonathan W. and Shlens, Jonathon and Paninski, Liam and Sher, Alexander and Litke, Alan M. and Chichilnisky, E. J. and Simoncelli, Eero P.}, year={2008}, month=aug, pages={995–999}, language={en} }
@article{Platkiewicz_Brette_2010, title={A Threshold Equation for Action Potential Initiation}, volume={6}, ISSN={1553-7358}, DOI={10.1371/journal.pcbi.1000850}, abstractNote={In central neurons, the threshold for spike initiation can depend on the stimulus and varies between cells and between recording sites in a given cell, but it is unclear what mechanisms underlie this variability. Properties of ionic channels are likely to play a role in threshold modulation. We examined in models the influence of Na channel activation, inactivation, slow voltage-gated channels and synaptic conductances on spike threshold. We propose a threshold equation which quantifies the contribution of all these mechanisms. It provides an instantaneous time-varying value of the threshold, which applies to neurons with fluctuating inputs. We deduce a differential equation for the threshold, similar to the equations of gating variables in the Hodgkin-Huxley formalism, which describes how the spike threshold varies with the membrane potential, depending on channel properties. We find that spike threshold depends logarithmically on Na channel density, and that Na channel inactivation and K channels can dynamically modulate it in an adaptive way: the threshold increases with membrane potential and after every action potential. Our equation was validated with simulations of a previously published multicompartemental model of spike initiation. Finally, we observed that threshold variability in models depends crucially on the shape of the Na activation function near spike initiation (about 255 mV), while its parameters are adjusted near half-activation voltage (about 230 mV), which might explain why many models exhibit little threshold variability, contrary to experimental observations. We conclude that ionic channels can account for large variations in spike threshold.}, number={7}, journal={PLoS Computational Biology}, author={Platkiewicz, Jonathan and Brette, Romain}, editor={Graham, Lyle J.}, year={2010}, month=jul, pages={e1000850}, language={en} }
@article{Plesser_Gerstner_2000, title={Noise in Integrate-and-Fire Neurons: From Stochastic Input to Escape Rates}, volume={12}, ISSN={0899-7667}, DOI={10.1162/089976600300015835}, abstractNote={We analyze the effect of noise in integrate-and-fire neurons driven by time-dependent input and compare the diffusion approximation for the membrane potential to escape noise. It is shown that for time-dependent subthreshold input, diffusive noise can be replaced by escape noise with a hazard function that has a gaussian dependence on the distance between the (noise-free) membrane voltage and threshold. The approximation is improved if we add to the hazard function a probability current proportional to the derivative of the voltage. Stochastic resonance in response to periodic input occurs in both noise models and exhibits similar characteristics.}, number={2}, journal={Neural Computation}, author={Plesser, Hans E. and Gerstner, Wulfram}, year={2000}, month=feb, pages={367–384} }
@article{Pradhan_Yüksel_2023, title={Controlled Diffusions under Full, Partial and Decentralized Information: Existence of Optimal Policies and Discrete-Time Approximations}, journal={preprint}, url={http://arxiv.org/abs/2311.03254}, abstractNote={We present existence and discrete-time approximation results on optimal control policies for continuous-time stochastic control problems under a variety of information structures. These include fully observed models, partially observed models and multi-agent models with decentralized information structures. While there exist comprehensive existence and approximations results for the fully observed setup in the literature, few prior research exists on discrete-time approximation results for partially observed models. For decentralized models, even existence results have not received much attention except for specialized models and approximation has been an open problem. Our existence and approximations results lead to the applicability of well-established partially observed Markov decision processes and the relatively more mature theory of discrete-time decentralized stochastic control to be applicable for computing near optimal solutions for continuous-time stochastic control.}, number={arXiv:2311.03254}, publisher={arXiv}, author={Pradhan, Somnath and Yüksel, Serdar}, year={2023}, month=nov, language={en} }
@article{Prinz_Billimoria_Marder_2003, title={Alternative to hand-tuning conductance-based models: construction and analysis of databases of model neurons}, volume={90}, ISSN={0022-3077}, DOI={10.1152/jn.00641.2003}, abstractNote={Conventionally, the parameters of neuronal models are hand-tuned using trial-and-error searches to produce a desired behavior. Here, we present an alternative approach. We have generated a database of about 1.7 million single-compartment model neurons by independently varying 8 maximal membrane conductances based on measurements from lobster stomatogastric neurons. We classified the spontaneous electrical activity of each model neuron and its responsiveness to inputs during runtime with an adaptive algorithm and saved a reduced version of each neuron’s activity pattern. Our analysis of the distribution of different activity types (silent, spiking, bursting, irregular) in the 8-dimensional conductance space indicates that the coarse grid of conductance values we chose is sufficient to capture the salient features of the distribution. The database can be searched for different combinations of neuron properties such as activity type, spike or burst frequency, resting potential, frequency-current relation, and phase-response curve. We demonstrate how the database can be screened for models that reproduce the behavior of a specific biological neuron and show that the contents of the database can give insight into the way a neuron’s membrane conductances determine its activity pattern and response properties. Similar databases can be constructed to explore parameter spaces in multicompartmental models or small networks, or to examine the effects of changes in the voltage dependence of currents. In all cases, database searches can provide insight into how neuronal and network properties depend on the values of the parameters in the models.}, number={6}, journal={Journal of Neurophysiology}, author={Prinz, Astrid A. and Billimoria, Cyrus P. and Marder, Eve}, year={2003}, month=dec, pages={3998–4015}, language={eng} }
@article{Reynaud-Bouret_Rivoirard_Grammont_Tuleau-Malot_2014, title={Goodness-of-Fit Tests and Nonparametric Adaptive Estimation for Spike Train Analysis}, volume={4}, ISSN={2190-8567}, DOI={10.1186/2190-8567-4-3}, abstractNote={When dealing with classical spike train analysis, the practitioner often performs goodness-of-fit tests to test whether the observed process is a Poisson process, for instance, or if it obeys another type of probabilistic model (Yana et al. in Biophys. J. 46(3):323–330, 1984; Brown et al. in Neural Comput. 14(2):325–346, 2002; Pouzat and Chaffiol in Technical report, http://arxiv.org/abs/arXiv:0909.2785, 2009). In doing so, there is a fundamental plug-in step, where the parameters of the supposed underlying model are estimated. The aim of this article is to show that plug-in has sometimes very undesirable effects. We propose a new method based on subsampling to deal with those plug-in issues in the case of the Kolmogorov–Smirnov test of uniformity. The method relies on the plug-in of good estimates of the underlying model that have to be consistent with a controlled rate of convergence. Some nonparametric estimates satisfying those constraints in the Poisson or in the Hawkes framework are highlighted. Moreover, they share adaptive properties that are useful from a practical point of view. We show the performance of those methods on simulated data. We also provide a complete analysis with these tools on single unit activity recorded on a monkey during a sensory-motor task.}, number={1}, journal={The Journal of Mathematical Neuroscience}, author={Reynaud-Bouret, Patricia and Rivoirard, Vincent and Grammont, Franck and Tuleau-Malot, Christine}, year={2014}, month=apr, pages={3} }
@article{Rinzel_Ermentrout_1998, title={Analysis of Neural Excitability and Oscillations}, journal={Methods of Neuronal Modeling}, author={Rinzel, John and Ermentrout, Bard}, year={1998}, month=jan }
@article{Sanjari_Saldi_Yüksel_2023, title={Optimality of Independently Randomized Symmetric Policies for Exchangeable Stochastic Teams with Infinitely Many Decision Makers}, volume={48}, ISSN={0364-765X}, DOI={10.1287/moor.2022.1296}, abstractNote={We study stochastic teams (known also as decentralized stochastic control problems or identical interest stochastic dynamic games) with large or countably infinite numbers of decision makers and characterize the existence and structural properties of (globally) optimal policies. We consider both static and dynamic nonconvex teams where the cost function and dynamics satisfy an exchangeability condition. To arrive at existence and structural results for optimal policies, we first introduce a topology on control policies, which involves various relaxations given the decentralized information structure. This is then utilized to arrive at a de Finetti–type representation theorem for exchangeable policies. This leads to a representation theorem for policies that admit an infinite exchangeability condition. For a general setup of stochastic team problems with N decision makers, under exchangeability of observations of decision makers and the cost function, we show that, without loss of global optimality, the search for optimal policies can be restricted to those that are N-exchangeable. Then, by extending N-exchangeable policies to infinitely exchangeable ones, establishing a convergence argument for the induced costs, and using the presented de Finetti–type theorem, we establish the existence of an optimal decentralized policy for static and dynamic teams with countably infinite numbers of decision makers, which turns out to be symmetric (i.e., identical) and randomized. In particular, unlike in prior work, convexity of the cost in policies is not assumed. Finally, we show the near optimality of symmetric independently randomized policies for finite N-decision-maker teams and thus establish approximation results for N-decision-maker weakly coupled stochastic teams. Funding: This work was supported by Natural Sciences and Engineering Research Council of Canada.}, number={3}, journal={Mathematics of Operations Research}, publisher={INFORMS}, author={Sanjari, Sina and Saldi, Naci and Yüksel, Serdar}, year={2023}, month=aug, pages={1254–1285} }
@article{Sanz-Leon_Knock_Spiegler_Jirsa_2015, title={Mathematical framework for large-scale brain network modeling in The Virtual Brain}, volume={111}, ISSN={1053-8119}, DOI={10.1016/j.neuroimage.2015.01.002}, abstractNote={In this article, we describe the mathematical framework of the computational model at the core of the tool The Virtual Brain (TVB), designed to simulate collective whole brain dynamics by virtualizing brain structure and function, allowing simultaneous outputs of a number of experimental modalities such as electro- and magnetoencephalography (EEG, MEG) and functional Magnetic Resonance Imaging (fMRI). The implementation allows for a systematic exploration and manipulation of every underlying component of a large-scale brain network model (BNM), such as the neural mass model governing the local dynamics or the structural connectivity constraining the space time structure of the network couplings. Here, a consistent notation for the generalized BNM is given, so that in this form the equations represent a direct link between the mathematical description of BNMs and the components of the numerical implementation in TVB. Finally, we made a summary of the forward models implemented for mapping simulated neural activity (EEG, MEG, sterotactic electroencephalogram (sEEG), fMRI), identifying their advantages and limitations.}, journal={NeuroImage}, author={Sanz-Leon, Paula and Knock, Stuart A. and Spiegler, Andreas and Jirsa, Viktor K.}, year={2015}, month=may, pages={385–430} }
@article{Schmetterling_Burghi_Sepulchre_2022, title={Adaptive conductance control}, volume={54}, ISSN={1367-5788}, DOI={10.1016/j.arcontrol.2022.07.005}, abstractNote={Neuromodulation is central to the adaptation and robustness of animal nervous systems. This paper explores the classical paradigm of indirect adaptive control to design neuromodulatory controllers in conductance-based neuronal models. The adaptive control of maximal conductance parameters is shown to provide a methodology aligned with the central concepts of neuromodulation in physiology and of impedance control in robotics.}, journal={Annual Reviews in Control}, author={Schmetterling, Raphael and Burghi, Thiago B. and Sepulchre, Rodolphe}, year={2022}, month=jan, pages={352–362} }
@article{Schüz_Palm_1989, title={Density of neurons and synapses in the cerebral cortex of the mouse}, volume={286}, rights={Copyright © 1989 Alan R. Liss, Inc.}, ISSN={1096-9861}, DOI={10.1002/cne.902860404}, abstractNote={Quantitative anatomical investigations provide the basis for functional models. In this study the density of neurons and synapses was measured in three different areas (8, 6, and 17) of the neocortex of the mouse. Both kinds of measurements were made on the same material, embedded in Epon/Araldit. In order to determine the synaptic density per mm3, the proportion of synaptic neuropil was also measured; it was found to be 84%. The cortical volume occupied by cell bodies of neurons and glia cells amounted to 12%, that by blood vessels to 4%. The total average was 9.2 × 104 neurons/mm3 and 7.2 × 108 synapses/mm3. About 11% of the synapses were of type II. The density of neurons increased with decreasing cortical thickness; thus the number of neurons under a given surface area was about constant. The synaptic density, on the other hand, was almost constant in the three areas, the number of synapses under a given cortical surface area tended, therefore, to increase with cortical thickness. The average number of synapses per neuron was 8,200, with a tendency to increase with increasing cortical thickness. Shrinkage of the tissue was also measured for various staining techniques. No shrinkage occurred during perfusion with 3.7% formaldehyde or with a solution of buffered paraformaldehyde and glutaraldehyde and during fixation in situ. Electron microscopical material showed almost no shrinkage, whereas Nissl-preparations on paraffin-embedded material had only 43% of their original volume. After Nissl stain on frozen sections the volume had shrunken to 68% and after Golgi impregnation and embedding in celloidin to 70%. The total volume of the neocortex was 112 mm3 (both hemispheres together). The total number of neurons was thus 1.0 × 107 and the total number of synapses 8.1 × 1010.}, number={4}, journal={Journal of Comparative Neurology}, author={Schüz, Almut and Palm, Günther}, year={1989}, pages={442–455}, language={en} }
@article{Sepulchre_2022, title={Spiking Control Systems}, volume={110}, ISSN={1558-2256}, DOI={10.1109/JPROC.2022.3163926}, abstractNote={Spikes and rhythms organize control and communication in the animal world, in contrast to the bits and clocks of digital technology. As continuous-time signals that can be counted, spikes have a mixed nature. This article reviews ongoing efforts to develop a control theory of spiking systems. The central thesis is that the mixed nature of spiking results from a mixed feedback principle, and a control theory of mixed feedback can be grounded in the operator theoretic concept of maximal monotonicity. As a nonlinear generalization of passivity, maximal monotonicity acknowledges at once the physics of electrical circuits, the algorithmic tractability of convex optimization, and the feedback control theory of incremental passivity. We discuss the relevance of a theory of spiking control systems in the emerging age of event-based technology.}, number={5}, journal={Proceedings of the IEEE}, author={Sepulchre, Rodolphe}, year={2022}, month=may, pages={577–589} }
@article{Strong_et_al_1998, title={Entropy and Information in Neural Spike Trains}, volume={80}, DOI={10.1103/PhysRevLett.80.197}, abstractNote={The nervous system represents time dependent signals in sequences of discrete, identical action potentials or spikes; information is carried only in the spike arrival times. We show how to quantify this information, in bits, free from any assumptions about which features of the spike train or input signal are most important, and we apply this approach to the analysis of experiments on a motion sensitive neuron in the fly visual system. This neuron transmits information about the visual stimulus at rates of up to 90 bits/s, within a factor of 2 of the physical limit set by the entropy of the spike train itself.}, number={1}, journal={Physical Review Letters}, publisher={American Physical Society}, author={Strong, S. P. and Koberle, Roland and de Ruyter van Steveninck, Rob R. and Bialek, William}, year={1998}, month=jan, pages={197–200} }
@article{Lovász_Szegedy_2007, title={Szemerédi’s Lemma for the Analyst}, volume={17}, rights={http://www.springer.com/tdm}, ISSN={1016-443X, 1420-8970}, DOI={10.1007/s00039-007-0599-6}, abstractNote={Szemer´edi’s Regularity Lemma is a fundamental tool in graph theory: it has many applications to extremal graph theory, graph property testing, combinatorial number theory, etc. The goal of this paper is to point out that Szemer´edi’s Lemma can be thought of as a result in analysis. We show three diﬀerent analytic interpretations.}, number={1}, journal={GAFA Geometric And Functional Analysis}, author={Lovász, László and Szegedy, Balázs}, year={2007}, month=apr, pages={252–270}, language={en} }
@article{Touboul_2009, title={Importance of the cutoff value in the quadratic adaptive integrate-and-fire model}, volume={21}, ISSN={0899-7667}, DOI={10.1162/neco.2009.09-08-853}, abstractNote={The quadratic adaptive integrate-and-fire model (Izhikevich, 2003 , 2007 ) is able to reproduce various firing patterns of cortical neurons and is widely used in large-scale simulations of neural networks. This model describes the dynamics of the membrane potential by a differential equation that is quadratic in the voltage, coupled to a second equation for adaptation. Integration is stopped during the rise phase of a spike at a voltage cutoff value V(c) or when it blows up. Subsequently the membrane potential is reset, and the adaptation variable is increased by a fixed amount. We show in this note that in the absence of a cutoff value, not only the voltage but also the adaptation variable diverges in finite time during spike generation in the quadratic model. The divergence of the adaptation variable makes the system very sensitive to the cutoff: changing V(c) can dramatically alter the spike patterns. Furthermore, from a computational viewpoint, the divergence of the adaptation variable implies that the time steps for numerical simulation need to be small and adaptive. However, divergence of the adaptation variable does not occur for the quartic model (Touboul, 2008 ) and the adaptive exponential integrate-and-fire model (Brette & Gerstner, 2005 ). Hence, these models are robust to changes in the cutoff value.}, number={8}, journal={Neural Computation}, author={Touboul, Jonathan}, year={2009}, month=aug, pages={2114–2122}, language={eng} }
@article{Uhlhaas_Singer_2006, title={Neural Synchrony in Brain Disorders: Relevance for Cognitive Dysfunctions and Pathophysiology}, volume={52}, ISSN={0896-6273}, DOI={10.1016/j.neuron.2006.09.020}, number={1}, journal={Neuron}, publisher={Elsevier}, author={Uhlhaas, Peter J. and Singer, Wolf}, year={2006}, month=oct, pages={155–168}, language={English} }
@article{Walch_Eisenberg_2016, title={Parameter identifiability and identifiable combinations in generalized Hodgkin–Huxley models}, volume={199}, ISSN={09252312}, DOI={10.1016/j.neucom.2016.03.027}, abstractNote={Semantic Scholar extracted view of “Parameter identifiability and identifiable combinations in generalized Hodgkin-Huxley models” by Olivia J. Walch et al.}, journal={Neurocomputing}, author={Walch, Olivia J. and Eisenberg, Marisa C.}, year={2016}, month=jul, pages={137–143}, language={en} }
@article{Wang_Wang_Yu_Chen_2016, title={Spike-Threshold Variability Originated from Separatrix-Crossing in Neuronal Dynamics}, volume={6}, rights={2016 The Author(s)}, ISSN={2045-2322}, DOI={10.1038/srep31719}, abstractNote={The threshold voltage for action potential generation is a key regulator of neuronal signal processing, yet the mechanism of its dynamic variation is still not well described. In this paper, we propose that threshold phenomena can be classified as parameter thresholds and state thresholds. Voltage thresholds which belong to the state threshold are determined by the ‘general separatrix’ in state space. We demonstrate that the separatrix generally exists in the state space of neuron models. The general form of separatrix was assumed as the function of both states and stimuli and the previously assumed threshold evolving equation versus time is naturally deduced from the separatrix. In terms of neuronal dynamics, the threshold voltage variation, which is affected by different stimuli, is determined by crossing the separatrix at different points in state space. We suggest that the separatrix-crossing mechanism in state space is the intrinsic dynamic mechanism for threshold voltages and post-stimulus threshold phenomena. These proposals are also systematically verified in example models, three of which have analytic separatrices and one is the classic Hodgkin-Huxley model. The separatrix-crossing framework provides an overview of the neuronal threshold and will facilitate understanding of the nature of threshold variability.}, number={1}, journal={Scientific Reports}, publisher={Nature Publishing Group}, author={Wang, Longfei and Wang, Hengtong and Yu, Lianchun and Chen, Yong}, year={2016}, month=aug, pages={31719}, language={en} }
@article{Wilson_Cowan_1973, title={A mathematical theory of the functional dynamics of cortical and thalamic nervous tissue}, volume={13}, ISSN={1432-0770}, DOI={10.1007/BF00288786}, abstractNote={It is proposed that distinct anatomical regions of cerebral cortex and of thalamic nuclei are functionally two-dimensional. On this view, the third (radial) dimension of cortical and thalamic structures is associated with a redundancy of circuits and functions so that reliable signal processing obtains in the presence of noisy or ambiguous stimuli.}, number={2}, journal={Kybernetik}, author={Wilson, H. R. and Cowan, J. D.}, year={1973}, month=sep, pages={55–80}, language={en} }
@article{White_Southgate_Thomson_Brenner_1986, title={The structure of the nervous system of the nematode Caenorhabditis elegans}, volume={314}, ISSN={0962-8436}, DOI={10.1098/rstb.1986.0056}, abstractNote={The structure and connectivity of the nervous system of the nematode Caenorhabditis elegans has been deduced from reconstructions of electron micrographs of serial sections. The hermaphrodite nervous system has a total complement of 302 neurons, which are arranged in an essentially invariant structure. Neurons with similar morphologies and connectivities have been grouped together into classes; there are 118 such classes. Neurons have simple morphologies with few, if any, branches. Processes from neurons run in defined positions within bundles of parallel processes, synaptic connections being made en passant. Process bundles are arranged longitudinally and circumferentially and are often adjacent to ridges of hypodermis. Neurons are generally highly locally connected, making synaptic connections with many of their neighbours. Muscle cells have arms that run out to process bundles containing motoneuron axons. Here they receive their synaptic input in defined regions along the surface of the bundles, where motoneuron axons reside. Most of the morphologically identifiable synaptic connections in a typical animal are described. These consist of about 5000 chemical synapses, 2000 neuromuscular junctions and 600 gap junctions.}, number={1165}, journal={Philosophical Transactions of the Royal Society of London. Series B, Biological Sciences}, author={White, J. G. and Southgate, E. and Thomson, J. N. and Brenner, S.}, year={1986}, month=nov, pages={1–340}, language={eng} }
@inproceedings{Witsenhausen_1975, address={Berlin, Heidelberg}, title={The Intrinsic Model for Discrete Stochastic Control: Some Open Problems}, ISBN={978-3-642-46317-4}, DOI={10.1007/978-3-642-46317-4_24}, abstractNote={An important difference between deterministic and stochastic control is that in the stochastic case much depends on what data are available for each control decision. In the simplest case all control decisions are made from one station where also all data are gathered. The station remembers all its observations and can base each decision on all data gathered up to the time at which the decision must be made. However not all systems are completely centralized in this fashion nor is it always true that unlimited memory is available to store data. There may be several control stations communicating only by signaling through the system itself or through noisy channels which should be considered part of the system. Likewise the storage devices for data are also parts of the system to be controlled. In this view each control selection is a separate event, the selection being based on data provided out of the system specifically for that event. Whether this data comes from sensors, communication channels or from memory devices is only a matter of specific detail.}, booktitle={Control Theory, Numerical Methods and Computer Systems Modelling}, publisher={Springer}, author={Witsenhausen, H. S.}, editor={Bensoussan, A. and Lions, J. L.}, year={1975}, pages={322–335}, language={en} }
@article{Witsenhausen_1988, title={Equivalent stochastic control problems}, volume={1}, ISSN={1435-568X}, DOI={10.1007/BF02551232}, abstractNote={A decentralized stochastic control problem is called static if the observations available for any one decision do not depend on the other decisions. Otherwise it is called dynamic. We consider only problems with a finite number of decisions. A notion of equivalence between problems, suitable for complexity analysis, is defined. It turns out that a large class of dynamic problems can be reduced to equivalent static problems. The class includes all sequential discrete variable problems and some of the most studied continuous variable problems.}, number={1}, journal={Mathematics of Control, Signals and Systems}, author={Witsenhausen, H. S.}, year={1988}, month=feb, pages={3–11}, language={en} }
@inproceedings{Yuksel_2018, address={Miami Beach, FL}, title={Witsenhausen’s Standard Form Revisited: A General Dynamic Program for Decentralized Stochastic Control and Existence Results}, ISBN={978-1-5386-1395-5}, url={https://ieeexplore.ieee.org/document/8619226/}, DOI={10.1109/CDC.2018.8619226}, abstractNote={For sequential stochastic control problems with standard Borel measurement and control action spaces, we introduce a general dynamic programming formulation, establish its well-posedness, and provide new existence results for optimal policies. Our dynamic program builds in part on Witsenhausen’s standard form, but with a different formulation for the state, action, and transition dynamics. Using recent results on measurability properties of strategic measures in decentralized control, we obtain a controlled Markov model with standard Borel state and state dependent action sets. This allows for a well-posed formulation for the controlled Markov model for a general class of sequential decentralized stochastic control in that it leads to well-deﬁned dynamic programming recursions through universal measurability properties of the value functions for each time stage. In addition, new existence results are obtained for optimal team policies in decentralized stochastic control. These state that for a static team with independent measurements, it sufﬁces for the cost function to be continuous (only) in the actions for the existence of an optimal policy under mild compactness conditions. These also apply to dynamic teams which admit static reductions with independent measurements.}, booktitle={2018 IEEE Conference on Decision and Control (CDC)}, publisher={IEEE}, author={Yüksel, Serdar}, year={2018}, month=dec, pages={5051–5056}, language={en} }
@book{Ermentrout_Terman_2010, author={Ermentrout, B and Terman, D}, year={2010}, title={Mathematical foundations of neuroscience}, publisher={Springer}, address={New York, NY} }
@book{Penfield_Jasper_1954, author={Penfield, W and Penfield, J}, year={1954}, title={Epilepsy and the functional anatomy of the human brain}, publisher={Little, Brown \& Co.}, address={Oxford, England} }
@book{Sterling_Laughlin_2015, author={Sterling, Peter and Laughlin, Simon}, year={2015}, title={Principles of Neural Design}, publisher={The MIT Press}, isbn={9780262327312}, doi={10.7551/mitpress/9780262028707.001.0001}, url={https://doi.org/10.7551/mitpress/9780262028707.001.0001} }
@book{Izhikevich_2007, author={Izhikevich, Eugene M}, year={2006}, title={Dynamical Systems in Neuroscience: The Geometry of Excitability and Bursting}, publisher={The MIT Press}, isbn={9780262276078}, doi={10.7551/mitpress/2526.001.0001}, url={https://doi.org/10.7551/mitpress/2526.001.0001} }
@book{Gerstner_Kistler_Naud_Paninski_2014, place={Cambridge}, title={Neuronal Dynamics: From Single Neurons to Networks and Models of Cognition}, publisher={Cambridge University Press}, author={Gerstner, Wulfram and Kistler, Werner M. and Naud, Richard and Paninski, Liam}, year={2014}}
@book{Yuksel_Basar_2024, author={Yüksel, Serdar and Başar, Tamer}, title={Stochastic Teams, Games, and Control under Information Constraints}, year={2024}, publisher={Birkhäuser Cham}, isbn={9783031540707}, doi={https://doi.org/10.1007/978-3-031-54071-4} }
@book{Hernandez-Lerma_Lasserre_1996, address={New York, NY}, title={Discrete-Time Markov Control Processes}, rights={http://www.springer.com/tdm}, ISBN={978-1-4612-6884-0}, url={http://link.springer.com/10.1007/978-1-4612-0729-0}, DOI={10.1007/978-1-4612-0729-0}, publisher={Springer}, author={Hernández-Lerma, Onésimo and Lasserre, Jean Bernard}, year={1996} }
@book{Bertsekas_Shreve_2007, author = {Bertsekas, Dimitri P. and Shreve, Steven E.}, title = {Stochastic Optimal Control: The Discrete-Time Case}, year = {2007}, isbn = {1886529035}, publisher = {Athena Scientific} }
@book{Bremaud_1981, author={Br{\'e}maud, Pierre}, year={1981}, title={Point Processes and Queues: Martingale Dynamics}, address={New York, NY}, publisher={Springer-Verlag}}
@book{Ikeda_Watanabe_1989, author={Ikeda, Nobuyuki and Watanabe, Shinzo}, year={1989}, title={Stochastic Differential Equations and Diffusion Processes}, isbn={0-444-873783}, publisher={Kodansha ltd.}, address={Tokyo, Japan} }
@book{Doya_et_al_2006, author = {Doya, Kenji and Ishii, Shin and Pouget, Alexandre and Rao, Rajesh P.N.}, title = "{Bayesian Brain: Probabilistic Approaches to Neural Coding}", publisher = {The MIT Press}, year = {2006}, month = {12}, isbn = {9780262042383}, doi = {10.7551/mitpress/9780262042383.001.0001}, url = {https://doi.org/10.7551/mitpress/9780262042383.001.0001} }
@book{Platen_Bruti_Liberati_2010, author={Platen, Eckhard and Bruti-Liberati, Nicola}, year={2010}, title={Numerical Solution of Stochastic Differential Equations with Jumps in Finance}, publisher={Springer}, address={Berlin, Heidelberg} }
@book{Johnston_Wu_1995, author={Johnston, Daniel and Wu, Samuel}, year={1995}, title={Foundations of Cellular Neurophysiology}, publisher={The MIT Press}, address={Cambridge, MA}}
@book{Lovasz_2012, author={Lov\'asz, L\'aszl\'o}, title={Large Networks and Graph Limits}, publisher={American Mathematical Society}, year={2012}, volume={60}}
@article{Parr_Markovic_Kiebel_Friston_2019, title={Neuronal message passing using Mean-field, Bethe, and Marginal approximations}, volume={9}, ISSN={2045-2322}, DOI={10.1038/s41598-018-38246-3}, abstractNote={Neuronal computations rely upon local interactions across synapses. For a neuronal network to perform inference, it must integrate information from locally computed messages that are propagated among elements of that network. We review the form of two popular (Bayesian) message passing schemes and consider their plausibility as descriptions of inference in biological networks. These are variational message passing and belief propagation - each of which is derived from a free energy functional that relies upon different approximations (mean-field and Bethe respectively). We begin with an overview of these schemes and illustrate the form of the messages required to perform inference using Hidden Markov Models as generative models. Throughout, we use factor graphs to show the form of the generative models and of the messages they entail. We consider how these messages might manifest neuronally and simulate the inferences they perform. While variational message passing offers a simple and neuronally plausible architecture, it falls short of the inferential performance of belief propagation. In contrast, belief propagation allows exact computation of marginal posteriors at the expense of the architectural simplicity of variational message passing. As a compromise between these two extremes, we offer a third approach - marginal message passing - that features a simple architecture, while approximating the performance of belief propagation. Finally, we link formal considerations to accounts of neurological and psychiatric syndromes in terms of aberrant message passing.}, number={1}, journal={Scientific Reports}, author={Parr, Thomas and Markovic, Dimitrije and Kiebel, Stefan J. and Friston, Karl J.}, year={2019}, month=feb, pages={1889}, language={eng} }
@article{Sanjari_Saldi_Yüksel_2024, title={Optimality of Decentralized Symmetric Policies for Stochastic Teams with Mean-Field Information Sharing}, url={http://arxiv.org/abs/2404.04957}, DOI={10.48550/arXiv.2404.04957}, abstractNote={We study a class of stochastic exchangeable teams comprising a finite number of decision makers (DMs) as well as their mean-field limits involving infinite numbers of DMs. In the finite population regime, we study exchangeable teams under the centralized information structure. For the infinite population setting, we study exchangeable teams under the decentralized mean-field information sharing. The paper makes the following main contributions: i) For finite population exchangeable teams, we establish the existence of a randomized optimal policy that is exchangeable (permutation invariant) and Markovian; ii) As our main result in the paper, we show that a sequence of exchangeable optimal policies for finite population settings converges to a conditionally symmetric (identical), independent, and decentralized randomized policy for the infinite population problem, which is globally optimal for the infinite population problem. This result establishes the existence of a symmetric, independent, decentralized optimal randomized policy for the infinite population problem. Additionally, this proves the optimality of the limiting measure-valued MDP for the representative DM; iii) Finally, we show that symmetric, independent, decentralized optimal randomized policies are approximately optimal for the corresponding finite-population team with a large number of DMs under the centralized information structure. Our paper thus establishes the relation between the controlled McKean-Vlasov dynamics and the optimal infinite population decentralized stochastic control problem (without an apriori restriction of symmetry in policies of individual agents), for the first time, to our knowledge.}, number={arXiv:2404.04957}, publisher={arXiv}, author={Sanjari, Sina and Saldi, Naci and Yüksel, Serdar}, year={2024}, month=apr }
@article{Yüksel_Saldi_2017, title={Convex Analysis in Decentralized Stochastic Control, Strategic Measures, and Optimal Solutions}, volume={55}, ISSN={0363-0129}, DOI={10.1137/15M1049129}, abstractNote={This paper studies convex stochastic dynamic team problems with finite and infinite time horizons under decentralized information structures. First, we introduce two notions called exchangeable teams and symmetric information structures. We show that in convex exchangeable team problems an optimal policy exhibits a symmetry structure. We give a characterization for such symmetrically optimal teams for a general class of convex dynamic team problems under a mild conditional independence condition. In addition, through concentration of measure arguments, we establish the convergence of optimal policies for teams with $N$ decision makers to the corresponding optimal policies for symmetric mean-field teams with infinitely many decision makers.  As a by-product, we present an existence result for convex mean-field teams, where the main contribution of our paper is with respect to the information structure in the system when compared with the related results in the literature that have assumed either a classical information structure or a static information structure. We also apply these results to the important special case of linear quadratic Gaussian (LQG) team problems, where while for partially nested LQG team problems with finite time horizons it is known that the optimal policies are linear, for infinite horizon problems the linearity of optimal policies has not been established in full generality. We also study average cost finite and infinite horizon dynamic team problems with a symmetric partially nested information structure and obtain globally optimal solutions where we establish linearity of optimal policies.}, number={1}, journal={SIAM Journal on Control and Optimization}, publisher={Society for Industrial and Applied Mathematics}, author={Yüksel, Serdar and Saldi, Naci}, year={2017}, month=jan, pages={1–28} }
@article{Saldi_Yüksel_2022, title={Geometry of information structures, strategic measures and associated stochastic control topologies}, volume={19}, ISSN={1549-5787, 1549-5787}, DOI={10.1214/20-PS356}, abstractNote={In many areas of applied mathematics, decentralization of information is a ubiquitous attribute affecting how to approach a stochastic optimization, decision and estimation, or control problem. In this review article, we present a general formulation of information structures under a probability theoretic and geometric formulation. We define information structures, place various topologies on them, and study closedness, compactness and convexity properties on the strategic measures induced by information structures and decentralized control/decision policies under varying degrees of relaxations with regard to access to private or common randomness. Ultimately, we present existence and tight approximation results for optimal decision/control policies. We discuss various lower bounding techniques, through relaxations and convex programs ranging from classically realizable and classically non-realizable (such as quantum theoretic and non-signaling) relaxations. For each of these, we establish closedness and convexity properties and also a hierarchy of correlation structures. As a further theme, we review and introduce various topologies on decision/control strategies defined independent of information structures, but for which information structures determine whether the topologies entail utility in arriving at existence, compactness, convexification or approximation results. These approaches, which we term as the strategic measures approach and the control topology approach, lead to complementary results on existence, approximations and upper and lower bounds in optimal decentralized stochastic decision, estimation, and control problems.}, number={none}, journal={Probability Surveys}, publisher={Institute of Mathematical Statistics and Bernoulli Society}, author={Saldi, Naci and Yüksel, Serdar}, year={2022}, month=jan, pages={450–532} }
@book{Aldous_Ibragimov_1985, year={1985}, author={Aldous, David and Ibragimov, Illdar and Jacod, Jean}, title={Ecole d'Ete de Probabilites de Saint-Flour XIII, 1983}, publisher={Springer}}
@article{Sanjari_Yüksel_2021, title={Optimal Policies for Convex Symmetric Stochastic Dynamic Teams and their Mean-Field Limit}, volume={59}, ISSN={0363-0129}, DOI={10.1137/19M1284294}, abstractNote={In this paper, we identify sufficient conditions under which static teams and a class of sequential dynamic teams admit team-optimal solutions. We first investigate the existence of optimal solutions in static teams where the observations of the decision makers are conditionally independent given the state and satisfy certain regularity conditions. Building on these findings and the static reduction method of Witsenhausen, we then extend the analysis to sequential dynamic teams. In particular, we show that a large class of dynamic linear-quadratic-Gaussian (LQG) teams, including the vector version of the well-known Witsenhausen’s counterexample and the Gaussian relay channel problem viewed as a dynamic team, admit team-optimal solutions. Results in this paper substantially broaden the class of stochastic control problems with nonclassical information known to have optimal solutions.}, number={2}, journal={SIAM Journal on Control and Optimization}, publisher={Society for Industrial and Applied Mathematics}, author={Sanjari, Sina and Yüksel, Serdar}, year={2021}, month=jan, pages={777–804} }
@book{Kallenberg_2005, title={Probabilistic Symmetries and Invariance Principles}, author={Kallenberg, Olav}, year={2005}, publisher={Springer Science \& Business Media}}
@book{Dudley_2002, author={Dudley, Richard}, year={2002}, title={Real Analysis and Probability}, publisher={Cambridge University Press}}
@article{Prokhorov_1956, address={Philadelphia, United States}, title={Convergence of Random Processes and Limit Theorems in Probability Theory}, volume={1}, rights={[Copyright] © 1956 © Society for Industrial and Applied Mathematics}, ISSN={0040585X}, DOI={10.1137/1101016}, number={2}, journal={Theory of Probability and its Applications}, publisher={Society for Industrial and Applied Mathematics}, author={Prokhorov, Yu V.}, year={1956}, pages={58}, language={English} }
@article{D’Angelo_Mazzarello_Prestori_Mapelli_Solinas_Lombardo_Cesana_Gandolfi_Congi_2011, series={Camillo Golgi and Modern Neuroscience}, title={The cerebellar network: From structure to function and dynamics}, volume={66}, ISSN={0165-0173}, DOI={10.1016/j.brainresrev.2010.10.002}, abstractNote={Since the discoveries of Camillo Golgi and Ramón y Cajal, the precise cellular organization of the cerebellum has inspired major computational theories, which have then influenced the scientific thought not only on the cerebellar function but also on the brain as a whole. However, six major issues revealing a discrepancy between morphologically inspired hypothesis and function have emerged. (1) The cerebellar granular layer does not simply operate a simple combinatorial decorrelation of the inputs but performs more complex non-linear spatio-temporal transformations and is endowed with synaptic plasticity. (2) Transmission along the ascending axon and parallel fibers does not lead to beam formation but rather to vertical columns of activation. (3) The olivo-cerebellar loop could perform complex timing operations rather than error detection and teaching. (4) Purkinje cell firing dynamics are much more complex than for a linear integrator and include pacemaking, burst–pause discharges, and bistable states in response to mossy and climbing fiber synaptic inputs. (5) Long-term synaptic plasticity is far more complex than traditional parallel fiber LTD and involves also other cerebellar synapses. (6) Oscillation and resonance could set up coherent cycles of activity designing a functional geometry that goes far beyond pre-wired anatomical circuits. These observations clearly show that structure is not sufficient to explain function and that a precise knowledge on dynamics is critical to understand how the cerebellar circuit operates.}, number={1}, journal={Brain Research Reviews}, author={D’Angelo, E. and Mazzarello, P. and Prestori, F. and Mapelli, J. and Solinas, S. and Lombardo, P. and Cesana, E. and Gandolfi, D. and Congi, L.}, year={2011}, month=jan, pages={5–15}, collection={Camillo Golgi and Modern Neuroscience} }
@article{Iolov_Ditlevsen_Longtin_2014, title={Stochastic optimal control of single neuron spike trains}, volume={11}, ISSN={1741-2560, 1741-2552}, DOI={10.1088/1741-2560/11/4/046004}, abstractNote={Objective. External control of spike times in single neurons can reveal important information about a neuron’s sub-threshold dynamics that lead to spiking, and has the potential to improve brain–machine interfaces and neural prostheses. The goal of this paper is the design of optimal electrical stimulation of a neuron to achieve a target spike train under the physiological constraint to not damage tissue. Approach. We pose a stochastic optimal control problem to precisely specify the spike times in a leaky integrate-and-ﬁre (LIF) model of a neuron with noise assumed to be of intrinsic or synaptic origin. In particular, we allow for the noise to be of arbitrary intensity. The optimal control problem is solved using dynamic programming when the controller has access to the voltage (closed-loop control), and using a maximum principle for the transition density when the controller only has access to the spike times (open-loop control). Main results. We have developed a stochastic optimal control algorithm to obtain precise spike times. It is applicable in both the supra-threshold and sub-threshold regimes, under open-loop and closed-loop conditions and with an arbitrary noise intensity; the accuracy of control degrades with increasing intensity of the noise. Simulations show that our algorithms produce the desired results for the LIF model, but also for the case where the neuron dynamics are given by more complex models than the LIF model. This is illustrated explicitly using the Morris–Lecar spiking neuron model, for which an LIF approximation is ﬁrst obtained from a spike sequence using a previously published method. We further show that a related control strategy based on the assumption that there is no noise performs poorly in comparison to our noise-based strategies. The algorithms are numerically intensive and may require efﬁciency reﬁnements to achieve real-time control; in particular, the open-loop context is more numerically demanding than the closed-loop one. Signiﬁcance. Our main contribution is the online feedback control of a noisy neuron through modulation of the input, taking into account physiological constraints on the control. A precise and robust targeting of neural activity based on stochastic optimal control has great potential for regulating neural activity in e.g. prosthetic applications and to improve our understanding of the basic mechanisms by which neuronal ﬁring patterns can be controlled in vivo.}, number={4}, journal={Journal of Neural Engineering}, author={Iolov, Alexandre and Ditlevsen, Susanne and Longtin, André}, year={2014}, month=aug, pages={046004}, language={en} }
@article{Ahmadian_Packer_Yuste_Paninski_2011, title={Designing optimal stimuli to control neuronal spike timing}, volume={106}, ISSN={0022-3077}, DOI={10.1152/jn.00427.2010}, abstractNote={Recent advances in experimental stimulation methods have raised the following important computational question: how can we choose a stimulus that will drive a neuron to output a target spike train with optimal precision, given physiological constraints? Here we adopt an approach based on models that describe how a stimulating agent (such as an injected electrical current or a laser light interacting with caged neurotransmitters or photosensitive ion channels) affects the spiking activity of neurons. Based on these models, we solve the reverse problem of finding the best time-dependent modulation of the input, subject to hardware limitations as well as physiologically inspired safety measures, that causes the neuron to emit a spike train that with highest probability will be close to a target spike train. We adopt fast convex constrained optimization methods to solve this problem. Our methods can potentially be implemented in real time and may also be generalized to the case of many cells, suitable for neural prosthesis applications. With the use of biologically sensible parameters and constraints, our method finds stimulation patterns that generate very precise spike trains in simulated experiments. We also tested the intracellular current injection method on pyramidal cells in mouse cortical slices, quantifying the dependence of spiking reliability and timing precision on constraints imposed on the applied currents.}, number={2}, journal={Journal of Neurophysiology}, publisher={American Physiological Society}, author={Ahmadian, Yashar and Packer, Adam M. and Yuste, Rafael and Paninski, Liam}, year={2011}, month=aug, pages={1038–1053} }
@article{Osborne_1979, title={Is Dale’s principle valid?}, volume={2}, rights={https://www.elsevier.com/tdm/userlicense/1.0/}, ISSN={01662236}, DOI={10.1016/0166-2236(79)90031-6}, journal={Trends in Neurosciences}, author={Osborne, Neville N.}, year={1979}, month=jan, pages={73–75}, language={en} }
@article{Barranca_Bhuiyan_Sundgren_Xing_2022, title={Functional Implications of Dale’s Law in Balanced Neuronal Network Dynamics and Decision Making}, volume={16}, ISSN={1662-4548}, DOI={10.3389/fnins.2022.801847}, abstractNote={The notion that a neuron transmits the same set of neurotransmitters at all of its post-synaptic connections, typically known as Dale’s law, is well supported throughout the majority of the brain and is assumed in almost all theoretical studies investigating the mechanisms for computation in neuronal networks. Dale’s law has numerous functional implications in fundamental sensory processing and decision-making tasks, and it plays a key role in the current understanding of the structure-function relationship in the brain. However, since exceptions to Dale’s law have been discovered for certain neurons and because other biological systems with complex network structure incorporate individual units that send both positive and negative feedback signals, we investigate the functional implications of network model dynamics that violate Dale’s law by allowing each neuron to send out both excitatory and inhibitory signals to its neighbors. We show how balanced network dynamics, in which large excitatory and inhibitory inputs are dynamically adjusted such that input fluctuations produce irregular firing events, are theoretically preserved for a single population of neurons violating Dale’s law. We further leverage this single-population network model in the context of two competing pools of neurons to demonstrate that effective decision-making dynamics are also produced, agreeing with experimental observations from honeybee dynamics in selecting a food source and artificial neural networks trained in optimal selection. Through direct comparison with the classical two-population balanced neuronal network, we argue that the one-population network demonstrates more robust balanced activity for systems with less computational units, such as honeybee colonies, whereas the two-population network exhibits a more rapid response to temporal variations in network inputs, as required by the brain. We expect this study will shed light on the role of neurons violating Dale’s law found in experiment as well as shared design principles across biological systems that perform complex computations.}, journal={Frontiers in Neuroscience}, author={Barranca, Victor J. and Bhuiyan, Asha and Sundgren, Max and Xing, Fangzhou}, year={2022}, month=feb, pages={801847} }
@article{Herreras_2016, title={Local Field Potentials: Myths and Misunderstandings}, volume={10}, ISSN={1662-5110}, url={https://www.frontiersin.org/journals/neural-circuits/articles/10.3389/fncir.2016.00101/full}, DOI={10.3389/fncir.2016.00101}, abstractNote={<p>The intracerebral local field potential (LFP) is a measure of brain activity that reflects the highly dynamic flow of information across neural networks. This is a composite signal that receives contributions from multiple neural sources, yet interpreting its nature and significance may be hindered by several confounding factors and technical limitations. By and large, the main factor defining the amplitude of LFPs is the geometry of the current sources, over and above the degree of synchronization or the properties of the media. As such, similar levels of activity may result in potentials that differ in several orders of magnitude in different populations. The geometry of these sources has been experimentally inaccessible until intracerebral high density recordings enabled the co-activating sources to be revealed. Without this information, it has proven difficult to interpret a century’s worth of recordings that used temporal cues alone, such as event or spike related potentials and frequency bands. Meanwhile, a collection of biophysically ill-founded concepts have been considered legitimate, which can now be corrected in the light of recent advances. The relationship of LFPs to their sources is often counterintuitive. For instance, most LFP activity is not local but remote, it may be larger further from rather than close to the source, the polarity does not define its excitatory or inhibitory nature, and the amplitude may increase when source’s activity is reduced. As technological developments foster the use of LFPs, the time is now ripe to raise awareness of the need to take into account spatial aspects of these signals and of the errors derived from neglecting to do so.</p>}, journal={Frontiers in Neural Circuits}, publisher={Frontiers}, author={Herreras, Oscar}, year={2016}, month=dec, language={English} }
@article{Fournier_Löcherbach_2016, title={On a toy model of interacting neurons}, volume={52}, ISSN={0246-0203}, DOI={10.1214/15-AIHP701}, abstractNote={Cet article continue l’étude du système stochastique de neurones en interaction introduit par De Masi, Galves, Löcherbach et Presutti (J. Stat. Phys. 158 (2015) 866–902). Le système est composé de $N$ neurones. Chaque neurone décharge un potentiel d’action à des instants aléatoires, à un taux qui dépend de son potentiel de membrane. Ce potentiel est alors remis à $0$, et tous les autres neurones reçoivent une charge supplémentaire de $1/N$. De plus, des synapses électriques induisent une dérive déterministe qui attire le système vers sa valeur moyenne. Nous établissons la propriété de propagation du chaos lorsque $Ntoinfty$, vers la solution d’une équation différentielle stochastique non-linéaire à sauts. Nous améliorons les résultats obtenus dans (J. Stat. Phys. 158 (2015) 866–902) puisque (i) nous levons la condition de support compact imposée aux données initiales, (ii) nous obtenons une vitesse de convergence en $1/sqrt{N}$. Enfin, nous proposons une étude de l’équation limite : nous décrivons la forme de ses lois marginales (en temps), nous démontrons l’existence d’une unique loi invariante non-triviale et montrons que la mesure invariante triviale n’est pas attractive. Enfin, nous obtenons la convergence vers l’équilibre dans un cas particulier.}, number={4}, journal={Annales de l’Institut Henri Poincaré, Probabilités et Statistiques}, publisher={Institut Henri Poincaré}, author={Fournier, Nicolas and Löcherbach, Eva}, year={2016}, month=nov, pages={1844–1876} }
@article{Cormier_Tanré_Veltz_2021, title={Hopf bifurcation in a Mean-Field model of spiking neurons}, url={http://arxiv.org/abs/2008.11116}, DOI={10.48550/arXiv.2008.11116}, abstractNote={We study a family of non-linear McKean-Vlasov SDEs driven by a Poisson measure, modelling the mean-field asymptotic of a network of generalized Integrate-and-Fire neurons. We give sufficient conditions to have periodic solutions through a Hopf bifurcation. Our spectral conditions involve the location of the roots of an explicit holomorphic function. The proof relies on two main ingredients. First, we introduce a discrete time Markov Chain modeling the phases of the successive spikes of a neuron. The invariant measure of this Markov Chain is related to the shape of the periodic solutions. Secondly, we use the Lyapunov-Schmidt method to obtain self-consistent oscillations. We illustrate the result with a toy model for which all the spectral conditions can be analytically checked.}, note={arXiv:2008.11116}, number={arXiv:2008.11116}, publisher={arXiv}, author={Cormier, Quentin and Tanré, Etienne and Veltz, Romain}, year={2021}, month=aug }
@article{Ribar_Sepulchre_2019, title={Neuromodulation of Neuromorphic Circuits}, volume={66}, ISSN={1549-8328, 1558-0806}, DOI={10.1109/TCSI.2019.2907113}, abstractNote={We present a novel methodology to enable control of a neuromorphic circuit in close analogy with the physiological neuromodulation of a single neuron. The methodology is general in that it only relies on a parallel interconnection of elementary voltage-controlled current sources. In contrast to controlling a nonlinear circuit through the parameter tuning of a state-space model, our approach is purely input-output. The circuit elements are controlled and interconnected to shape the current-voltage characteristics (I-V curves) of the circuit in prescribed timescales. In turn, shaping those I-V curves determines the excitability properties of the circuit. We show that this methodology enables both robust and accurate control of the circuit behavior and resembles the biophysical mechanisms of neuromodulation. As a proof of concept, we simulate a SPICE model composed of MOSFET transconductance ampliﬁers operating in the weak inversion regime.}, note={arXiv:1805.05696 [cs, eess, q-bio]}, number={8}, journal={IEEE Transactions on Circuits and Systems I: Regular Papers}, author={Ribar, Luka and Sepulchre, Rodolphe}, year={2019}, month=aug, pages={3028–3040}, language={en} }

% CGS-D Bibliography
@article{Adolphs_2015, title={The unsolved problems of neuroscience}, volume={19}, ISSN={13646613}, DOI={10.1016/j.tics.2015.01.007}, number={4}, journal={Trends in Cognitive Sciences}, author={Adolphs, Ralph}, year={2015}, month=apr, pages={173–175}, language={en} }
@article{Aimone_2019, title={Neural algorithms and computing beyond Moore’s law}, volume={62}, ISSN={0001-0782, 1557-7317}, DOI={10.1145/3231589}, abstractNote={Advances in neurotechnologies are reigniting opportunities to bring neural computation insights into broader computing applications.}, number={4}, journal={Communications of the ACM}, author={Aimone, James B.}, year={2019}, month=mar, pages={110–110}, language={en} }
@article{Aimone_Parekh_2023, title={The brain’s unique take on algorithms}, volume={14}, rights={2023 The Author(s)}, ISSN={2041-1723}, DOI={10.1038/s41467-023-40535-z}, abstractNote={Perspectives for understanding the brain vary across disciplines and this has challenged our ability to describe the brain’s functions. In this comment, we discuss how emerging theoretical computing frameworks that bridge top-down algorithm and bottom-up physics approaches may be ideally suited for guiding the development of neural computing technologies such as neuromorphic hardware and artificial intelligence. Furthermore, we discuss how this balanced perspective may be necessary to incorporate the neurobiological details that are critical for describing the neural computational disruptions within mental health and neurological disorders.}, number={1}, journal={Nature Communications}, publisher={Nature Publishing Group}, author={Aimone, James B. and Parekh, Ojas}, year={2023}, month=aug, pages={4910}, language={en} }
@article{Aimone_Weick_2013, title={Perspectives for computational modeling of cell replacement for neurological disorders}, volume={7}, ISSN={1662-5188}, url={https://www.frontiersin.org/journals/computational-neuroscience/articles/10.3389/fncom.2013.00150/full}, DOI={10.3389/fncom.2013.00150}, abstractNote={<p>Mathematical modeling of anatomically-constrained neural networks has provided significant insights regarding the response of networks to neurological disorders or injury. A logical extension of these models is to incorporate treatment regimens to investigate network responses to intervention. The addition of nascent neurons from stem cell precursors into damaged or diseased tissue has been used as a successful therapeutic tool in recent decades. Interestingly, models have been developed to examine the incorporation of new neurons into intact adult structures, particularly the dentate granule neurons of the hippocampus. These studies suggest that the unique properties of maturing neurons, can impact circuit behavior in unanticipated ways. In this perspective, we review the current status of models used to examine damaged CNS structures with particular focus on cortical damage due to stroke. Secondly, we suggest that computational modeling of cell replacement therapies can be made feasible by implementing approaches taken by current models of adult neurogenesis. The development of these models is critical for generating hypotheses regarding transplant therapies and improving outcomes by tailoring transplants to desired effects.</p>}, journal={Frontiers in Computational Neuroscience}, publisher={Frontiers}, author={Aimone, James B. and Weick, Jason P.}, year={2013}, month=nov, language={English} }
@article{Barabási_Bianconi_Bullmore_Burgess_Chung_Eliassi-Rad_George_Kovács_Makse_Nichols_etal._2023, title={Neuroscience Needs Network Science}, volume={43}, rights={Copyright © 2023 the authors. SfN exclusive license.}, ISSN={0270-6474, 1529-2401}, DOI={10.1523/JNEUROSCI.1014-23.2023}, abstractNote={The brain is a complex system comprising a myriad of interacting neurons, posing significant challenges in understanding its structure, function, and dynamics. Network science has emerged as a powerful tool for studying such interconnected systems, offering a framework for integrating multiscale data and complexity. To date, network methods have significantly advanced functional imaging studies of the human brain and have facilitated the development of control theory-based applications for directing brain activity. Here, we discuss emerging frontiers for network neuroscience in the brain atlas era, addressing the challenges and opportunities in integrating multiple data streams for understanding the neural transitions from development to healthy function to disease. We underscore the importance of fostering interdisciplinary opportunities through workshops, conferences, and funding initiatives, such as supporting students and postdoctoral fellows with interests in both disciplines. By bringing together the network science and neuroscience communities, we can develop novel network-based methods tailored to neural circuits, paving the way toward a deeper understanding of the brain and its functions, as well as offering new challenges for network science.}, number={34}, journal={Journal of Neuroscience}, publisher={Society for Neuroscience}, author={Barabási, Dániel L. and Bianconi, Ginestra and Bullmore, Ed and Burgess, Mark and Chung, SueYeon and Eliassi-Rad, Tina and George, Dileep and Kovács, István A. and Makse, Hernán and Nichols, Thomas E. and Papadimitriou, Christos and Sporns, Olaf and Stachenfeld, Kim and Toroczkai, Zoltán and Towlson, Emma K. and Zador, Anthony M. and Zeng, Hongkui and Barabási, Albert-László and Bernard, Amy and Buzsáki, György}, year={2023}, month=aug, pages={5989–5995}, language={en} }
@article{Bassett_Sporns_2017, title={Network neuroscience}, volume={20}, rights={2017 Springer Nature America, Inc.}, ISSN={1546-1726}, DOI={10.1038/nn.4502}, abstractNote={Network neuroscience tackles the challenge of discovering the principles underlying complex brain function and cognition from an explicitly integrative perspective. Here, the authors discuss emerging trends in network neuroscience, charting a path towards a better understanding of the brain that bridges computation, theory and experiment across spatial scales and species.}, number={3}, journal={Nature Neuroscience}, publisher={Nature Publishing Group}, author={Bassett, Danielle S. and Sporns, Olaf}, year={2017}, month=mar, pages={353–364}, language={en} }
@article{Bäuerle_2023, title={Mean Field Markov Decision Processes}, volume={88}, ISSN={1432-0606}, DOI={10.1007/s00245-023-09985-1}, abstractNote={We consider mean-field control problems in discrete time with discounted reward, infinite time horizon and compact state and action space. The existence of optimal policies is shown and the limiting mean-field problem is derived when the number of individuals tends to infinity. Moreover, we consider the average reward problem and show that the optimal policy in this mean-field limit is $$varepsilon $$-optimal for the discounted problem if the number of individuals is large and the discount factor close to one. This result is very helpful, because it turns out that in the special case when the reward does only depend on the distribution of the individuals, we obtain a very interesting subclass of problems where an average reward optimal policy can be obtained by first computing an optimal measure from a static optimization problem and then achieving it with Markov Chain Monte Carlo methods. We give two applications: Avoiding congestion an a graph and optimal positioning on a market place which we solve explicitly.}, number={1}, journal={Applied Mathematics \& Optimization}, author={Bäuerle, Nicole}, year={2023}, month=apr, pages={12}, language={en} }
@article{Betzel_Bassett_2017, series={Functional Architecture of the Brain}, title={Multi-scale brain networks}, volume={160}, ISSN={1053-8119}, DOI={10.1016/j.neuroimage.2016.11.006}, abstractNote={The network architecture of the human brain has become a feature of increasing interest to the neuroscientific community, largely because of its potential to illuminate human cognition, its variation over development and aging, and its alteration in disease or injury. Traditional tools and approaches to study this architecture have largely focused on single scales—of topology, time, and space. Expanding beyond this narrow view, we focus this review on pertinent questions and novel methodological advances for the multi-scale brain. We separate our exposition into content related to multi-scale topological structure, multi-scale temporal structure, and multi-scale spatial structure. In each case, we recount empirical evidence for such structures, survey network-based methodological approaches to reveal these structures, and outline current frontiers and open questions. Although predominantly peppered with examples from human neuroimaging, we hope that this account will offer an accessible guide to any neuroscientist aiming to measure, characterize, and understand the full richness of the brain’s multiscale network structure—irrespective of species, imaging modality, or spatial resolution.}, journal={NeuroImage}, author={Betzel, Richard F. and Bassett, Danielle S.}, year={2017}, month=oct, pages={73–83}, collection={Functional Architecture of the Brain} }
@book{Boole_1854, year={1854}, title={An investigation of the laws of thought, on which are founded the mathematical theories of logic and probabilities}, author={Boole, George}, language={en}, publisher={Walton and Maberly}, address={London} }
@article{Breakspear_2017, title={Dynamic models of large-scale brain activity}, volume={20}, rights={2017 Springer Nature America, Inc.}, ISSN={1546-1726}, DOI={10.1038/nn.4497}, abstractNote={Cognitive activity requires the collective behavior of cortical, thalamic and spinal neurons across large-scale systems of the CNS. This paper provides an illustrated introduction to dynamic models of large-scale brain activity, from the tenets of the underlying theory to challenges, controversies and recent breakthroughs.}, number={3}, journal={Nature Neuroscience}, publisher={Nature Publishing Group}, author={Breakspear, Michael}, year={2017}, month=mar, pages={340–352}, language={en} }
@article{Bressler_Kelso_2016, title={Coordination Dynamics in Cognitive Neuroscience}, volume={10}, ISSN={1662-453X}, url={https://www.frontiersin.org/journals/neuroscience/articles/10.3389/fnins.2016.00397/full}, DOI={10.3389/fnins.2016.00397}, abstractNote={<p>Many researchers and clinicians in cognitive neuroscience hold to a modular view of cognitive function in which the cerebral cortex operates by the activation of areas with circumscribed elementary cognitive functions. Yet an ongoing paradigm shift to a dynamic network perspective is underway. This new viewpoint treats cortical function as arising from the coordination dynamics within and between cortical regions. Cortical coordination dynamics arises due to the unidirectional influences imposed on a cortical area by inputs from other areas that project to it, combined with the projection reciprocity that characterizes cortical connectivity and gives rise to reentrant processing. As a result, cortical dynamics exhibits both segregative and integrative tendencies and gives rise to both cooperative and competitive relations within and between cortical areas that are hypothesized to underlie the emergence of cognition in brains.</p>}, journal={Frontiers in Neuroscience}, publisher={Frontiers}, author={Bressler, Steven L. and Kelso, J. A. Scott}, year={2016}, month=sep, language={English} }
@article{Buzsáki_2010, title={Neural Syntax: Cell Assemblies, Synapsembles, and Readers}, volume={68}, ISSN={0896-6273}, DOI={10.1016/j.neuron.2010.09.023}, number={3}, journal={Neuron}, publisher={Elsevier}, author={Buzsáki, György}, year={2010}, month=nov, pages={362–385}, language={English} }
@inproceedings{Caines_Huang_2019, title={Graphon Mean Field Games and the GMFG Equations: $\varepsilon$-Nash Equilibria}, ISSN={2576-2370}, url={https://ieeexplore.ieee.org/document/9029871}, DOI={10.1109/CDC40024.2019.9029871}, abstractNote={Very large networks linking dynamical agents are now ubiquitous and the need to analyse, design and control them is evident. The emergence of the graphon theory of large networks and their infinite limits has enabled the formulation of a theory of the centralized control of dynamical systems distributed on asymptotically infinite networks [Gao and Caines, CDC 2017, 2018]. Moreover, the study of the decentralized control of such systems was initiated in [Caines and Huang, CDC 2018] where Graphon Mean Field Games (GMFG) and the GMFG equations were formulated for the analysis of noncooperative dynamic games on unbounded networks. In that work, existence and uniqueness results were established for the GMFG equations, while the current work continues that analysis by developing an ε-Nash theory for GMFG systems by relating the infinite population equilibria on infinite networks to finite population equilibria on finite networks.}, booktitle={2019 IEEE 58th Conference on Decision and Control (CDC)}, author={Caines, Peter E. and Huang, Minyi}, year={2019}, month=dec, pages={286–292} }
@article{Caines_Huang_2021, title={Graphon Mean Field Games and Their Equations}, volume={59}, ISSN={0363-0129}, DOI={10.1137/20M136373X}, abstractNote={This work considers stochastic differential games with a large number of players, whose costs and dynamics interact through the empirical distribution of both their states and their controls. We develop a new framework to prove convergence of finite-player games to the asymptotic mean field game. Our approach is based on the concept of propagation of chaos for forward and backward weakly interacting particles which we investigate by stochastic analysis methods, and which appear to be of independent interest. These propagation of chaos arguments allow us to derive moment and concentration bounds for the convergence of Nash equilibria.}, number={6}, journal={SIAM Journal on Control and Optimization}, publisher={Society for Industrial and Applied Mathematics}, author={Caines, Peter E. and Huang, Minyi}, year={2021}, month=jan, pages={4373–4399} }
@inproceedings{Caines_Huang_2024, title={Mean Field Games on Dense and Sparse Networks: The Graphexon MFG Equations}, ISSN={2378-5861}, url={https://ieeexplore.ieee.org/document/10644709}, DOI={10.23919/ACC60939.2024.10644709}, abstractNote={For sequences of networks embedded in the unit cube [0,1]m in mathbbR^m (more generally compact sets in Riemannian manifolds), a notion related to that of graphons was introduced in [Caines, CDC 2022] in terms of (weak) measure limits of (sub-) sequences of empirical measures of vertex densities (vertexons) on [0,1]m and the associated (weak) measure limits of (sub-) sequences of empirical measures of edge densities (graphexons) on łceil 0,1rceil2n, both of which exist regardless of sparsity or density of the limit graphs. This paper presents an extension of the Graphon Mean Field Game (GMFG) theory of [Caines-Huang, SICON, 2021] to the vertexon-graphexon MFG set-up (here denoted GXMFG). In particular, for sparse limit graphexons, an LQG GXMFG example is presented where the influence between agent populations on neighbouring nodes is modeled via a first order PDE.}, booktitle={2024 American Control Conference (ACC)}, author={Caines, Peter E. and Huang, Minyi}, year={2024}, month=jul, pages={4230–4235} }
@article{D’Angelo_Jirsa_2022, title={The quest for multiscale brain modeling}, volume={45}, ISSN={0166-2236, 1878-108X}, DOI={10.1016/j.tins.2022.06.007}, number={10}, journal={Trends in Neurosciences}, publisher={Elsevier}, author={D’Angelo, Egidio and Jirsa, Viktor}, year={2022}, month=oct, pages={777–790}, language={English} }
@inproceedings{Fabian_Cui_Koeppl_2023, title={Learning Sparse Graphon Mean Field Games}, ISSN={2640-3498}, url={https://proceedings.mlr.press/v206/fabian23a.html}, abstractNote={Although the field of multi-agent reinforcement learning (MARL) has made considerable progress in the last years, solving systems with a large number of agents remains a hard challenge. Graphon mean field games (GMFGs) enable the scalable analysis of MARL problems that are otherwise intractable. By the mathematical structure of graphons, this approach is limited to dense graphs which are insufficient to describe many real-world networks such as power law graphs. Our paper introduces a novel formulation of GMFGs, called LPGMFGs, which leverages the graph theoretical concept of LpLpL^p graphons and provides a machine learning tool to efficiently and accurately approximate solutions for sparse network problems. This especially includes power law networks which are empirically observed in various application areas and cannot be captured by standard graphons. We derive theoretical existence and convergence guarantees and give empirical examples that demonstrate the accuracy of our learning approach for systems with many agents. Furthermore, we extend the Online Mirror Descent (OMD) learning algorithm to our setup to accelerate learning speed, empirically show its capabilities, and conduct a theoretical analysis using the novel concept of smoothed step graphons. In general, we provide a scalable, mathematically well-founded machine learning approach to a large class of otherwise intractable problems of great relevance in numerous research fields.}, booktitle={Proceedings of The 26th International Conference on Artificial Intelligence and Statistics}, publisher={PMLR}, author={Fabian, Christian and Cui, Kai and Koeppl, Heinz}, year={2023}, month=apr, pages={4486–4514}, language={en} }
@article{Harris_2005, title={Neural signatures of cell assembly organization}, volume={6}, rights={2005 Springer Nature Limited}, ISSN={1471-0048}, DOI={10.1038/nrn1669}, abstractNote={Cortical neurons show irregular but structured spike trains. This has been interpreted as evidence for “temporal coding”, whereby stimuli are represented by precise spike-timing patterns. Here, we suggest an alternative interpretation based on the older concept of the cell assembly. The dynamic evolution of assembly sequences, which are steered but not deterministically controlled by sensory input, is the proposed substrate of psychological processes beyond simple stimulus–response associations. Accordingly, spike trains show a temporal structure that is stimulus-dependent and more variable than would be predicted by strict sensory control. We propose four signatures of assembly organization that can be experimentally tested. We argue that many observations that have been interpreted as evidence for temporal coding might instead reflect an underlying assembly structure.}, number={5}, journal={Nature Reviews Neuroscience}, publisher={Nature Publishing Group}, author={Harris, Kenneth D.}, year={2005}, month=may, pages={399–407}, language={en} }
@article{Harris_Shepherd_2015, title={The neocortical circuit: themes and variations}, volume={18}, rights={2015 Springer Nature America, Inc.}, ISSN={1546-1726}, DOI={10.1038/nn.3917}, abstractNote={Harris and Shepherd review our knowledge of input and output patterns for different classes of cortical cells. They propose that cortex, like other parts of the body, has a serially homologous organization, featuring area- and species-specific variations on a basic theme, that allows different types of function to emerge.}, number={2}, journal={Nature Neuroscience}, publisher={Nature Publishing Group}, author={Harris, Kenneth D. and Shepherd, Gordon M. G.}, year={2015}, month=feb, pages={170–181}, language={en} }
@article{Huang_Malhame_Caines_2006, title={Large population stochastic dynamic games: closed-loop McKean-Vlasov systems and the Nash certainty equivalence principle}, volume={6}, ISSN={1526-7555, 2163-4548}, number={3}, journal={Communications in Information \& Systems}, publisher={International Press of Boston}, author={Huang, Minyi and Malhamé, Roland P. and Caines, Peter E.}, year={2006}, pages={221–252} }
@article{Jaeger_Noheda_vanderWiel_2023, title={Toward a formal theory for computing machines made out of whatever physics offers}, volume={14}, rights={2023 The Author(s)}, ISSN={2041-1723}, DOI={10.1038/s41467-023-40533-1}, abstractNote={Approaching limitations of digital computing technologies have spurred research in neuromorphic and other unconventional approaches to computing. Here we argue that if we want to engineer unconventional computing systems in a systematic way, we need guidance from a formal theory that is different from the classical symbolic-algorithmic Turing machine theory. We propose a general strategy for developing such a theory, and within that general view, a specific approach that we call fluent computing. In contrast to Turing, who modeled computing processes from a top-down perspective as symbolic reasoning, we adopt the scientific paradigm of physics and model physical computing systems bottom-up by formalizing what can ultimately be measured in a physical computing system. This leads to an understanding of computing as the structuring of processes, while classical models of computing systems describe the processing of structures.}, number={1}, journal={Nature Communications}, publisher={Nature Publishing Group}, author={Jaeger, Herbert and Noheda, Beatriz and van der Wiel, Wilfred G.}, year={2023}, month=aug, pages={4911}, language={en} }
@article{Jirsa_2004, title={Connectivity and dynamics of neural information processing}, volume={2}, ISSN={1559-0089}, DOI={10.1385/NI:2:2:183}, abstractNote={In this article, we systematically review the current literature on neural connectivity and dynamics, or equivalently, structure and function. In particular, we discuss how changes in the connectivity of a neural network affect the spatiotemporal network dynamics qualitatively. The three major criteria of comparison are, first, the local dynamics at the network nodes which includes fixed point dynamics, oscillatory and chaotic dynamics; second, the presence of time delays via propagation along connecting pathways; and third, the properties of the connectivity matrix such as its statistics, symmetry, and translational invariance. Since the connection topology changes when anatomical scales are traversed, so will the corresponding network dynamics change. As a consequence different types of networks are encountered on different levels of neural organization.}, number={2}, journal={Neuroinformatics}, author={Jirsa, Viktor K.}, year={2004}, month=jun, pages={183–204}, language={en} }
@article{Kanerva_2009, title={Hyperdimensional Computing: An Introduction to Computing in Distributed Representation with High-Dimensional Random Vectors}, volume={1}, ISSN={1866-9964}, DOI={10.1007/s12559-009-9009-8}, abstractNote={The 1990s saw the emergence of cognitive models that depend on very high dimensionality and randomness. They include Holographic Reduced Representations, Spatter Code, Semantic Vectors, Latent Semantic Analysis, Context-Dependent Thinning, and Vector-Symbolic Architecture. They represent things in high-dimensional vectors that are manipulated by operations that produce new high-dimensional vectors in the style of traditional computing, in what is called here hyperdimensional computing on account of the very high dimensionality. The paper presents the main ideas behind these models, written as a tutorial essay in hopes of making the ideas accessible and even provocative. A sketch of how we have arrived at these models, with references and pointers to further reading, is given at the end. The thesis of the paper is that hyperdimensional representation has much to offer to students of cognitive science, theoretical neuroscience, computer science and engineering, and mathematics.}, number={2}, journal={Cognitive Computation}, author={Kanerva, Pentti}, year={2009}, month=jun, pages={139–159}, language={en} }
@article{Knill_Pouget_2004, title={The Bayesian brain: the role of uncertainty in neural coding and computation}, volume={27}, ISSN={0166-2236, 1878-108X}, DOI={10.1016/j.tins.2004.10.007}, number={12}, journal={Trends in Neurosciences}, publisher={Elsevier}, author={Knill, David C. and Pouget, Alexandre}, year={2004}, month=dec, pages={712–719}, language={English} }
@article{Lasry_Lions_2007, title={Mean field games}, volume={2}, ISSN={1861-3624}, DOI={10.1007/s11537-007-0657-8}, abstractNote={We survey here some recent studies concerning what we call mean-field models by analogy with Statistical Mechanics and Physics. More precisely, we present three examples of our mean-field approach to modelling in Economics and Finance (or other related subjects...). Roughly speaking, we are concerned with situations that involve a very large number of “rational players” with a limited information (or visibility) on the “game”. Each player chooses his optimal strategy in view of the global (or macroscopic) informations that are available to him and that result from the actions of all players. In the three examples we mention here, we derive a mean-field problem which consists in nonlinear differential equations. These equations are of a new type and our main goal here is to study them and establish their links with various fields of Analysis. We show in particular that these nonlinear problems are essentially well-posed problems i.e., have unique solutions. In addition, we give various limiting cases, examples and possible extensions. And we mention many open problems.}, number={1}, journal={Japanese Journal of Mathematics}, author={Lasry, Jean-Michel and Lions, Pierre-Louis}, year={2007}, month=mar, pages={229–260}, language={en} }
@article{Lynn_Bassett_2019, title={The physics of brain network structure, function and control}, volume={1}, rights={2019 The Author(s), under exclusive licence to Springer Nature Limited}, ISSN={2522-5820}, DOI={10.1038/s42254-019-0040-8}, abstractNote={The brain is characterized by heterogeneous patterns of structural connections supporting unparalleled feats of cognition and a wide range of behaviours. New non-invasive imaging techniques now allow comprehensive mapping of these patterns. However, a fundamental challenge remains to understand how the brain’s structural wiring supports cognitive processes, with major implications for personalized mental health treatments. Here, we review recent efforts to meet this challenge, drawing on physics intuitions, models and theories, spanning the domains of statistical mechanics, information theory, dynamical systems and control. We first describe the organizing principles of brain network architecture instantiated in structural wiring under constraints of spatial embedding and energy minimization. We then survey models of brain network function that stipulate how neural activity propagates along structural connections. Finally, we discuss perturbative experiments and models for brain network control; these use the physics of signal transmission along structural connections to infer intrinsic control processes that support goal-directed behaviour and to inform stimulation-based therapies for neurological and psychiatric disease. Throughout, we highlight open questions that invite the creative efforts of pioneering physicists.}, number={5}, journal={Nature Reviews Physics}, publisher={Nature Publishing Group}, author={Lynn, Christopher W. and Bassett, Danielle S.}, year={2019}, month=may, pages={318–332}, language={en} }
@article{McCulloch_Pitts_1943, title={A logical calculus of the ideas immanent in nervous activity}, volume={5}, DOI={10.1007/BF02478259}, abstractNote={Because of the “all-or-none” character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time. Various applications of the calculus are discussed. (PsycINFO Database Record (c) 2016 APA, all rights reserved)}, journal={Bulletin of Mathematical Biophysics}, author={McCulloch, W. S. and Pitts, W.}, year={1943}, pages={115–133} }
@article{Markram_Muller_Ramaswamy_Reimann_Abdellah_Sanchez_Ailamaki_Alonso-Nanclares_Antille_Arsever_et_al_2015, title={Reconstruction and Simulation of Neocortical Microcircuitry}, volume={163}, ISSN={1097-4172}, DOI={10.1016/j.cell.2015.09.029}, abstractNote={We present a first-draft digital reconstruction of the microcircuitry of somatosensory cortex of juvenile rat. The reconstruction uses cellular and synaptic organizing principles to algorithmically reconstruct detailed anatomy and physiology from sparse experimental data. An objective anatomical method defines a neocortical volume of 0.29 ± 0.01 mm(3) containing ~31,000 neurons, and patch-clamp studies identify 55 layer-specific morphological and 207 morpho-electrical neuron subtypes. When digitally reconstructed neurons are positioned in the volume and synapse formation is restricted to biological bouton densities and numbers of synapses per connection, their overlapping arbors form ~8 million connections with ~37 million synapses. Simulations reproduce an array of in vitro and in vivo experiments without parameter tuning. Additionally, we find a spectrum of network states with a sharp transition from synchronous to asynchronous activity, modulated by physiological mechanisms. The spectrum of network states, dynamically reconfigured around this transition, supports diverse information processing strategies.
PAPERCLIP: VIDEO ABSTRACT.}, number={2}, journal={Cell}, author={Markram, Henry and Muller, Eilif and Ramaswamy, Srikanth and Reimann, Michael W. and Abdellah, Marwan and Sanchez, Carlos Aguado and Ailamaki, Anastasia and Alonso-Nanclares, Lidia and Antille, Nicolas and Arsever, Selim and Kahou, Guy Antoine Atenekeng and Berger, Thomas K. and Bilgili, Ahmet and Buncic, Nenad and Chalimourda, Athanassia and Chindemi, Giuseppe and Courcol, Jean-Denis and Delalondre, Fabien and Delattre, Vincent and Druckmann, Shaul and Dumusc, Raphael and Dynes, James and Eilemann, Stefan and Gal, Eyal and Gevaert, Michael Emiel and Ghobril, Jean-Pierre and Gidon, Albert and Graham, Joe W. and Gupta, Anirudh and Haenel, Valentin and Hay, Etay and Heinis, Thomas and Hernando, Juan B. and Hines, Michael and Kanari, Lida and Keller, Daniel and Kenyon, John and Khazen, Georges and Kim, Yihwa and King, James G. and Kisvarday, Zoltan and Kumbhar, Pramod and Lasserre, Sébastien and Le Bé, Jean-Vincent and Magalhães, Bruno R. C. and Merchán-Pérez, Angel and Meystre, Julie and Morrice, Benjamin Roy and Muller, Jeffrey and Muñoz-Céspedes, Alberto and Muralidhar, Shruti and Muthurasa, Keerthan and Nachbaur, Daniel and Newton, Taylor H. and Nolte, Max and Ovcharenko, Aleksandr and Palacios, Juan and Pastor, Luis and Perin, Rodrigo and Ranjan, Rajnish and Riachi, Imad and Rodríguez, José-Rodrigo and Riquelme, Juan Luis and Rössert, Christian and Sfyrakis, Konstantinos and Shi, Ying and Shillcock, Julian C. and Silberberg, Gilad and Silva, Ricardo and Tauheed, Farhan and Telefont, Martin and Toledo-Rodriguez, Maria and Tränkler, Thomas and Van Geit, Werner and Díaz, Jafet Villafranca and Walker, Richard and Wang, Yun and Zaninetta, Stefano M. and DeFelipe, Javier and Hill, Sean L. and Segev, Idan and Schürmann, Felix}, year={2015}, month=oct, pages={456–492}, language={eng} }
@article{Mead_1990, title={Neuromorphic electronic systems}, volume={78}, ISSN={1558-2256}, DOI={10.1109/5.58356}, abstractNote={It is shown that for many problems, particularly those in which the input data are ill-conditioned and the computation can be specified in a relative manner, biological solutions are many orders of magnitude more effective than those using digital methods. This advantage can be attributed principally to the use of elementary physical phenomena as computational primitives, and to the representation of information by the relative values of analog signals rather than by the absolute values of digital signals. This approach requires adaptive techniques to mitigate the effects of component differences. This kind of adaptation leads naturally to systems that learn about their environment. Large-scale adaptive analog systems are more robust to component degradation and failure than are more conventional systems, and they use far less power. For this reason, adaptive analog technology can be expected to utilize the full potential of wafer-scale silicon fabrication.<>}, number={10}, journal={Proceedings of the IEEE}, author={Mead, C.}, year={1990}, month=oct, pages={1629–1636} }
@inbook{Neumann_1956, title={Probabilistic Logics and the Synthesis of Reliable Organisms From Unreliable Components}, ISBN={978-1-4008-8261-8}, url={https://www.degruyter.com/document/doi/10.1515/9781400882618-003/pdf?licenseType=restricted}, DOI={10.1515/9781400882618-003}, booktitle={Automata Studies}, publisher={Princeton University Press}, author={Neumann, J. von}, editor={Shannon, C. E. and McCarthy, J.}, year={1956}, pages={43–98}, language={en} }
@article{Papadimitriou_Vempala_Mitropolsky_Collins_Maass_2020, title={Brain computation by assemblies of neurons}, volume={117}, DOI={10.1073/pnas.2001893117}, abstractNote={Assemblies are large populations of neurons believed to imprint memories, concepts, words, and other cognitive information. We identify a repertoire of operations on assemblies. These operations correspond to properties of assemblies observed in experiments, and can be shown, analytically and through simulations, to be realizable by generic, randomly connected populations of neurons with Hebbian plasticity and inhibition. Assemblies and their operations constitute a computational model of the brain which we call the Assembly Calculus, occupying a level of detail intermediate between the level of spiking neurons and synapses and that of the whole brain. The resulting computational system can be shown, under assumptions, to be, in principle, capable of carrying out arbitrary computations. We hypothesize that something like it may underlie higher human cognitive functions such as reasoning, planning, and language. In particular, we propose a plausible brain architecture based on assemblies for implementing the syntactic processing of language in cortex, which is consistent with recent experimental results.}, number={25}, journal={Proceedings of the National Academy of Sciences}, publisher={Proceedings of the National Academy of Sciences}, author={Papadimitriou, Christos H. and Vempala, Santosh S. and Mitropolsky, Daniel and Collins, Michael and Maass, Wolfgang}, year={2020}, month=jun, pages={14464–14472} }
@article{Park_Friston_2013, title={Structural and Functional Brain Networks: From Connections to Cognition}, volume={342}, DOI={10.1126/science.1238411}, abstractNote={How rich functionality emerges from the invariant structural architecture of the brain remains a major mystery in neuroscience. Recent applications of network theory and theoretical neuroscience to large-scale brain networks have started to dissolve this mystery. Network analyses suggest that hierarchical modular brain networks are particularly suited to facilitate local (segregated) neuronal operations and the global integration of segregated functions. Although functional networks are constrained by structural connections, context-sensitive integration during cognition tasks necessarily entails a divergence between structural and functional networks. This degenerate (many-to-one) function-structure mapping is crucial for understanding the nature of brain networks. The emergence of dynamic functional networks from static structural connections calls for a formal (computational) approach to neuronal information processing that may resolve this dialectic between structure and function.}, number={6158}, journal={Science}, publisher={American Association for the Advancement of Science}, author={Park, Hae-Jeong and Friston, Karl}, year={2013}, month=nov, pages={1238411} }
@article{Pinotsis_Robinson_beim_Graben_Friston_2014, title={Neural masses and fields: modeling the dynamics of brain activity}, volume={8}, ISSN={1662-5188}, url={https://www.frontiersin.org/journals/computational-neuroscience/articles/10.3389/fncom.2014.00149/full}, DOI={10.3389/fncom.2014.00149}, journal={Frontiers in Computational Neuroscience}, publisher={Frontiers}, author={Pinotsis, Dimitris and Robinson, Peter and beim Graben, Peter and Friston, Karl}, year={2014}, month=nov, language={English} }
@article{Rabinovich_Huerta_Varona_Afraimovich_2008, title={Transient Cognitive Dynamics, Metastability, and Decision Making}, volume={4}, ISSN={1553-7358}, DOI={10.1371/journal.pcbi.1000072}, abstractNote={The idea that cognitive activity can be understood using nonlinear dynamics has been intensively discussed at length for the last 15 years. One of the popular points of view is that metastable states play a key role in the execution of cognitive functions. Experimental and modeling studies suggest that most of these functions are the result of transient activity of large-scale brain networks in the presence of noise. Such transients may consist of a sequential switching between different metastable cognitive states. The main problem faced when using dynamical theory to describe transient cognitive processes is the fundamental contradiction between reproducibility and flexibility of transient behavior. In this paper, we propose a theoretical description of transient cognitive dynamics based on the interaction of functionally dependent metastable cognitive states. The mathematical image of such transient activity is a stable heteroclinic channel, i.e., a set of trajectories in the vicinity of a heteroclinic skeleton that consists of saddles and unstable separatrices that connect their surroundings. We suggest a basic mathematical model, a strongly dissipative dynamical system, and formulate the conditions for the robustness and reproducibility of cognitive transients that satisfy the competing requirements for stability and flexibility. Based on this approach, we describe here an effective solution for the problem of sequential decision making, represented as a fixed time game: a player takes sequential actions in a changing noisy environment so as to maximize a cumulative reward. As we predict and verify in computer simulations, noise plays an important role in optimizing the gain.}, number={5}, journal={PLOS Computational Biology}, publisher={Public Library of Science}, author={Rabinovich, Mikhail I. and Huerta, Ramón and Varona, Pablo and Afraimovich, Valentin S.}, year={2008}, month=may, pages={e1000072}, language={en} }
@article{Radner_1962, title={Team Decision Problems}, volume={33}, ISSN={0003-4851}, DOI={10.1214/AOMS/1177704455}, abstractNote={(DOI: 10.1214/AOMS/1177704455) This article is published in Annals of Mathematical Statistics. The article was published on 01 Sep 1962.  and is currently open access.  The article focuses on the topics: Decision analysis \& Decision support system.}, number={3}, journal={Annals of Mathematical Statistics}, publisher={Institute of Mathematical Statistics}, author={Radner, Roy}, year={1962}, month=sep, pages={857–881}, language={en} }
@article{Ranhel_2012, title={Neural Assembly Computing}, volume={23}, ISSN={2162-2388}, DOI={10.1109/TNNLS.2012.2190421}, abstractNote={Spiking neurons can realize several computational operations when firing cooperatively. This is a prevalent notion, although the mechanisms are not yet understood. A way by which neural assemblies compute is proposed in this paper. It is shown how neural coalitions represent things (and world states), memorize them, and control their hierarchical relations in order to perform algorithms. It is described how neural groups perform statistic logic functions as they form assemblies. Neural coalitions can reverberate, becoming bistable loops. Such bistable neural assemblies become short- or long-term memories that represent the event that triggers them. In addition, assemblies can branch and dismantle other neural groups generating new events that trigger other coalitions. Hence, such capabilities and the interaction among assemblies allow neural networks to create and control hierarchical cascades of causal activities, giving rise to parallel algorithms. Computing and algorithms are used here as in a nonstandard computation approach. In this sense, neural assembly computing (NAC) can be seen as a new class of spiking neural network machines. NAC can explain the following points: 1) how neuron groups represent things and states; 2) how they retain binary states in memories that do not require any plasticity mechanism; and 3) how branching, disbanding, and interaction among assemblies may result in algorithms and behavioral responses. Simulations were carried out and the results are in agreement with the hypothesis presented. A MATLAB code is available as a supplementary material.}, number={6}, journal={IEEE Transactions on Neural Networks and Learning Systems}, author={Ranhel, João}, year={2012}, month=jun, pages={916–927} }
@article{Richards_Lillicrap_Beaudoin_Bengio_Bogacz_Christensen_Clopath_Costa_deBerker_Ganguli_etal_2019, title={A deep learning framework for neuroscience}, volume={22}, rights={2019 Springer Nature America, Inc.}, ISSN={1546-1726}, DOI={10.1038/s41593-019-0520-2}, abstractNote={Systems neuroscience seeks explanations for how the brain implements a wide variety of perceptual, cognitive and motor tasks. Conversely, artificial intelligence attempts to design computational systems based on the tasks they will have to solve. In artificial neural networks, the three components specified by design are the objective functions, the learning rules and the architectures. With the growing success of deep learning, which utilizes brain-inspired architectures, these three designed components have increasingly become central to how we model, engineer and optimize complex artificial learning systems. Here we argue that a greater focus on these components would also benefit systems neuroscience. We give examples of how this optimization-based framework can drive theoretical and experimental progress in neuroscience. We contend that this principled perspective on systems neuroscience will help to generate more rapid progress.}, number={11}, journal={Nature Neuroscience}, publisher={Nature Publishing Group}, author={Richards, Blake A. and Lillicrap, Timothy P. and Beaudoin, Philippe and Bengio, Yoshua and Bogacz, Rafal and Christensen, Amelia and Clopath, Claudia and Costa, Rui Ponte and de Berker, Archy and Ganguli, Surya and Gillon, Colleen J. and Hafner, Danijar and Kepecs, Adam and Kriegeskorte, Nikolaus and Latham, Peter and Lindsay, Grace W. and Miller, Kenneth D. and Naud, Richard and Pack, Christopher C. and Poirazi, Panayiota and Roelfsema, Pieter and Sacramento, João and Saxe, Andrew and Scellier, Benjamin and Schapiro, Anna C. and Senn, Walter and Wayne, Greg and Yamins, Daniel and Zenke, Friedemann and Zylberberg, Joel and Therien, Denis and Kording, Konrad P.}, year={2019}, month=nov, pages={1761–1770}, language={en} }
@article{Rolls_Loh_Deco_Winterer_2008, title={Computational models of schizophrenia and dopamine modulation in the prefrontal cortex}, volume={9}, rights={2008 Springer Nature Limited}, ISSN={1471-0048}, DOI={10.1038/nrn2462}, abstractNote={In attractor networks, a reduction in the depth of the basins of attraction of cortical attractor states destabilizes the activity at the network level owing to the constant statistical fluctuations that are caused by the stochastic spiking of neurons.A decrease in the NMDA (N-methyl-D-aspartate)-receptor (NMDAR) conductances, which reduces the depth of the attractor basins, reduces the stability of short-term memory states and increases distractibility. These effects decrease the signal-to-noise ratio of the networks.The cognitive symptoms of schizophrenia, such as distractibility, working-memory deficits and poor attention, could be caused by such instability of attractor states in prefrontal cortical networks.Information processing in patients with schizophrenia is characterized by a diminished cortical signal-to-noise ratio during tasks that require the allocation of attention and short-term memory (as suggested by, for example, electrophysiological recordings and functional MRI).In patients with schizophrenia, reduced dopamine D1 receptor activation in the prefrontal cortex can decrease the signal-to-noise ratio, at least in part by reducing NMDAR-activated synaptic currents. The underlying reason for this might be the decrease in the stability of the cortical attractor networks that is produced by a reduction in NMDAR-activated currents: this reduction decreases the depth of the basins of attraction, making short-term memory and attention unstable in the context of the spiking-related noise in cortical networks.This computational approach enables us to link factors that modulate currents in synapses to the effects of this modulation on the global performance of a network — for example, to implement cognitive processes such as short-term memory and attention.A reduction of NMDAR-activated synaptic conductances produces lower firing rates in neurons; in the orbitofrontal and anterior cingulate cortices this could account for the negative symptoms of schizophrenia, including reduced emotions.Decreasing the GABA (g-aminobutyric acid) and the NMDA conductances produces not only switches between the attractor states, but also jumps from spontaneous activity into one of the attractor states. This might be related to the positive symptoms of schizophrenia, including delusions, paranoia and hallucinations: these symptoms could arise because the basins of attraction are shallow and there is instability in temporal lobe semantic-memory networks, leading thoughts to move too freely around the attractor energy landscape.}, number={9}, journal={Nature Reviews Neuroscience}, publisher={Nature Publishing Group}, author={Rolls, Edmund T. and Loh, Marco and Deco, Gustavo and Winterer, Georg}, year={2008}, month=sep, pages={696–709}, language={en} }
Funding: This work was supported by Natural Sciences and Engineering Research Council of Canada.}, number={3}, journal={Mathematics of Operations Research}, publisher={INFORMS}, author={Sanjari, Sina and Saldi, Naci and Yüksel, Serdar}, year={2023}, month=aug, pages={1254–1285} }
@article{Schuman_Kulkarni_Parsa_Mitchell_Date_Kay_2022, title={Opportunities for neuromorphic computing algorithms and applications}, volume={2}, rights={2022 Springer Nature America, Inc.}, ISSN={2662-8457}, DOI={10.1038/s43588-021-00184-y}, abstractNote={Neuromorphic computing technologies will be important for the future of computing, but much of the work in neuromorphic computing has focused on hardware development. Here, we review recent results in neuromorphic computing algorithms and applications. We highlight characteristics of neuromorphic computing technologies that make them attractive for the future of computing and we discuss opportunities for future development of algorithms and applications on these systems.}, number={1}, journal={Nature Computational Science}, publisher={Nature Publishing Group}, author={Schuman, Catherine D. and Kulkarni, Shruti R. and Parsa, Maryam and Mitchell, J. Parker and Date, Prasanna and Kay, Bill}, year={2022}, month=jan, pages={10–19}, language={en} }
@article{Song_Sjöström_Reigl_Nelson_Chklovskii_2005, title={Highly Nonrandom Features of Synaptic Connectivity in Local Cortical Circuits}, volume={3}, ISSN={1545-7885}, DOI={10.1371/journal.pbio.0030068}, abstractNote={How different is local cortical circuitry from a random network? To answer this question, we probed synaptic connections with several hundred simultaneous quadruple whole-cell recordings from layer 5 pyramidal neurons in the rat visual cortex. Analysis of this dataset revealed several nonrandom features in synaptic connectivity. We confirmed previous reports that bidirectional connections are more common than expected in a random network. We found that several highly clustered three-neuron connectivity patterns are overrepresented, suggesting that connections tend to cluster together. We also analyzed synaptic connection strength as defined by the peak excitatory postsynaptic potential amplitude. We found that the distribution of synaptic connection strength differs significantly from the Poisson distribution and can be fitted by a lognormal distribution. Such a distribution has a heavier tail and implies that synaptic weight is concentrated among few synaptic connections. In addition, the strengths of synaptic connections sharing pre- or postsynaptic neurons are correlated, implying that strong connections are even more clustered than the weak ones. Therefore, the local cortical network structure can be viewed as a skeleton of stronger connections in a sea of weaker ones. Such a skeleton is likely to play an important role in network dynamics and should be investigated further.}, number={3}, journal={PLOS Biology}, publisher={Public Library of Science}, author={Song, Sen and Sjöström, Per Jesper and Reigl, Markus and Nelson, Sacha and Chklovskii, Dmitri B.}, year={2005}, month=mar, pages={e68}, language={en} }
@article{Sporns_Tononi_Edelman_2000, title={Theoretical neuroanatomy: relating anatomical and functional connectivity in graphs and cortical connection matrices}, volume={10}, ISSN={1047-3211}, DOI={10.1093/cercor/10.2.127}, abstractNote={Neuroanatomy places critical constraints on the functional connectivity of the cerebral cortex. To analyze these constraints we have examined the relationship between structural features of networks (expressed as graphs) and the patterns of functional connectivity to which they give rise when implemented as dynamical systems. We selected among structurally varying graphs using as selective criteria a number of global information-theoretical measures that characterize functional connectivity. We selected graphs separately for increases in measures of entropy (capturing statistical independence of graph elements), integration (capturing their statistical dependence) and complexity (capturing the interplay between their functional segregation and integration). We found that dynamics with high complexity were supported by graphs whose units were organized into densely linked groups that were sparsely and reciprocally interconnected. Connection matrices based on actual neuroanatomical data describing areas and pathways of the macaque visual cortex and the cat cortex showed structural characteristics that coincided best with those of such complex graphs, revealing the presence of distinct but interconnected anatomical groupings of areas. Moreover, when implemented as dynamical systems, these cortical connection matrices generated functional connectivity with high complexity, characterized by the presence of highly coherent functional clusters. We also found that selection of graphs as they responded to input or produced output led to increases in the complexity of their dynamics. We hypothesize that adaptation to rich sensory environments and motor demands requires complex dynamics and that these dynamics are supported by neuroanatomical motifs that are characteristic of the cerebral cortex.}, number={2}, journal={Cerebral Cortex (New York, N.Y.: 1991)}, author={Sporns, O. and Tononi, G. and Edelman, G. M.}, year={2000}, month=feb, pages={127–141}, language={eng} }
@article{Teufel_Fletcher_2016, title={The promises and pitfalls of applying computational models to neurological and psychiatric disorders}, volume={139}, ISSN={0006-8950}, DOI={10.1093/brain/aww209}, abstractNote={Computational models have become an integral part of basic neuroscience and have facilitated some of the major advances in the field. More recently, such models have also been applied to the understanding of disruptions in brain function. In this review, using examples and a simple analogy, we discuss the potential for computational models to inform our understanding of brain function and dysfunction. We argue that they may provide, in unprecedented detail, an understanding of the neurobiological and mental basis of brain disorders and that such insights will be key to progress in diagnosis and treatment. However, there are also potential problems attending this approach. We highlight these and identify simple principles that should always govern the use of computational models in clinical neuroscience, noting especially the importance of a clear specification of a model’s purpose and of the mapping between mathematical concepts and reality.}, number={10}, journal={Brain}, author={Teufel, Christoph and Fletcher, Paul C.}, year={2016}, month=oct, pages={2600–2608} }
@book{VonNeumann_1986, title={The Computer and the Brain}, ISBN={978-0-300-18111-1}, url={https://www.jstor.org/stable/j.ctt5vkr1h}, abstractNote={In this classic work, one of the greatest mathematicians of the twentieth century explores the analogies between computing machines and the living human brain. John von Neumann, whose many contributions to science, mathematics, and engineering include the basic organizational framework at the heart of today’s computers, concludes that the brain operates both digitally and analogically, but also has its own peculiar statistical language.  In his foreword to this new edition, Ray Kurzweil, a futurist famous in part for his own reflections on the relationship between technology and intelligence, places von Neumann’s work in a historical context and shows how it remains relevant today.}, publisher={Yale University Press}, author={Von Neumann, John}, year={1986} }
@book{Wiener_1948, address={Oxford, England}, series={Cybernetics; or control and communication in the animal and the machine}, title={Cybernetics; or control and communication in the animal and the machine}, abstractNote={“Cybernetics” refers to the field of control and communications engineering, and is based on statistical mechanics. Statistical methods and quantum mechanics can be applied to the problem of predicting the future of a system. When this system utilizes feedback in its operation, it becomes similar in operation to the living organism and is covered by the mathematics of servo-mechanisms. Another similarity in function to the human nervous system is found in the ultra-rapid computing machine into which numerical values and logical rules are inserted and which then goes through a cycle comparable to the organism’s life cycle. This machine is concerned with the recording, preservation, transmission, and use of information in the form of a decision. It is shown that the mathematical procedures developed for communications engineering, machine calculation, and servo-mechanisms are applicable to human problems of motor dysfunction, formation and recognition of concepts, and the social effects of mass communication. Extensive mathematical derivations are presented. (PsycINFO Database Record (c) 2016 APA, all rights reserved)}, publisher={John Wiley}, author={Wiener, Norbert}, year={1948}, pages={194}, collection={Cybernetics; or control and communication in the animal and the machine} }
@article{Witsenhausen_1973, title={A standard form for sequential stochastic control}, volume={7}, ISSN={1433-0490}, DOI={10.1007/BF01824800}, abstractNote={Causal discrete-time stochastic control problems with nonclassical information patterns are not necessarily sequential in the sense that the order of the various control actions is fixed in advance. When this sequential condition does hold, then they can be reduced to a (theoretical) standard form which enables the establishment of a maximum principle, a reachable set concept and dynamic programming, all in a straightforward and transparent way.}, number={1}, journal={Mathematical systems theory}, author={Witsenhausen, H. S.}, year={1973}, month=mar, pages={5–11}, language={en} }
@article{Yongacoglu_Arslan_Yüksel_2023, journal={preprint}, title={Independent Learning in Mean-Field Games: Satisficing Paths and Convergence to Subjective Equilibria}, url={http://arxiv.org/abs/2209.05703}, DOI={10.48550/arXiv.2209.05703}, note={arXiv:2209.05703}, publisher={arXiv}, author={Yongacoglu, Bora and Arslan, Gürdal and Yüksel, Serdar}, year={2023}, month=nov }
@article{Young_Hilgetag_Burns_O’Neill_Scannell_Young_2000, title={Anatomical connectivity defines the organization of clusters of cortical areas in the macaque and the cat}, volume={355}, DOI={10.1098/rstb.2000.0551}, abstractNote={The number of different cortical structures in mammalian brains and the number of extrinsic fibres linking these regions are both large. As with any complex system, systematic analysis is required to draw reliable conclusions about the organization of the complex neural networks comprising these numerous elements. One aspect of organization that has long been suspected is that cortical networks are organized into ‘streams’ or ‘systems’. Here we report computational analyses capable of showing whether clusters of strongly interconnected areas are aspects of the global organization of cortical systems in macaque and cat. We used two different approaches to analyse compilations of corticocortical connection data from the macaque and the cat. The first approach, optimal set analysis, employed an explicit definition of a neural ‘system’ or ‘stream’, which was based on differential connectivity. We defined a two–component cost function that described the cost of the global cluster arrangement of areas in terms of the areas‘ connectivity within and between candidate clusters. Optimal cluster arrangements of cortical areas were then selected computationally from the very many possible arrangements, using an evolutionary optimization algorithm. The second approach, non–parametric cluster analysis (NPCA), grouped cortical areas on the basis of their proximity in multidimensional scaling representations. We used non–metric multidimensional scaling to represent the cortical connectivity structures metrically in two and five dimensions. NPCA then analysed these representations to determine the nature of the clusters for a wide range of different cluster shape parameters. The results from both approaches largely agreed. They showed that macaque and cat cortices are organized into densely intra–connected clusters of areas, and identified the constituent members of the clusters. These clusters reflected functionally specialized sets of cortical areas, suggesting that structure and function are closely linked at this gross, systems level.}, number={1393}, journal={Philosophical Transactions of the Royal Society of London. Series B: Biological Sciences}, publisher={Royal Society}, author={Young, M. P. and Hilgetag, Claus–C. and Burns, Gully A. P. C. and O’Neill, Marc A. and Scannell, Jack W. and Young, Malcolm P.}, year={2000}, month=jan, pages={91–110} }
@article{Zador_Escola_Richards_Ölveczky_Bengio_Boahen_Botvinick_Chklovskii_Churchland_Clopath_et_al._2023, title={Catalyzing next-generation Artificial Intelligence through NeuroAI}, volume={14}, rights={2023 The Author(s)}, ISSN={2041-1723}, DOI={10.1038/s41467-023-37180-x}, abstractNote={Neuroscience has long been an essential driver of progress in artificial intelligence (AI). We propose that to accelerate progress in AI, we must invest in fundamental research in NeuroAI. A core component of this is the embodied Turing test, which challenges AI animal models to interact with the sensorimotor world at skill levels akin to their living counterparts. The embodied Turing test shifts the focus from those capabilities like game playing and language that are especially well-developed or uniquely human to those capabilities – inherited from over 500 million years of evolution – that are shared with all animals. Building models that can pass the embodied Turing test will provide a roadmap for the next generation of AI.}, number={1}, journal={Nature Communications}, publisher={Nature Publishing Group}, author={Zador, Anthony and Escola, Sean and Richards, Blake and Ölveczky, Bence and Bengio, Yoshua and Boahen, Kwabena and Botvinick, Matthew and Chklovskii, Dmitri and Churchland, Anne and Clopath, Claudia and DiCarlo, James and Ganguli, Surya and Hawkins, Jeff and Körding, Konrad and Koulakov, Alexei and LeCun, Yann and Lillicrap, Timothy and Marblestone, Adam and Olshausen, Bruno and Pouget, Alexandre and Savin, Cristina and Sejnowski, Terrence and Simoncelli, Eero and Solla, Sara and Sussillo, David and Tolias, Andreas S. and Tsao, Doris}, year={2023}, month=mar, pages={1597}, language={en} }
@book{Coombes_2014, author={Coombes, Stephen and beim Graben, Peter and Potthast, Roland and Wright, James}, year={2014}, title={Neural fields: Theory and applications}, publisher={Springer}, address={London}}
@inproceedings{Gao_Caines_2019, title={Spectral Representations of Graphons in Very Large Network Systems Control}, ISSN={2576-2370}, url={https://ieeexplore.ieee.org/document/9030220/?arnumber=9030220}, DOI={10.1109/CDC40024.2019.9030220}, abstractNote={Graphon-based control has recently been proposed and developed to solve control problems for dynamical systems on networks which are very large or growing without bound (see Gao and Caines, CDC 2017, CDC 2018). In this paper, spectral representations, eigenfunctions and approximations of graphons, and their applications to graphon-based control are studied. First, spectral properties of graphons are presented and then approximations based on Fourier approximated eigenfunctions are analyzed. Within this framework, two classes of graphons with simple spectral representations are given. Applications to graphon-based control analysis are next presented; in particular, the controllability of systems distributed over very large networks is expressed in terms of the properties of the corresponding graphon dynamical systems. Moreover, spectral analysis based upon real-world network data is presented, which demonstrates that low-dimensional spectral approximations of networks are possible. Finally, an initial, exploratory investigation of the utility of the spectral analysis methodology in graphon systems control to study the control of epidemic spread is presented.}, booktitle={2019 IEEE 58th Conference on Decision and Control (CDC)}, author={Gao, Shuang and Caines, Peter E.}, year={2019}, month=dec, pages={5068–5075} }
@misc{Jabin_Poyato_Soler_2021, title={Mean-field limit of non-exchangeable systems}, url={https://arxiv.org/abs/2112.15406v3}, abstractNote={This paper deals with the derivation of the mean-field limit for multi-agent systems on a large class of sparse graphs. More specifically, the case of non-exchangeable multi-agent systems consisting of non-identical agents is addressed. The analysis does not only involve PDEs and stochastic analysis but also graph theory through a new concept of limits of sparse graphs (extended graphons) that reflect the structure of the connectivities in the network and has critical effects on the collective dynamics. In this article some of the main restrictive hypothesis in the previous literature on the connectivities between the agents (dense graphs) and the cooperation between them (symmetric interactions) are removed.}, journal={arXiv.org}, author={Jabin, Pierre-Emmanuel and Poyato, David and Soler, Juan}, year={2021}, month=dec, language={en} }
@article{Borgs_Chayes_Cohn_Zhao_2018, title={An $L^p$ theory of sparse graph convergence II: LD convergence, quotients, and right convergence}, volume={46}, ISSN={0091-1798}, url={http://arxiv.org/abs/1408.0744}, DOI={10.1214/17-AOP1187}, abstractNote={We extend the Lp theory of sparse graph limits, which was introduced in a companion paper, by analyzing diﬀerent notions of convergence. Under suitable restrictions on node weights, we prove the equivalence of metric convergence, quotient convergence, microcanonical ground state energy convergence, microcanonical free energy convergence, and large deviation convergence. Our theorems extend the broad applicability of dense graph convergence to all sparse graphs with unbounded average degree, while the proofs require new techniques based on uniform upper regularity. Examples to which our theory applies include stochastic block models, power law graphs, and sparse versions of W -random graphs.}, number={1}, journal={The Annals of Probability}, author={Borgs, Christian and Chayes, Jennifer T. and Cohn, Henry and Zhao, Yufei}, year={2018}, month=jan, language={en} }
@article{Legenstein_Maass_2014, title={Ensembles of spiking neurons with noise support optimal probabilistic inference in a dynamically changing environment}, volume={10}, ISSN={1553-7358}, DOI={10.1371/journal.pcbi.1003859}, abstractNote={It has recently been shown that networks of spiking neurons with noise can emulate simple forms of probabilistic inference through “neural sampling”, i.e., by treating spikes as samples from a probability distribution of network states that is encoded in the network. Deficiencies of the existing model are its reliance on single neurons for sampling from each random variable, and the resulting limitation in representing quickly varying probabilistic information. We show that both deficiencies can be overcome by moving to a biologically more realistic encoding of each salient random variable through the stochastic firing activity of an ensemble of neurons. The resulting model demonstrates that networks of spiking neurons with noise can easily track and carry out basic computational operations on rapidly varying probability distributions, such as the odds of getting rewarded for a specific behavior. We demonstrate the viability of this new approach towards neural coding and computation, which makes use of the inherent parallelism of generic neural circuits, by showing that this model can explain experimentally observed firing activity of cortical neurons for a variety of tasks that require rapid temporal integration of sensory information.}, number={10}, journal={PLoS computational biology}, author={Legenstein, Robert and Maass, Wolfgang}, year={2014}, month=oct, pages={e1003859}, language={eng} }
@article{Frégnac_2021, title={How Blue is the Sky?}, volume={8}, rights={https://creativecommons.org/licenses/by-nc-sa/4.0/}, ISSN={2373-2822}, DOI={10.1523/ENEURO.0130-21.2021}, abstractNote={The recent trend toward an industrialization of brain exploration and the technological prowess of artificial intelligence algorithms and high-performance computing has caught the imagination of the public. These impressive advances are fueling an uncontrolled societal hype, the more amplified, the more “Blue Sky” the claim is. Will we ever be able to simulate a brain in silico? Will “it” (the digital avatar) be conscious? The Blue Brain Project (BBP) and the European flagship the Human Brain Project (HBP) have surfed on this wave for the past 10 years. Their already significant lifetimes now offer new case studies for neuroscience sociology and epistemology, as the projects mature. Their distinctive “Blue Sky” flavor has been a key feature in securing unprecedented funding (more than one billion Euros) mostly through supranational institutions. The longitudinal analysis of these ventures provides clues to how the neuromyth they propagate sells science, in a scientific world based on an economy of promises.}, number={2}, journal={eneuro}, author={Frégnac, Yves}, year={2021}, month=mar, pages={ENEURO.0130-21.2021}, language={en} }
@article{Ma_Jazayeri_2014, title={Neural Coding of Uncertainty and Probability}, volume={37}, ISSN={0147-006X, 1545-4126}, DOI={10.1146/annurev-neuro-071013-014017}, abstractNote={Organisms must act in the face of sensory, motor, and reward uncertainty stemming from a pandemonium of stochasticity and missing information. In many tasks, organisms can make better decisions if they have at their disposal a representation of the uncertainty associated with task-relevant variables. We formalize this problem using Bayesian decision theory and review recent behavioral and neural evidence that the brain may use knowledge of uncertainty, confidence, and probability.}, number={Volume 37, 2014}, journal={Annual Review of Neuroscience}, publisher={Annual Reviews}, author={Ma, Wei Ji and Jazayeri, Mehrdad}, year={2014}, month=jul, pages={205–220}, language={en} }
@article{Parr_Markovic_Kiebel_Friston_2019, title={Neuronal message passing using Mean-field, Bethe, and Marginal approximations}, volume={9}, ISSN={2045-2322}, DOI={10.1038/s41598-018-38246-3}, abstractNote={Neuronal computations rely upon local interactions across synapses. For a neuronal network to perform inference, it must integrate information from locally computed messages that are propagated among elements of that network. We review the form of two popular (Bayesian) message passing schemes and consider their plausibility as descriptions of inference in biological networks. These are variational message passing and belief propagation - each of which is derived from a free energy functional that relies upon different approximations (mean-field and Bethe respectively). We begin with an overview of these schemes and illustrate the form of the messages required to perform inference using Hidden Markov Models as generative models. Throughout, we use factor graphs to show the form of the generative models and of the messages they entail. We consider how these messages might manifest neuronally and simulate the inferences they perform. While variational message passing offers a simple and neuronally plausible architecture, it falls short of the inferential performance of belief propagation. In contrast, belief propagation allows exact computation of marginal posteriors at the expense of the architectural simplicity of variational message passing. As a compromise between these two extremes, we offer a third approach - marginal message passing - that features a simple architecture, while approximating the performance of belief propagation. Finally, we link formal considerations to accounts of neurological and psychiatric syndromes in terms of aberrant message passing.}, number={1}, journal={Scientific Reports}, author={Parr, Thomas and Markovic, Dimitrije and Kiebel, Stefan J. and Friston, Karl J.}, year={2019}, month=feb, pages={1889}, language={eng} }
@article{Chialvo_2010, title={Emergent complex neural dynamics}, volume={6}, rights={2010 Springer Nature Limited}, ISSN={1745-2481}, DOI={10.1038/nphys1803}, abstractNote={A large repertoire of spatiotemporal activity patterns in the brain is the basis for adaptive behaviour. Understanding the mechanism by which the brain’s hundred billion neurons and hundred trillion synapses manage to produce such a range of cortical configurations in a flexible manner remains a fundamental problem in neuroscience. One plausible solution is the involvement of universal mechanisms of emergent complex phenomena evident in dynamical systems poised near a critical point of a second-order phase transition. We review recent theoretical and empirical results supporting the notion that the brain is naturally poised near criticality, as well as its implications for better understanding of the brain.}, number={10}, journal={Nature Physics}, publisher={Nature Publishing Group}, author={Chialvo, Dante R.}, year={2010}, month=oct, pages={744–750}, language={en} }
@article{Horsman_Stepney_Wagner_Kendon_2014, title={When does a physical system compute?}, volume={470}, ISSN={1364-5021, 1471-2946}, DOI={10.1098/rspa.2014.0182}, number={2169}, journal={Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences}, author={Horsman, Dominic and Stepney, Susan and Wagner, Rob C. and Kendon, Viv}, year={2014}, month=sep, pages={20140182}, language={en} }

