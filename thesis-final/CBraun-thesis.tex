\documentclass[12pt, oneside]{report}
\usepackage[margin=2cm]{geometry}

% Packages
\usepackage{amsmath, amsfonts, amssymb, amsthm}
\usepackage{bm}
\usepackage{bbm} 
\usepackage[shortlabels]{enumitem}
\usepackage{fancyhdr}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{sectsty}
\usepackage{setspace}
\usepackage{titlesec}
\usepackage[tracking]{microtype}
\usepackage{array, booktabs, ragged2e}
\usepackage[bookmarks,hypertexnames=false,debug,linktocpage=true,hidelinks]{hyperref}

% Palatino for text goes well with Euler \usepackage[sc,osf]{mathpazo}   % With
% old-style figures and real smallcaps. \linespread{1.025}              %
% Palatino leads a little more leading \setcounter{MaxMatrixCols}{20}  % Permits
% more columns in amsmath matrices

\allowdisplaybreaks 
\doublespacing

% \pagestyle{fancy}

\hypersetup{ colorlinks, linktoc=all, linkcolor={black}, citecolor={black},
urlcolor={black} }

\titleformat{\chapter}
  {\color{black}\normalfont\LARGE\bfseries} {\thechapter}{1em} {}[]

% \newpagestyle{mystyle} {\sethead[\thepage][][\chaptertitle]{}{}{\thepage}}
% \pagestyle{mystyle}

\sectionfont{\color{black}} \subsectionfont{\color{black}}

% Euler for math and numbers \usepackage[euler-digits,small]{eulervm}

% Command initialization \DeclareMathAlphabet{\pazocal}{OMS}{zplm}{m}{n}
\graphicspath{{./figures/}}

% Custom Commands
\newcommand{\bs}[1]{\boldsymbol{#1}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\var}[1]{\text{Var}\left(#1\right)}
\newcommand{\bp}[1]{\left({#1}\right)}
\newcommand{\mbb}[1]{\mathbb{#1}}
\newcommand{\1}[1]{\mathbbm{1}_{\{#1\}}}
\newcommand{\mc}[1]{\mathcal{#1}}
\newcommand{\nck}[2]{{#1\choose#2}}
\newcommand{\pc}[1]{\pazocal{#1}}
\newcommand{\ra}[1]{\renewcommand{\arraystretch}{#1}}
\newcommand*{\floor}[1]{\left\lfloor#1\right\rfloor}
\newcommand*{\ceil}[1]{\left\lceil#1\right\rceil}
\newcommand*{\td}[1]{\overset{\bm .}{#1}}
\newcommand{\vsismax}[1]{\Upsilon^{V}_{\leq#1}}
\newcommand{\vsisstrict}[1]{\Upsilon^{V}_{#1}}
\newcommand{\ssisstrict}[1]{\Upsilon^{S}_{#1}}
\newcommand{\ssismax}[1]{\Upsilon^{S}_{\leq#1}}

\DeclareMathOperator{\Var}{Var} \DeclareMathOperator{\Cov}{Cov}
\DeclareMathOperator{\diag}{diag} \DeclareMathOperator{\as}{a.s.}
\DeclareMathOperator{\ale}{a.e.} \DeclareMathOperator{\st}{s.t.}
\DeclareMathOperator{\io}{i.o.} \DeclareMathOperator{\wip}{w.p.}
\DeclareMathOperator{\iid}{i.i.d.}
\DeclareMathOperator{\ifff}{if\;and\;only\;if} \DeclareMathOperator{\inv}{inv}
\DeclareMathOperator{\DM}{DM} \DeclareMathOperator{\argmin}{argmin}
\DeclareMathOperator{\argmax}{argmax} \DeclareMathOperator{\erf}{erf}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}

\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{notation}[theorem]{Notation}
\newtheorem{example}[theorem]{Example}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{assumptions}[theorem]{Assumptions}
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{fig}[theorem]{Figure}

\begin{document}
\begin{titlepage}
\vspace*{\fill}
\begin{center}
    {\bf\Large Synthesis of Discrete Time Decentralized Control Models of Neuronal Networks}\\[20pt]
    {\bf by}\\[20pt]
    {\bf Connor Braun}\\[60pt]
    {\bf Queen's University}\\[10pt]
    {\bf Kingston, Ontario, Canada}\\[10pt]
    {\bf August 2024}
\end{center}
\vspace*{\fill}
\begin{center}
    {\it A THESIS SUBMITTED TO THE DEPARTMENT OF MATHEMATICS AND STATISTICS IN
    CONFORMITY WITH THE REQUIREMENTS FOR THE DEGREE OF MASTER OF SCIENCE}
\end{center}
\end{titlepage}
\begin{center}
    {\bf\Large Abstract}
\end{center}

A fundamental goal of modern neuroscience is to distill brain function into a
collection of computational objectives, and relate these to the structure and
dynamics of the biological networks implementing them. For this objective,
theoreticians have focused largely on dynamical models of neuronal activity,
imposing statistical priors on the parameters to obtain sufficiently realistic
dynamics. While this approach is not without merit, it is deficient in two ways.
First, the ensuing models can quickly become analytically intractable from a
dynamical point of view. Second, this approach leaves the relationship between
structure and function implicit. Either of these limitations inhibits the
development of satisfying mathematical theories of neurocomputation. This report marks the outset of a novel
strategy for studying neurocomputational processes in very large networks of
neurons, which will form the core of my subsequent PhD research. Building upon classical cybernetic perspectives of brain function,
neuronal networks are viewed as control systems striving to realize computations
as solutions to a set of optimal control problems. Some natural neuroscientific
postulates then further lead us to regard these as decentralized and cooperative
multi-agent systems, where each neuron is viewed as a individual agent,
generating local actions using only a locally-available information process. Control models
of this type fall in the category of stochastic teams, the theory of which is both well developed
and an active area of research. After motivating this perspective, we embark on the main objective of this report, which is to propose a discrete time stochastic team formulation for a biological neuronal network.
To this end, we first survey existing (uncontrolled) conductance-based and probabilistic network models,
and settle on a jump-diffusion model which we then show to be a stochastic realization of general integrate-and-fire methods. We then derive discrete time dynamics before
installing the necessary structure to elevate this model to a stochastic team problem. All design choices are explicitly motivated with some
biological or mathematical discussion. The idiosyncracies of the resulting team prevent one from immediately applying much of the existing theory,
so we conclude by reviewing some recent work on decentralized control in large-to-infinite populations which we hope to extend to
our model in future research.  
\newpage
\begin{center}
    \noindent{\bf\Large Acknowledgements}\\[5pt]
\end{center}

\indent I am deeply grateful for the mentorship and support of Professors Gunnar
Blohm and Serdar Y\"uksel; thank you for accepting me as a student, and for welcoming me into
the brilliant research groups you have so carefully crafted. I could not have learned so fast, nor been so productive
without our many discussions which so often altered my perspective entirely.
I am also thankful to Professors Sina Sanjari and Troy Day for agreeing to sit on my examination committee
and for their time in reviewing this report. 

\newpage
\thispagestyle{empty}
\vspace*{\fill}
\begin{center}
    \textit{To Jahanara, for her help making home in lands afar}
\end{center}
\vspace*{\fill}

\newpage
\tableofcontents
\chapter{Introduction}
\section{Impetus}
The notion that brain function is essentially computational has a long history
in neuroscientific thought \cite{Boole_1854} and was firmly established with
pioneering theoretical contributions of the mid 20th-century
\cite{McCulloch_Pitts_1943}, \cite{Wiener_1948}. Ever since, a core drive of
neuroscience has been to formally characterize brain computations
(neurocomputations, or NCs) supporting perception, behaviour, learning and
memory. This enterprise holds vast potential, both in the characterization (and
subsequent treatment) of neuropsychiatric debilitation
\cite{Rolls_Loh_Deco_Winterer_2008}, \cite{Teufel_Fletcher_2016} and for the
invention of powerful new computing hardware \cite{Mead_1990},
\cite{Schuman_Kulkarni_Parsa_Mitchell_Date_Kay_2022} along with the next
generation of artificial intelligence technologies \cite{Aimone_2019},
\cite{Zador_Escola_Richards_Ölveczky_Bengio_Boahen_Botvinick_Chklovskii_Churchland_Clopath_et_al._2023}.
Standing in the way of this vision is the overwhelming complexity of brain
composition and function: NCs are implemented in enormous biological networks
(consisting of possibly billions of cells) operating at various spatial and
temporal resolutions simultaneously \cite{Buzsáki_2010},
\cite{Betzel_Bassett_2017}, \cite{D’Angelo_Jirsa_2022}. To build models
traversing these multiple resolutions, theoreticians have relied largely on
dynamical descriptions of neurons \cite{Jirsa_2004}, \cite{Izhikevich_2007},
\cite{Breakspear_2017}, composing these to form mesoscale networks, and
subsequently obtaining mean field dynamics to characterize the flow of an
infinite network analogue -- often to great descriptive effect
\cite{Coombes_2014}, \cite{Pinotsis_Robinson_beim_Graben_Friston_2014}. However,
without an explicit functional interpretation (i.e., some task that the network
is trying to accomplish) it is difficult to associate the resulting dynamics
with NCs in any precise sense
\cite{Markram_Muller_Ramaswamy_Reimann_Abdellah_Sanchez_Ailamaki_Alonso-Nanclares_Antille_Arsever_et_al_2015},
\cite{Frégnac_2021}. In contrast with this "bottom-up" approach, various
"top-down" models exist in the domain of artificial intelligence
\cite{Richards_Lillicrap_Beaudoin_Bengio_Bogacz_Christensen_Clopath_Costa_deBerker_Ganguli_etal_2019}
which explicitly model putative NCs as objectives or tasks to be implemented by
an underlying network. Acknowledging the sheer diversity of formulations in this
category, such models (i) tend to ignore essential features of biological
networks, (ii) are often mathematically intractable, and (iii) do not admit
clear and well-defined solutions. Any subset of these limitations diminishes the
utility of the model, and consequently the development of convincing theories of
neurocomputation. In response, various authors have posited that new theoretical
frameworks uniting the bottom-up and top-down approaches are needed to study NCs
\cite{Jaeger_Noheda_vanderWiel_2023}, \cite{Aimone_Parekh_2023}.\\[5pt]
\indent I propose to develop a novel theoretical approach for studying NCs at
various resolutions in biologically-detailed network models. Ultimately, this
framework will be constructed as an applied extension of multi-agent systems
theory \cite{Radner_1962}, \cite{Witsenhausen_1973}, \cite{Yuksel_Basar_2024}
and more recent developments from the domain of mean field games
\cite{Huang_Malhame_Caines_2006}, \cite{Lasry_Lions_2007}. These mathematical
theories provide a highly general language to describe how very large systems of
complexly-interacting agents (which I envision as individual neurons) had ought
to behave in order to achieve some prescribed collective goal (i.e., a NC).
Technically speaking, neurocomputational goals are encoded by a control
objective, in the sense that the system must regulate its structure and function
to control its output in a prescribed manner. The ethos underlying this approach
is shared with classical cybernetic theories of NC \cite{Wiener_1948} although
the multi-agent angle is entirely new, to the best of my knowledge. In
particular, and unlike AI models, agents must learn and act using only
locally-observable information, rather than under the command of a global
learning algorithm. This property is called decentralization, and I argue that
it is an essential feature of biological neuronal networks; synonymous with the
neuroscientific concept of self-organization \cite{Park_Friston_2013}.\\[5pt]
\indent The main objective of this project is to obtain a discrete-time
stochastic team description (see appendix \ref{app2} for technical definitions) of biological neuronal networks, which is loosely
split into three subobjectives. First is to propose a class of pre-control (i.e.
purely dynamical) models, justifying this selection in light of the existing
literature and those biological properties of neurons which are deemed essential
to NC. Second is the technical task of discrete-time approximation, and finally
to bestow this model with all the necessary constructs for a stochastic team
realization. Each of these stages entails some degree of subjectivity, and indeed the greatest
challenge at this early stage is to biologically justify modeling decisions.
In this way I strive to resolve limitations (i), (ii) and (iii), listed above.
Summarizing, this research is an initial modeling effort and
foundation for subsequent work exploring the following hypothesis.\\[5pt]
\indent\textbf{Principle Hypothesis}. \textit{ Biological neuronal networks are
    decentralized, multi-agent systems striving to realize NCs as optimal
    solutions to a set of unknown control problems. As such, it is possible to
    construct mathematical models using multi-agent systems theory which
    explain/predict data. }\\[5pt]
\indent In the remainder of this chapter some basic notions from neurophysiology are reviewed. The
treatment is by no means exhaustive, and is instead directed toward a concise description of
individual neurons and the pairwise interactions between them.
These features will be the minimal elements needed to motivate our modeling efforts in the subsequent chapters.
Throughout the report, it will be necessary to distinguish
between biological and abstract neurons. I will adopt the convention that
\textit{neuronal} refers to the former, biological context, and \textit{neural} the
artificial one (i.e., a neuronal network versus a neural network). Further, artificial neurons may
be referred to as \textit{units}, \textit{decision makers} (DMs) or \textit{agents} interchangeably,
with preference for units in the context of uncontrolled models.
\section{Report Structure and Main Contributions}
The report is structured as follows: in chapter 2, we present a general overview of uncontrolled, bottom-up network
models from computational neuroscience. The discussion begins with Hodgkin Huxley-type, or conductance-based models, where we emphasize
various biological considerations and the importance of a well-defined spiking process, viewing these as characteristic of NCs under a
hypothesis of spike coding. This leads to a review of point process models, and a lesser-known jump-diffusion model of the neuronal membrane
which unites the more "mechanistic" (or biophysically-inspired) integrate-and-fire and probabilistic point process perspectives. We then obtain a discrete time model
for these uncontrolled dynamics.\\[5pt]
\indent With this review and the immediate technical considerations aside, in chapter 3 we develop a stochastic team model of neuronal networks,
introducing control, biologically compatible decentralized information structures and proposing some constraints for a permissible class of cost functions. Importantly, cost functions are not yet designed
with a specific, putative NC in mind, and instead are of a mathematically expedient form.\\[5pt]
\indent The resulting \textit{neural team problem} is somewhat idiosyncratic in the broader panorama of large-scale
stochastic control, so in chapter 4 we highlight some recent contributions/perspectives which will guide the immediate future of this project. These include some recent results
on the infinite population limit for uncontrolled models of our kind, and structural results on the optimal policy for exchangeable stochastic teams, from which the aforementioned "expedient" choice
of the cost function originates. We refer the reader unfamiliar with the language and basic notions of multi-agent cooperative stochastic control to the appendices.\\[5pt]
\indent To summarize, the main contribution of this report is the provision of:
\begin{enumerate}
    \item A broad survey of existing neuron modeling strategies, and critical reflection on their compatibility with
    our control perspective of neurocomputation.
    \item A mathematically rigorous exposition of point process theory for neuroscientific modeling, including the proof of a general technique
    for constructing point processes with stochastic, state dependent rates. While this result is often referenced, the only proof
    we were able to find (in \cite[theorem B.11]{Chevallier_Caceres_Doumic_Reynaud_Bouret_2015}) contains a minor technical error, which we correct.
    % This perspective, while classical in mathematics, is rarely addressed in the neuroscience literature -- even in works centered around probabilistic modeling.
    \item Results demonstrating that a class of jump-diffusion models can be viewed as a superset of traditional Hawkes-type neural networks,
    and stochastically approximate the fixed threshold behavior of traditional integrate-and-fire-type models.
    \item The derivation of a discrete time approximation for the unifying network model in (3).
    \item Novel interpretations of neuronal networks as stochastic teams and neurocomputation as a corresponding optimal control problem. We codify these ideas with a concrete, if preliminary, formulation,
    striving for parsimony and motivating our choices with biological considerations.
    \item A review of some existing results on the control of large-scale, exchangeable teams, which we would like to extend to our (neural) stochastic team problem as a matter of future research.
\end{enumerate}
\section{Elements of Neurophysiology}\label{sec4}
Nervous systems vary radically in structure and complexity between
organisms, from the three hundred two neurons comprising that of {\it
Caenorhabditis elegans} \cite{White_Southgate_Thomson_Brenner_1986} to the
roughly one hundred billion composing the brain of {\it Homo sapiens}
\cite{Herculano-Houzel_2009}. Despite this chasm of complexity, and the
behavioral specifications their corresponding nervous systems must fulfill, the
principle computational units -- the neurons themselves -- are both structurally
and functionally analogous, and in fact this conservation holds across much of
the animal kingdom \cite{Moroz_2009}, \cite{Sterling_Laughlin_2015}. While the
computational flexibility of these essential units hints at the possibility of a
general theory of neurocomputation, the staggering complexity of even small
circuits has posed a tremendous challenge to neuroscientists attempting to
describe how computations might emerge out of a system on the basis of network
structure \cite{Daur_Nadim_Bucher_2016}, \cite{Gjorgjieva_Drion_Marder_2016},
\cite{Haspel_et_al}. To handle this, a conventional approach has been to
stratify nervous systems into sufficiently granular levels of complexity and
study each individually \cite{Parker_2022}. For landmark examples,
\cite{Penfield_Jasper_1954} were able to establish the localization of various
functions to vaguely circumscribed regions of cerebral cortex, while around the
same time \cite{Hodgkin_Huxley_1952} constructed a detailed mathematical account
of the biophysical underpinnings of single neurons which, modulo some relatively
straightforward developments, remains the state-of-the-art for biophysically
realistic neuron modeling \cite{Ermentrout_Terman_2010}. In the decades since,
much of neuroscientific progress has been similarly categorizable: either as an
account of low level (individual neurons or small assemblies thereof) or high
level (entire brain regions consisting of millions or even billions of neurons)
function \cite{Parker_2022}. In accordance with the principle hypothesis, and
more specifically our agentic perspective on neural function, we subscribe to
the bottom-up approach and attempt to construct complex brain structures from
simpler constituents. We begin with individual neurons, which are the
indivisible elements comprising all nervous tissues.\\[5pt]
\begin{center}
    \includegraphics[width=140mm]{neuron-diagram.png}
\end{center}
\begin{fig}\label{fig1} Schematic diagram depicting two neurons and the basic
    structures/processes supporting their interaction. Relevant features are
    labelled: (i) dendrites, (ii) the axon hillock, (iii) the axon, (iv) an
    inset depicting chemical (synaptic) transmission at the axon terminal, and
    (v) an inset depicting the general appearance of neuronal spiking, as might
    be recorded at a point along the axon.
\end{fig}

\indent Neurons are a class of cells specialized to complex with one another to
form networks, and generate organized electrical (membrane potential, or
voltage) activity as the basis of intercellular communication. Crudely speaking,
signals flow unidirectionally between neurons, so it is sensible to describe
their components following an input-output sequence. Dendrites (\ref{fig1}.i),
collectively referred to as the dendritic tree/arbor, are a collection of highly
ramified projections which receive signals transmitted from other neurons. In
mouse cerebral cortex, it has been estimated that a single neuron can receive on the order
of eight thousand inputs \cite{Schüz_Palm_1989}. These signals propagate to the
cell body, or soma, where they are superimposed. This combined perturbation
travels to the axon hillock (\ref{fig1}.ii) which is the origination of the
output projection, called the axon (\ref{fig1}.iii). The axon extends away from
the soma, projecting up to $\sim$15 cm in the human brain
\cite{Liewald_Miller_Logothetis_Wagner_Schüz_2014}, ramifies, and impinges upon
the dendrites of other neurons. At the axon terminal, electrical signals are
transduced into a small release of some chemical (specific to the transmitting
neuron), called a neurotransmitter, which diffuses across a small ($\sim$20 nm)
gap (called a synapse) to induce a neurotransmitter-dependent perturbation on
a recipient cell (\ref{fig1}.iv).\\[5pt]
\indent Membrane composition at the axon hillock facilitates the generation of
action potentials, or spikes (\ref{fig1}.v) in an all-or-none fashion when
sufficiently excited by the combined dendritic input. Spikes are so-called for
their appearance; they are extremely brief ($\sim$1 ms) and morphologically
stereotyped waveforms which rapidly propagate along the axon by progressively
exciting the adjacent membrane. The basic biophysics of spiking have been
relatively well understood since the seminal work of \cite{Hodgkin_Huxley_1952}
and are reviewed here in preparation for some of the discussion in section
\ref{sec1}.\\[5pt]
\indent Neurons work by regulating the permeability of their outermost plasma
membrane to the ions present in the intracellular and extracellular
compartments. The resulting electrochemical gradient produces a transmembrane
voltage which varies as a function of membrane permeability to individual
species of ions. While the membrane itself is completely impermeable to all
ions, embedded within it are transmembrane channel proteins which can open and
close in response to changes in membrane potential (voltage-gated) or the
presence of neurotransmitters. Further, channels are selective in the sense that
a given type will often only admit passage of one ion species. Gating,
selectivity, and other properties of channels are a result of their structure,
and significant efforts have been made to construct bespoke models for different
channel species \cite[ch. 5,8]{Ermentrout_Terman_2010}. Ion concentrations remain
relatively constant in the intra- and extracellular compartments and will flow through open channels
until an equilibrium (or Nernst) membrane potential is reached and the
electrochemical gradient vanishes. For an ion species $X$ with formal charge
$z\in\{-1,1,2\}$ (e.g., $Na^+$, $K^+$, $Ca^{2+}$ or $Cl^-$) the equilibrium
potential is given by
\begin{align}
    E_X=\frac{RT}{zF}\log\left(\frac{[X]_{out}}{[X]_{in}}\right)
\end{align}
where $R$ is the gas constant, $F$ is Faraday's constant, $T$ is the temperature
in Kelvin, and $[X]_{out}$, $[X]_{in}$ indicate the concentration of ion species
$X$ in the extracellular and intracellular compartments, respectively. For
example, potassium ($K^+$) generally has a Nernst potential $E_K\approx -89.7$
mV \cite[$\S$ 2.5]{Johnston_Wu_1995}. The equilibrium potential accounting for
the simultaneous presence of multiple ion species is then given by the
Goldman-Hodgkin-Katz equation \cite[$\S$ 2.7.2]{Johnston_Wu_1995}. For a
collection of $M_1$ positive $\{X^+_i\}_{i=1}^{M_1}$ and $M_2$ negative
$\{X^-_j\}_{j=1}^{M_2}$ ion species and corresponding permeabilities $P^+_i\geq 0$ and
$P^-_j\geq 0$ (with units $\frac{\text{mol}}{\text{cm$\cdot$s}}$) the equilibrium
membrane potential is given by
\begin{align}
    V^\ast=\frac{RT}{F}\log\left(\frac{\sum_{i=1}^{M_1}P^+_i[X^+_i]_{out}+\sum_{j=1}^{M_2}P^-_j[X^-_j]_{in}}{\sum_{i=1}^{M_1}P^+_i[X^+_i]_{in}+\sum_{j=1}^{M_2}P^-_j[X^-_j]_{out}}\right)
\end{align}
which can be derived from biophysical principles \cite[$\S$
2.7]{Johnston_Wu_1995}. For our purposes, it is sufficient to observe that
neuronal voltage dynamics are a result of dynamic (voltage- or
neurotransmitter-gated) membrane permeabilities regulating the flow of
particular ion species. Unsurprisingly, this permeability is limited by the density of
channels over some area of membrane, and it is this parameter -- rather than the structure/properties of
the ion channels themselves -- that the subcellular machinery is able to control. This idea has been recently explored using
adaptive control models of single neurons \cite{Burghi_Sepulchre_2023}, \cite{Schmetterling_Burghi_Sepulchre_2022},
but has not been examined through the lens of decentralized optimal control. The relevance of the various
neurophysiological principles highlighted here will become apparent in the sequel, as we try to construct
an appropriate (uncontrolled) neuronal network model.

\chapter{Uncontrolled Network Models}\label{chap1}
In order to properly ground the development of controlled network models in the
existing computational neuroscience literature, and ultimately select an
appropriate model for a pilot treatment of neuronal networks as stochastic
teams, the primary objective of this chapter is to review well-studied modeling
strategies for treating the dynamics of finite networks of neurons in continuous
time. Ultimately, for systems as overwhelmingly complex as the brain, no one model
can be regarded as correct. Instead, models must be carefully crafted with respect
to an objective; meeting a set of methodological demands while preserving a
desired level of validity. The breadth of coverage here is motivated by this
proposition -- in particular a need to compare the abstractness, tractability
and expressiveness of existing models.\\[5pt]
\indent Attention will be constrained to conductance-based (CB), nonlinear
Hawkes (NH) and integrate-and-fire (IF) methods. The principle
difference between these is in how they each formalize a mechanism for
generating spikes. In the previous section, we saw that neuronal dynamics are of a hybrid
character; consisting of continuous voltage trajectories but with stereotyped,
discrete, temporally-localized waveforms (the spikes) which trigger events of
neurotransmission. It is widely argued that the informational content of a
network might be implicit in some population-level process based upon these
spikes, and moreover that computation might then reside in the laws governing
its evolution. Under this view, the resulting code would be fully characterized
by spike times, so as a postulate it is known as the temporal (or spike) coding
hypothesis \cite{Brette_2015}. In this way, representation could manifest as
statistical regularities (such as pairwise correlations, see \cite{Marre_El_Boustani_Frégnac_Destexhe_2009})
or dynamical phenomena \cite{Lisman_Buzsaki_2008} in the spike times, or even a
macroscopic distributional characterization of the spiking process itself
\cite{Buesing_Bill_Nessler_Maass_2011}, \cite{Habenschuss_Jonke_Maass_2013}. In light of the principle hypothesis, such
observations are to be viewed as byproducts of a decentralized system striving
for an optimum encoding some computational objective. This outlook 
prescribes two challenges: first, the model must admit a mathematically precise
notion of spiking. Second, the trajectories should be sufficiently expressive as
to be capable of capturing realistic neuron dynamics, where realism is supported
by a combination of biophysical and stochastic mechanisms. At the end of the chapter,
a soft-threshold variant of IF dynamics will be shown to most parsimoniously meet
these demands.\\[5pt]
\indent In $\S$\ref{sec1}, we present the general form of conductance based networks, including some discussion on
biophysical considerations in neuronal modeling, and the lack of a mathematically precise notion for spiking.\\[5pt]
\indent In $\S$\ref{sec5}, we present the main objects and propositions supporting a rigorous model for a spiking process in terms of a stochastic intensity (or instantaneous rate) process. This
discussion mainly follows \cite[ch.2,8]{Bremaud_1981}, with some supplementary references throughout, and builds toward an often utilized but rarely proven result
on realizing a large class of stochastic intensities via \textit{thinning}. For us, this is theorem \ref{thm1}, and both the statement and proof follow that found in
the relatively recent paper \cite[theorem B.11]{Chevallier_Caceres_Doumic_Reynaud_Bouret_2015} (although, we make a small technical correction -- see \ref{rem2}).\\[5pt]
\indent In $\S$\ref{sec6}, we introduce a less commonly utilized version of the generalized integrate-and-fire neuron model, and establish it as a generalization of a large class of multivariate neural point processes and
integrate-and-fire methods (with fixed spike thresholds) themselves (an original, if simple, contribution; see theorem \ref{thm2}).\\[5pt]
\indent Finally, in $\S$\ref{sec7}, we recast this generalized neuron model as a jump-diffusion process and present an alternative thinning formulation (proposition \ref{prop4}) for which questions of solution existence/uniqueness (theorem \ref{thm4}) and
discrete time approximation (theorem \ref{thm13}) are addressed. 
\section{Conductance-Based Networks}\label{sec1}
Conductance-based models, introduced by \cite{Hodgkin_Huxley_1952}, are those
which directly model the biophysics of the neuronal membrane, complete with
dimensional parameters and state variables \cite{Ermentrout_Terman_2010}, \cite{Izhikevich_2007}. The main advantages of this class of models, stemming from
their mechanistic fidelity, is the biological interpretability of their
states/parameters and continuous dynamics. However, coincident with these
theoretical and technical benefits are several shortcomings. Primarily,
rigorously localizing spike times is at best nontrivial \cite{Kistler_Gerstner_Hemmen_1997}, \cite{Platkiewicz_Brette_2010}, \cite{Wang_Wang_Yu_Chen_2016}
but potentially ill-conceived altogether \cite{Koch_Bernander_Douglas_1995}, \cite{Rinzel_Ermentrout_1998} such that a simple threshold-based criterion
is not available. Naturally, this confounds a mathematical study of CB spike
coding. A second criticism (albeit less relevant to this report) is that
CB models are quite complex, with even the original single neuron model
requiring the specification of over twenty parameters. Moreover, the structure
of CB models renders even single-unit models non-identifiable with respect to
typically available data \cite{Walch_Eisenberg_2016}. In short, this means the
mapping from a parameter space to the model output is non-invertible, such that
no amount of data can uniqely identify a 'true' parameterization (or
equivalently, eliminate uncertainty) under a given objective and learning
procedure. Side effects of this equifinality include initialization sensitivity
of optimization algorithms and failure to extrapolate \cite{Daly_Gavaghan_Holmes_Cooper_2015},
\cite{Guillaume_et_al_2019}. Nevertheless, the efficiency of
data-driven learning procedures has been steadily improving \cite{Prinz_Billimoria_Marder_2003},
\cite{Meliza_Kostuk_Huang_Nogaret_Margoliash_Abarbanel_2014}. While such practical challenges lie adjacent to the more
theoretically-focused setting (where the goal is to obtain
parameter-insensitive results), the point is that deployment of theory in experiment and simulation
is stifled by the (possibly) inessential complexities of CB models. Nonetheless, they
serve as a point of biological comparison for the more physically
remote/macroscopic models considered.\\[5pt]
\indent Consider a finite network of $N$ units ndexed by $\mc{N}$. Fix
$\mc{S}=\{\mc{S}_m\}_{m=1}^M$ a $M$-partition of $\mc{N}$, specifying
subpopulations sharing the same molecule of neurotransmission. The network
topology is a possibly asymmetric graph $\mc{G}=(\mc{N},\mc{E})$ assigning a
node to each unit, so that $(i,j)\in\mc{E}$ implies that unit $j$ impinges upon
$i$. To deploy these ideas, define $\mc{G}(i):=\{j\in\mc{N}:(i,j)\in\mc{E}\}$
the set of units presynaptic to $i$ and
$\mc{S}(i):=\argmax_m\{\1{i\in\mc{S}_m}\}$ the subpopulation $i$ belongs to. For
$i\in\mc{N}$, the equivalent circuit of a neuronal membrane indicates that the
charge stored at the membrane at time $t\geq 0$ is $c^iv^i_t$, with $c^i>0$ the
membrane capacitance and $v^i_t$ the membrane potential, or voltage. The time
derivative of this quantity is current, whereby Kirchoff's current law the
superposition of this with all other membrane currents vanishes, yelding the
voltage dynamics
\begin{align}
    c^i\frac{dv^i_t}{dt}=-I^i_\ell(v^i_t)-\sum_{k\in\mc{C}_I}I^i_k(v^i_t,t)-\sum_{j\in\mc{G}(i)}I^i_j(v^i_t,v^j_t,t)\label{eq17}
\end{align}
where $\mc{C}_I$ indexes a collection of intrinsic conductances. Similarly, the
currents $I^i_j(v^i_t,v^j_t,t)$ are called extrinsic conductances, and
$I^i_\ell(v^i_t)$ is called passive, or a leak. A technical definition of these
gives way to some useful discussion of the underlying biology.
\begin{definition}[Intrinsic Conductance]\label{def11} For $i\in\mc{N}$,
    $k\in\mc{C}_I$, a voltage-dependent current $I^i_k$ is called an {\it
    intrinsic conductance} if
    \[I^i_k(v^i_t,t)=g^i_k(m^i_k(t))^{p_k}(h^i_k(t))^{q_k}(v^i_t-E_k)\] with
    parameters $g^i_k\geq0$, $E_k\in\mbb{R}$, $p_k\in\mbb{N}$, $q_k\in\mbb{N}$.
    The $[0,1]$-valued trajectories $m^i_k$ and $h^i_k$ are solutions of the
    voltage-dependent dynamics $\td{m}^i_k=f_{m_k}(m^i_k,v^i_t)$,
    $\td{h}^i_k=f_{h_k}(h^i_k,v^i_t)$ (with some initial $m^i_k(0)$,
    $h^i_k(0)\in[0,1]$). Both
    $f_{m_k},f_{h_k}:[0,1]\times\mbb{R}\rightarrow\mbb{R}$ are jointly Lipschitz
    with $f_{m_k}(m^\prime,\cdot):\mbb{R}\rightarrow\mbb{R}$ nondecreasing and
    $f_{h_k}(h^\prime,\cdot):\mbb{R}\rightarrow\mbb{R}$ nonincreasing $\forall
    m^\prime,h^\prime\in[0,1]$. If $p_k=q_k=0$, then $I^i_k$ is additionally
    called {\it passive}, or a {\it leak}.
\end{definition}
\begin{remark}
    The parameters $g^i_k$, $E_k$ are called the {\it maximal conductance} and
    {\it Nernst potential}, respectively. The dynamic variables $m^i_k$,
    $h^i_k$ are called {\it gating variables}, and in particular the {\it
    activation} and {\it inactivation} gates for their monotonic voltage
    dependence. Finally, the dynamics $f_{m_k}$, $f_{h_k}$ are parametric, and
    of the form $f_{x}(z,v)=-z+\sigma(v)$ for $x\in\{m_k,h_k\}$, where here
    $\sigma:\mbb{R}\rightarrow (0,1)$ is a logistic sigmoid function \cite{Hodgkin_Huxley_1952}.
    Variations on this classical model have been designed to fit
    the behavior a veritable zoo of channel species, but do not stray far from
    the general form of (\ref{def11}) \cite[ch.5]{Ermentrout_Terman_2010}.
\end{remark}
Intrinsic here refers to the sole dependence of the current on the local
voltage. The indexing in (\ref{def11}) additionally highlights the parameter
dependence structure of intrinsic conductances: only the maximal conductances
$g^i_k$ vary with $i$. Even the gating dynamics are independent of the identity
of the unit on which they reside. This is biophysically motivated, since the
structure (and therefore dynamics) of the protein mediating these currents is
conserved across neurons. Further, the Nernst potential $E_k$ depends on the
concentration of the ion species comprising $I^i_k$. Supposing these
concentrations to be homogenous over the network medium and temporally constant
are physically weak assumptions, given the tiny magnitude of fluxions relative
to the extra/intracellular volume. On the other hand, the $g^i_k\geq0$
correspond to the density of conducting protein $k$ in a patch of membrane,
which is regulated by local intracellular machinery -- hence the $i$-dependence.
\begin{definition}[Extrinsic Conductance]\label{def13} For $i,j\in\mc{N}$, the
    doubly voltage-dependent current $I^i_j$ is called an {\it extrinsic
    conductance} (or a synaptic current, or synapse) if
    \[I^i_j(v^i_t,v^j_t,t)=w^i_jq_{\mc{S}(j)}s^i_j(t)(v^i_t-E_{\mc{S}(j)})\]
    where the $w^i_j\in\mbb{W}^i_j$ and for some
    $\overline{w}^i_j\geq\underline{w}^i_j\geq 0$,
    $\mbb{W}^i_j:=[\underline{w}^i_j,\overline{w}^i_j]$. The polarity of the
    synapse is encoded by $q_{\mc{S}(j)}\in\{-1,1\}$, and
    $E_{\mc{S}(j)}\in\mbb{R}$ is the Nernst potential. The $[0,1]$-valued
    trajectory $s^i_j$ is either (i) the solution of the $v^j_t$-dependent
    dynamics $\td{s}^i_j=\widetilde{f}_{\mc{S}(j)}(s^i_j,v^j_t)$ for some
    $s^i_j(0)\in[0,1]$ fixed, in which case $I^i_j$ is called {\it dynamic}, or
    (ii) given by $s^i_j(t)=\widehat{f}_{\mc{S}(j)}(v^j_t)$, in which case
    $I^i_j$ is called {\it static}. In either case, $\widetilde{f}_{\mc{S}(j)}$
    or $\widehat{f}_{\mc{S}(j)}$ are assumed Lipschitz on their respective
    domains.
\end{definition}
Extrinsic conductances are so-called because they depend on both a pre- and
postsynaptic voltage. Biophysically, the synaptic gating variable $s^i_j$
depends on $v^j_t$ through the spike-induced release of chemical
neurotransmitters from the presynapse. The postsynaptic proteins selectively
responsive to this neurotransmitter share the same structure, which is why the
functions $\widetilde{f}_{\mc{S}(j)}$, $\widehat{f}_{\mc{S}(j)}$ and parameters
$E_{\mc{S}(j)}$, $q_{\mc{S}(j)}$ depend on $j$ only through its subpopulation
membership. In fact, this is the only purpose of the subpopulation object
$\mc{S}$; to assign different signalling mechanisms to different subsets of the
network as a means of respecting the varied chemicals of neurotransmission in
the brain. In contrast with these subpopulation-dependent quantities, the
weights $w^i_j$ may vary pairwise between all units, but are again interpretable
as maximal (synaptic) conductances (except now corresponding to postsynaptic
receptor densities). Crucially, this means that $w^i_j$ is -- informally
speaking -- controlled by unit $i$ for all $i,j\in\mc{N}$. I will refer to these
as weights to distinguish them from intrinsic maximal conductances, despite
their shared physicality. Finally, since this report is unconcerned with any
particular model parameterizations, the subpopulation structure will henceforth
be suppressed.
\begin{example}[Dynamic and Static Synapses]
    A simple dynamical model \cite[$\S$ 8.1]{Ermentrout_Terman_2010} for the evolution of
    synaptic gate $s^i_j$ is given by
    \begin{align}
        \tau_{j}(v^j_t)\td{s}^i_j=-s^i_j+\alpha_{j}\tau_{j}(v^j_t)\sigma_{j}(v^j_t)\label{eq27}
    \end{align}
    where
    $\tau_{j}(v^j_t):=(\alpha_{j}\sigma_{j}(v^j_t)+\beta_{j})^{-1}$
    is a voltage-dependent time constant,
    $\sigma_{j}:\mbb{R}\rightarrow(0,1)$ is a logistic sigmoid function
    and $\alpha_{j},\beta_{j}>0$ are parameters. Alternatively, a static synapse
    could be given by another logistic sigmoid
    $s^i_j(t)=\sigma_{j}(v^j_t)$, parameterized such that the gate
    'opens' briefly when $j$ spikes. Static functions in general make for
    exceptionally crude models of real synapses.
\end{example}
With this discussion we are prepared to formally define the CB network model.
\begin{definition}[Conductance-Based Network]\label{def12} For $i\in\mc{N}$, define
    the continuous time
    $\mbb{X}^i\subseteq\mbb{R}^{1+2|\mc{C}_I|+|\mc{G}(i)|}$-valued state process
    $x^i_t$ with trajectory
    $x^i_t:=(c^iv^i_t,m^i_k,h^i_k,s^i_j)_{k\in\mc{C}_I,j\in\mc{G}(i)}$
    satisfying
    $\td{x}^i_t=(f_{v^i},f_{m_k},f_{h_k},\widetilde{f}_{\mc{S}(j)})_{k\in\mc{C}_I,j\in\mc{G}(i)}$
    with some initial state $x^i_0\in\mbb{X}^i$. In particular, if $f_{v^i}$ is
    given by (\ref{eq17}) with $I^i_\ell$ passive, $I^i_k$ intrinsic, and
    $I^i_j$ extrinsic for all $i\in\mc{N}$, $j\in\mc{G}(i)$ and $k\in\mc{C}_I$,
    then the trajectory $x_t:=(x^i_t)_{i\in\mc{N}}$ is that of a {\it
    conductance-based network} with state space
    $\mbb{X}:=\prod_{i\in\mc{N}}\mbb{X}^i$. 
\end{definition}
In definition (\ref{def12}) and the rest of this report, the full
parameterization of the CB network is suppressed, but understood to be a fixed
element of a suitable subset $\Theta\subset\mbb{R}^{N\times n}$ for some
$n\in\mbb{N}$. However, maximal conductances $g^i_k$ and weights $w^i_j$, which
are locally controlled by individual neurons (in a biological sense) are to be
excluded from $\Theta$ for the purpose of later viewing them as proper control
variables. Thus, to each $i\in\mc{N}$ define
$g^i:=(g^i_\ell,g^i_k)_{k\in\mc{C}_I}\in\mbb{R}_+^{|\mc{C}_I|+1}$ and
$g:=(g^i)_{i\in\mc{N}}$. Similarly, write
$w^i:=(w^i_j)_{j\in\mc{N}}\in\prod_{j\in\mc{N}}[\underline{w}^i_j,\overline{w}^i_j]=:\mbb{W}^i$,
$w:=(w^i_j)_{i,j\in\mc{N}}\in\prod_{i\in\mc{N}}\mbb{W}^i=:\mbb{W}$ and note
that the network topology $\mc{G}$ can be instantiated by the matrix $w$ by
requiring $\mbb{W}^i_j=\{0\}$ if and only if $(i,j)\notin\mc{E}$. With these, CB
network dynamics shall be more compactly expressed with
\begin{align}
    \td{x}^i_t:=f^i(x_t,g^i,w^i),\quad\text{and}\quad \td{x}_t:=F(x_t,g,w)\label{eq18}
\end{align}
and, in particular, both $f^i$ and $F$ are continuous. As mentioned, biological neuronal
networks are inherently noisy, especially at the microscale \cite{Faisal_Selen_Wolpert_2008} motivating the inclusion
of some membrane stochasticity. For this, let $B^i_t$ be a
$1+2|\mc{C}_I|+|\mc{G}(i)|$-dimensional standard Brownian motion with
$B_t:=(B_t^i)_{i\in\mc{N}}$ adapted to some filtration $\mc{F}_t$ on a subjacent
probability space $(\Omega,\mc{F},P)$. Define the symmetric, constant matrices
$\Sigma^i:=\diag(\sigma^i,0,0\dots,0)$ where $\sigma^i>0$ characterizes noise in
the voltage channel for $i\in\mc{N}$. A simple stochastic variant of
(\ref{eq18}) is given by an It\^o process
\begin{align}
    dx^i_t=f^i(x_t,g^i,w^i)dt+\Sigma^idB^i_t,\quad\text{and}\quad dx_t=F(x_t,g,w)dt+\Sigma dB_t\label{eq24}
\end{align}
where $\Sigma:=\diag(\Sigma^i)_{i\in\mc{N}}$ is a diagonal matrix of matrices.
For simplicity, the $(\sigma^i)_{i\in\mc{N}}$ are assumed constant and
considered part of the (suppressed) network parameterization. The convoluted
structure of $\Sigma$ is merely to render the voltage channels noisy, but leave
the gate dynamics deterministic. This is to respect the fact that gates already
represent the average binary state (open/closed) of a population of independent,
but stochastic channels. In turn, these are the source of voltage
stochasticity, since random gate transitions result in a myriad superposition of
tiny, transient and independent currents \cite{Destexhe_Mainen_Sejnowski_1994}.\\[5pt]
\indent Of course, not every model in the class of CB networks will produce neuron-like dynamics. The seminal
work of \cite{Hodgkin_Huxley_1952} is the origin of CB modelling, but moreover
demonstrated how a single (i.e., $|\mc{N}|=1$) unit can be made to exquisitely
reproduce voltage trajectories obtained from isolated neurons. In particular,
this was done with just a trio of intrinsic conductances, which now form the
canonical biophysical mechanism of neuronal spiking. 
\begin{example}[Hodgkin-Huxley Neuron Model]\label{ex1} Consider a CB network of
    just one unit. The Hudgkin-Huxley neuron model is given by the system of
    nonlinear differential equations 
    \begin{align*}
        c\frac{dv_t}{dt}&=-g_\ell(v_t-E_\ell)-g_{Na}m^3_{Na}(t)h_{Na}(t)(v_t-E_{Na})-g_Km^4_K(t)(v_t-E_K)+I(t)\\
        \tau_x(v_t)\frac{dx}{dt}&=-x+\phi_x(v_t)
    \end{align*}
    for intrinsic gating variables $x\in\{m_{Na},h_{Na},m_K\}$, $\phi_x$ a
    logistic sigmoid function, and $I(t)$ an arbitrary forcing current to drive
    activity in lieu of synaptic input.
\end{example}
While parameterizations for (\ref{ex1}) can vary, spiking critically relies on
the fast dynamics of sodium activation (that is, for $v\in\mbb{R}$,
$\tau_{m_{Na}}(v)<\tau_{m_K}(v),\tau_{h_{Na}}(v)$) \cite{Gerstner_Kistler_Naud_Paninski_2014},
\cite{Sepulchre_2022}. This spiking mechanism is nigh ubiquitous in neurons,
underscoring the fact that spikes are generated by the dynamics of intrinsic
gating variables depending on the local voltage, which is in turn  by
extrinsic conductances depending on nonlocal voltages (via spikes). This
apparent functional separation between intrinsic and extrinsic conductances aids
the development of the simpler phenomenological models to come.\\[5pt]
\indent As mentioned at the outset, precisely defining spike times from
solutions of (\ref{eq18}) and (\ref{eq24}) is highly nontrivial. Based on the
observation that voltage trajectories in real neurons appear to be separable by
a boundary into spiking and non-spiking regimes \cite{Brunel_van_Rossum_2007} one might seek
to formalize this mathematically via a voltage-threshold criterion
$\{\vartheta^i_t\}_{t\geq 0}$ for $i\in\mc{N}$ such that,
\begin{align}
    T^i_{k+1}:=\inf\{t\geq T^i_k+\delta_k:v^i_t\geq\vartheta^i_t\}\label{eq19}
\end{align}
with $T^i_k$ and $\delta_k>0$ the time and duration (respectively) of the $k$-th spike from
unit $i$. Efforts at finding closed form expressions for such a $\{\vartheta^i_t\}_{t\geq 0}$ process, even
in the case of isolated Hodgkin-Huxley units, have been largely fruitless \cite{Platkiewicz_Brette_2010}, \cite{Wang_Wang_Yu_Chen_2016}.
Instead, whether or not a stimulus recruits a
sufficient mass of sodium channels to initiate a spike depends jointly on the
dynamics of the neuron and stimulus \cite{Koch_Bernander_Douglas_1995}, \cite{Rinzel_Ermentrout_1998}.
Even the existence of such a $\delta_k$ could be ill-posed, since units smoothly transition from
spiking back to non-spiking via a period of relative refractoriness. Thus,
despite the apparent threshold nature of spiking, analytically precise rules such as (\ref{eq19}) are not available for CB networks.
This is a nonstarter for any theoretical treatment of neurocomputation formulated upon a spike coding postulate, such as we will later begin to construct. 
One way of resolving this is to consider more abstract models, where we regard CB networks as retaining too many inessential or otherwise \textit{implementational} details (i.e.,
those spent capturing the biophysical \textit{implementation} of neuronal dynamics, rather than simply describing them phenomenologically) which obscures
a richer underlying mathematical structure. It is perhaps unsurprising, given our emphasis on the computational significance of spiking,
that such simplifications would find purchase in the theory of point processes.
\section{Point Process Networks}\label{sec5}
Point process models of neuronal activity have seen routine use in neuroscience as a tool for inference
\cite{Brillinger_1988}, \cite{Chornoboy_Schramm_Karr_1988} dynamical modeling \cite{Reynaud-Bouret_Rivoirard_Grammont_Tuleau-Malot_2014},
\cite{Chevallier_Caceres_Doumic_Reynaud_Bouret_2015}, \cite{Gerhard_Deger_Truccolo_2017} and obtaining
information-theoretic results \cite{Johnson_1996}. Motivated by the inherent,
multiscale stochasticity of neurons \cite{Faisal_Selen_Wolpert_2008} and discrete nature of
spiking, these employ an abstract excitation process
(which can be defined in terms of both external and network stimuli \cite{Pillow_Shlens_Paninski_Sher_Litke_Chichilnisky_Simoncelli_2008})
to define an instantaneous spiking probability. Spikes are
represented by jumps or points, which are treated as indistinguishable except
for the times at which they occur. In contrast with CB models, point processes
are highly structured and well understood, but this comes at the expense of any
direct biophysical meaning. Hawkes processes are a particularly relevant class
of self-exciting point process introduced by \cite{Hawkes_1971}. They, along with
their nonlinear generalizations \cite{Bremaud_Massoulié_1996}, permit a
spiking unit to broadcast an excitatory/inhibitory effect through functions
approximating the time course of real synaptic currents, making them the
principle choice for modeling neuronal networks. Existing work on these has
focused on the stability of collective spiking processes \cite{Bremaud_Massoulié_1996}, \cite{Borovkov_Decrouez_Gilson_2014} but controlled variants have either not been
studied or else remain esoteric (to the best of my knowledge). In this section, largely following
\cite{Bremaud_1981}, \cite{Chevallier_Caceres_Doumic_Reynaud_Bouret_2015}, \cite{Chevallier_2017}, fundamental notions of
point process theory are presented in order to formally introduce
Hawkes-type models, along with some supporting results which are used in
the sequel to combine these with more mechanistically descriptive dynamics.\\[5pt]
\indent As usual, take $(\Omega,\mc{F},P)$ to be the underlying probability
space on which all random variables are defined. Of primary concern are point
processes over $(\mbb{R}_+,\mc{B}(\mbb{R}_+))$, but I will begin with a
definition for more general spaces (Karr, 1991).
\begin{definition}[Point Process]\label{def8} Let $\mbb{X}$ be a locally compact
    Polish space and $M$ the set of Radon measures on $\mbb{X}$. Equip this with
    the topology of vague convergence and its corresponding Borel
    $\sigma$-algebra $\mc{M}$. A random measure $Z:\Omega\rightarrow M$ is a
    $\mbb{X}$-{\it point process} if $Z(A)\in\mbb{Z}_+\cup\{+\infty\}$ for all
    $A\in\mc{B}(\mbb{X})$, and $Z(\mbb{X})=+\infty$ almost surely.
\end{definition}
\begin{remark}
    For narrative completeness: a measure $\mu$ on a locally compact Polish
    space is {\it Radon} if $\mu(A)<\infty$ whenever $A\in\mc{B}(\mbb{X})$ is
    bounded. Then a sequence $\mu_n$ of such measures is said to converge {\it
    vaguely} to $\mu$ iff $\int fd\mu_n\rightarrow\int fd\mu$ as
    $n\rightarrow\infty$ for all continuous and
    compactly supported real functionals $f$ on $\mbb{X}$. 
\end{remark}
Such a process is additionally called simple (or regular) if $Z(\{x\})\leq 1$
$\forall x\in\mbb{X}$. For point processes in time one associates to $Z$ the
c\'adl\'ag counting process $Z_t:=Z([0,t])$. Perhaps more intuitively, such a
process can be represented by a random sequence $\{T_k\}_{k\geq 0}$ in
$\mbb{R}_+$ satisfying $T_0=0$ and $T_k<T_{k+1}$ almost surely. These are
equivalent, as can be seen by setting $Z(A):=\sum_{k\geq 1}\delta_{T_k}(A)$ or
conversely $T_k:=\inf\{t\geq 0: Z_t=k\}$ for $k\geq 1$. For this reason, and by
an abuse of notation, $T\in Z$ can be taken to mean $T$ is a point in the
process $Z$. In any case, a point process is called locally finite or
nonexplosive iff $Z_t<\infty$ for $t\in\mbb{R}_+$ almost surely. Of course,
acceptable neuron models had ought to be locally finite, and simple technical conditions
for guaranteeing this are forthcoming (see lemma \ref{lem1}). The general
program of point process modeling is to relate the process history to the
generation of new points in a recursive fashion. However, for this loop to be
causal a stronger notion of measurability (called predictability) with respect
to a filtration is required.
\begin{definition}[Natural and Predictable Filtration of a Point
    Process]\label{def17} The {\it natural filtration} (resp. {\it predictable
    filtration}) of a point process $Z$ is the family of $\sigma$-algebras
    $\{\mc{F}^Z_t\}_{t\geq 0}$ (resp. $\{\mc{F}^Z_{t^-}\}_{t\geq 0}$) given by
    $\mc{F}^Z_t:=\sigma(Z(A):A\in\mc{B}(\mbb{R}_+), A\subseteq[0,t])$  (resp.
    $\mc{F}^Z_{t^-}:=\sigma(Z(A):A\in\mc{B}(\mbb{R}_+), A\subseteq [0,t))$).
\end{definition}
The usual interpretation of the natural filtration holds here: that $\mc{F}^Z_t$
contains information generated by $Z$ up to time $t$. On the other hand,
$\mc{F}^Z_{t^-}$ contains all information strictly preceding $t$, and a process
$\{C_t\}_{t\geq 0}$ is called $\mc{F}^Z_t$-predictable iff $C_t$ is
$\mc{F}^Z_{t^-}$-measurable $\forall t\geq 0$. With this, the main object
permitting a probabilistic description of $Z_t$, called a stochastic intensity,
is a nonnegative, $\mc{F}^Z_{t}$-predictable and $\as$ locally integrable
process $\lambda_t$ which characterizes the probability that a point occurs over
infinitessimal timespans:
\begin{align}
    P(Z([t, t+h])=1|\mc{F}^Z_t)=\lambda_th+o(h)\label{eq13}
\end{align}
$\forall t\geq 0$, provided $h$ is sufficiently small (and where $o(h)$ means
$o(h)/h\rightarrow 0$ as $h\rightarrow 0$). Such probabilistic descriptions are commonly employed
in neuroscience, and have been historically associated to membrane dynamics through the notion of "escape noise" \cite{Plesser_Gerstner_2000}, \cite{Habenschuss_Jonke_Maass_2013}.
However, (\ref{eq13}) is difficult to work with, and a more practical definition can be used to encapsulate this idea.
\begin{definition}[Stochastic Intensity]\label{def9} Let $Z_t$ be a point
    process, and $\lambda_t$ a nonnegative, $\mc{F}^Z_{t}$-predictable process
    such that $\int_0^t\lambda_sds<\infty$ $\forall t\geq 0$ $P$-almost surely.
    If for all $\mc{F}^Z_t$-predictable processes $C_t$, $\lambda_t$ satisfies
    \begin{align*}
        \E\left(\int_{0}^\infty C_sZ(ds)\right)=\E\left(\int_{0}^\infty C_s\lambda_sds\right)
    \end{align*}
    then $\lambda_t$ is called the $\mc{F}^Z_t$-{\it predictable intensity} (or
    just $\mc{F}^Z_t$-intensity) of $Z_t$. Here, the integral is defined in the Riemann-Stieltjes
    sense so that $\int_0^t C_sZ(ds):=\sum_{k\geq 1}C_{T_k}\1{T_k\leq t}$ $\forall
    t\in\mbb{R}_+$, and $\int_0^\infty C_sZ(ds):=\sum_{k\geq
    1}C_{T_k}\1{T_k<\infty}$.
\end{definition}
The next result \cite[ch.2, theorem 8]{Bremaud_1981} highlights some of the salient benefits of
definition (\ref{def9}).
\begin{lemma}[Br\'emaud (1981)]\label{lem1} If a point process $Z_t$ admits the
    $\mc{F}^Z_t$-intensity $\lambda_t$ in the sense of (\ref{def9}), then i)
    $Z_t$ is locally finite and ii) $Z_t-\int_0^t\lambda_sds$ is a
    $\mc{F}^Z_t$-local martingale.
\end{lemma}
With this local martingale property, a localizing sequence $\{S_n\}_{n\geq 1}$
of $\mc{F}^Z_t$-stopping times and $h>0$,
\begin{align*}
    \lim_{n\rightarrow\infty}\E\left(\int_{t\wedge S_n}^{t+h\wedge S_n}Z(dr)\bigg|\mc{F}^Z_t\right)&=\lim_{n\rightarrow\infty}\E\left(\int_{t\wedge S_n}^{t+h\wedge S_n}\lambda_sds\bigg|\mc{F}^Z_t\right)\\
    &\Rightarrow\quad\E\left(Z_{t+h}-Z_t|\mc{F}^Z_t\right)=\E\left(\int_t^{t+h}\lambda_sds\bigg|\mc{F}^Z_t\right)
\end{align*}
from which the more intuitive (\ref{eq13}) may be recovered
\begin{align*}
    \lim_{h\rightarrow 0^+}\frac{1}{h}\E\left(Z_{t+h}-Z_t|\mc{F}_t^Z\right)=\E\left(\lim_{h\rightarrow 0^+}\frac{1}{h}\int_t^{t+h}\lambda_sds\bigg|\mc{F}^Z_t\right)=\lambda_t
\end{align*}
which is justified by the Lebesgue differentiation theorem to compute
$\lim_{h\searrow 0}\int_t^{t+h}\lambda_sds=\lambda_t$, preceded by
dominated convergence since $\lambda_t$ is locally-integrable (note that
$Z_{t+h}-Z_t\in\{0,1\}$ as $h\rightarrow 0$ since $Z$ is regular). In a modeling
setting, it is the intensity $\lambda_t$ which is designed to recursively
specify how spikes influence the future spiking probability. In this way,
$\lambda_t$ may be interpreted as a dynamical law governing a probabilistic
spiking mechanism. However, treating $\lambda_t$ as a modeling tool raises
several questions of a technical nature. Most importantly is that of existence:
for a given $\lambda_t$, does an associated point process $Z_t$ always exist?
For the intensity of a linear Hawkes process, where
$\lambda_t:=v+\int_0^th(t-s)Z(ds)$ with base excitation $v\geq0$ and
self-excitation kernel $h\geq 0$, \cite{Hawkes_Oakes_1974} answered this question
in the affirmative by construction of an appropriate cluster process. However, a
more general and direct approach was described in \cite{Ogata_1981} based upon the
notion of point process thinning.\\[5pt]
\indent One way to proceed is by a method of \textit{spatial thinning} \cite[theorem B.11]{Chevallier_Caceres_Doumic_Reynaud_Bouret_2015} as follows. Suppose $\Pi$ is a $\mbb{R}_+^2$-Poisson process with rate $1$
adapted to some filtration $\mc{F}_t$. Definitionally, this means that i)
$P(\Pi(A)=m)=e^{-\mu(A)}\tfrac{\mu(A)^m}{m!}$ $\forall
m\in\mbb{Z}_+,\;A\in\mc{B}(\mbb{R}^2_+)$, where $\mu$ is the Lebesgue measure on
$\mc{B}(\mbb{R}_+^2)$ and ii) $\Pi(A_1), \Pi(A_2),\dots,\Pi(A_k)$ are mutually
independent whenever $A_1,A_2,\dots,A_k\in\mc{B}(\mbb{R}_+^2)$ with $k\in\mbb{N}$ are pairwise
disjoint. Spatial thinning says that if a process $\lambda_t$ is
nonnegative, $\mc{F}_t$-adapted and bounded by an integrable process, then the
measure $Z$ defined
\begin{align*}
    Z(A):=\int_{A}\int_{\mbb{R}_+}\1{y\in[0,\lambda_s]}\Pi(dy\times ds)
\end{align*}
for $A\in\mc{B}(\mbb{R}_+)$ is a point process admitting intensity $\lambda_t$.
This at once solves the immediate technical and operational needs, guaranteeing
existence and allowing one to freely tailor intensities to applications. As an
added benefit, it also furnishes simple procedures for realizing point processes
numerically \cite[algorithm 2]{Ogata_1981}, \cite{Mascart_Muzy_Reynaud-bouret_2021}.
Later, this result will be instrumental in defining a class of spiking network models which
encompass both Hawkes and generalized integrate-and-fire methods. As such, the
supporting theory and formal result is sufficiently germane to discuss in some
detail -- beginning with a generalization of definition (\ref{def17}) and point
processes themselves \cite[ch.8]{Bremaud_1981}.
\begin{definition}[Predictable $\sigma$-Algebras]\label{def18} For a filtered
    probability space $(\Omega,\mc{F},(\mc{F})_{t\geq 0},P)$, the $\sigma$-field
\begin{align}
    \mc{P}(\mc{F}_t)=\sigma((s,t]\times A:0\leq s\leq t,\;A\in\mc{F}_s)
\end{align}
is called the {\it predictable $\sigma$-algebra} of $\mc{F}_t$ on
$(0,\infty)\times\Omega$. If a process map $(t,\omega)\mapsto C_t(\omega)$ is
$\mc{P}(\mc{F}_t)$ measurable then it is called $\mc{F}_t$-predictable.
\end{definition}
\begin{definition}[Marked Point Processes]\label{def19} Suppose that to a point
    process $(T_n)_{n\geq 0}$ is associated a sequence of $Y$-valued random
    variables $(Y_n)_{n\geq 0}$ with $(Y,\mc{Y})$ a measurable space. Then
    $\{(T_n,Y_n)\}_{n\geq 0}$ is called a {\it marked point process} with {\it
    marks} $(Y_n)_{n\geq 0}$ and {\it mark space} $Y$.
\end{definition}
Marked point processes (MPPs), just like point processes, can be associated with
a counting process and measure; for $A\in\mc{Y}$,
\begin{align}
    Z_t(A)=\sum_{n\geq 1}\1{T_n\leq t}\1{Z_n\in A},\quad\text{and}\quad Z([0,t]\times A)=Z_t(A)
\end{align}
where $t\geq0$ and, by way of initialization, $Y_0\equiv\Delta\notin Y$. Marked
point processes are a generalization of (\ref{def8}), since for arbitrary mark
space $Y$ and another $Y^\prime=\{1,\dots,N\}$, with $Z_t(Y)$ and
$(Z_t(i))_{i\in Y^\prime}$ an unmarked univariate and multivariate process
(respectively) is recovered. Note that local-finiteness requires $Z([0,t]\times Y)<\infty$ for all $t\geq 0$ $\as$ which,
for instance, means that the $\mbb{R}^2_+$-standard Poisson process is not a MPP (since $P(\Pi([0,t]\times\mbb{R}_+)\leq m)=0$ $\as$ for $m\geq 1$). For MPPs, definition (\ref{def18}) is further
extended to marked predictable $\sigma$-algebras on
$(0,\infty)\times\Omega\times Y$ by defining
$\widetilde{\mc{P}}(\mc{F}_t):=\mc{P}(\mc{F}_t)\otimes\mc{Y}$. Correspondingly,
a process map $(t,\omega,y)\mapsto C(t,\omega,y)$ measurable with respect to
$\widetilde{\mc{P}}(\mc{F}_t)$ is called $\mc{F}_t$-predictable and indexed by
$Y$. Integration of such a process with respect to a MPP $Z$ is defined
\begin{align}
    \int_0^t\int_EC(s,\omega,y)Z(ds\times dy):=\sum_{n\geq 1}C(T_n,\omega, Z_n)\1{T_n\leq t}
\end{align}
where implicitly $Z$ is the MPP obtained from the sample $\omega\in\Omega$. The
natural filtration of a MPP is given by $\mc{F}^Z_t=\sigma(Z_s(A):0\leq s\leq
t,\;A\in\mc{Y})$, and the role played by the stochastic intensity for unmarked
processes is subsumed by an intensity kernel on the mark space.
\begin{definition}[Intensity Kernels]\label{def20} Let $Z$ be a MPP with mark
    space $(Y,\mc{Y})$ adapted to a history $\mc{F}_t\supseteq\mc{F}_t^Z$.
    Suppose that $\forall A\in \mc{Y}$, the point process $Z_t(\omega, A)$
    admits a $\mc{F}_t$-predictable stochastic intensity $\lambda_t(\omega, A)$,
    where $\lambda\in\mc{P}(Y|\mbb{R}_+\times\Omega)$. Then it is said that the
    MPP $Z$ admits the $\mc{F}_t$-intensity kernel $\lambda_t(dy)$.
\end{definition}
Intensity kernels define a measure on $(Y,\mc{Y})$ -- the marginal distribution of marks which
depends on the process history predictably via $\mc{F}_t$.
The next result, attributable to \cite{Jacod_1975} and called the projection theorem
in \cite[ch.8, theorem 3]{Bremaud_1981}, establishes the sense in which intensity kernels generalize
intensity processes.
\begin{proposition}[Br\'emaud (1981)]\label{prop1} Let $Z$ be a
    MPP with mark space $(Y,\mc{Y})$ admitting the $\mc{F}_t$-intensity kernel
    $\lambda_t(dy)$. Then for any $\mc{F}_t$-predictable, $Y$-indexed and real
    nonnegative process $C$ it holds that
    \begin{align}
        \E\left(\int_0^\infty\int_YC(s,y)Z(ds\times dy)\right)=\E\left(\int_0^\infty\int_YC(s,y)\lambda_s(dy)ds\right).\label{eq26}
    \end{align}
\end{proposition}
\noindent {\bf Proof}. One can show that the class $\mc{H}$ of real functions
for which (\ref{eq26}) holds satisfies the hypotheses of the monotone class
theorem with respect to an appropriate $\pi$-system. First, observe that
$\forall A\in\mc{Y}$ and $\mc{F}_t$-predictable $\widetilde{C}_t$,
$\widetilde{C}_t\1{y\in A}\in\mc{H}$ since
\begin{align*}
    \E\left(\int_0^\infty\int_Y\widetilde{C}_s\1{y\in A}Z(ds\times dy)\right)&=\E\left(\int_0^\infty \widetilde{C}_sZ(ds\times A)\right)\\
    &=\E\left(\int_0^\infty\widetilde{C}_s\lambda_s(A)ds\right)\\
    &=\E\left(\int_0^\infty\int_Y\widetilde{C}_s\1{y\in A}\lambda_s(dy)ds\right)
\end{align*}
where the penultimate equality holds since $\lambda_s(A)$ is the intensity of
$Z_s(A)$. Of course, for $0\leq t$ and $S\in\mc{F}_t$, $(t,\infty)\times
S\in\mc{P}(\mc{F}_t)$ so the above computation further implies that $\mc{H}$
contains indicators of sets in the $\pi$-system
\[\mc{K}=\{(t,\infty)\times S\times A:0\leq t,\;S\in\mc{F}_t,\;A\in\mc{Y}\}\]
generating $\widetilde{\mc{P}}(\mc{F}_t)$. Clearly $(0,\infty)\times\Omega\times
Y\in\mc{K}$, and $\mc{H}$ is closed under addition and scalar multiplication by
the linearity of integration. Finally, if $\{f_n\}_{n\geq 1}\subseteq\mc{H}$ is
a sequence of nonnegative functions with $\lim_{n\rightarrow\infty}f_n\nearrow
f$ pointwise and $f$ bounded, then successive application of the monotone
convergence theorem yields $f\in\mc{H}$. Invoking the monotone class theorem,
$\mc{H}$ contains all bounded and measurable functions with respect to
$\widetilde{\mc{P}}(\mc{F}_t)$.\hfill{$\qed$}\\[5pt]
\indent The definition of $\pi$-systems and the monotone class theorem shall be
left implicit in the proof of proposition \ref{prop1}. Thus, the defining
property of stochastic intensities (\ref{def9}) is recovered when
$C(s,\omega,y)=C(s,\omega)\1{y\in Y}$. In order to finally divulge a proof of
the spatial thinning theorem, two more results are needed which, in part, establish a
characterization of stochastic intensities in terms of local martingales.
\begin{proposition}[MPP Integration Theorem; Br\'emaud (1981)]\label{prop2} Let
    $Z$ be the same MPP as in theorem \ref{prop1} and $C$ a
    $\mc{F}_t$-predictable, $Y$-marked process satisfying
    $\int_0^t\int_Y|C(s,y)|\lambda_s(dy)ds<\infty$ $\forall t\geq 0$ almost
    surely. Then the process $X_t$ defined by
    \begin{align*}
        X_t:=\int_0^t\int_Y C(s,y)\left[Z(ds\times dy)-\lambda_s(dy)ds\right],\quad t\geq 0
    \end{align*}
    is a $\mc{F}_t$-local martingale. If instead $\E\left(\int_0^t\int_Y|C(s,y)|\lambda_s(dy)ds\right)<\infty$, then $X_t$ is a
    martingale.
\end{proposition}
\noindent{\bf Proof}. I will address the local martingale result. Fix some constants $0\leq a\leq b$ and define a sequence
of $\widetilde{\mc{P}}(\mc{F}_t)$-measurable processes
$C^\prime_n(t,\omega,y)=C(t,\omega,y)\1{a<t\leq b}\1{t\leq T_n(\omega)}$, where
$(T_n)_{n\geq 1}$ are the points of $Z$, and hence a sequence of
$\mc{F}_t$-stopping times both increasing and with
$\lim_{n\rightarrow\infty}T_n=+\infty$ almost surely. Then $C^\prime_n$
satisfies the hypotheses $\forall n$, and by theorem \ref{prop1}:
\begin{align*}
    &\E\left(\int_0^\infty\int_YC_n^\prime(s,y)Z(ds\times dy)\bigg|\mc{F}_a\right)=\E\left(\int_0^\infty\int_YC_n^\prime(s,y)\lambda_s(dy)ds\bigg|\mc{F}_a\right)\\
    \Rightarrow\quad&\E\left(\int_{a\wedge T_n}^{b\wedge T_n}\int_YC(s,y)Z(ds\times dy)\bigg|\mc{F}_a\right)=\E\left(\int_{a\wedge T_n}^{b\wedge T_n}\int_Y C(s,y)\lambda_s(dy)ds\bigg|\mc{F}_a\right)\\
    \Rightarrow\quad&\E\left(\int_0^{b\wedge T_n}\int_Y C(s,y)\left[Z(ds\times dy)-\lambda_s(dy)ds\right]\bigg|\mc{F}_a\right)=\int_0^{a\wedge T_n}\int_Y C(s,y)\left[Z(ds\times dy)-\lambda_s(dy)\right].
\end{align*}
The simple operations bridging the second and third lines are permitted since
$Z$ is locally finite and $C$ is locally integrable with respect to
$\lambda_s(dy)ds$. The last line says that $\E(X_{b\wedge
T_n}|\mc{F}_a)=X_{a\wedge T_n}$, which with the aforementioned properties of the
$T_n$ yields the result.\hfill{$\qed$}\\[5pt]
\indent An immediate consequence of proposition \ref{prop2}, obtained when
$C(s,y)=\1{y\in A}\1{s\in[0,t]}$, is that $Z_t(A)-\int_0^t\lambda_s(A)ds$ is a
$\mc{F}_t$-local martingale $\forall A\in\mc{Y}$, and in particular this holds
for all unmarked point processes with locally-integrable $\mc{F}_t$-intensities
(which was claimed earlier in lemma (\ref{lem1})). It turns out that the
converse is also true, so this local martingale property actually characterizes
the stochastic intensity corresponding to $Z$. This is the main fact used in
\cite{Chevallier_Caceres_Doumic_Reynaud_Bouret_2015} to prove the spatial thinning theorem, which is recapitulated here
after demonstrating this converse result.
\begin{proposition}[Br\'emaud (1981)]\label{prop3} Suppose $Z$ is a
   locally-finite point process, $\lambda_t$ is a nonnegative process, and both
   are adapted to $\mc{F}_t$. Then, if $Z_t-\int_0^t\lambda_sds$ is a $\mc{F}_t$
   local martingale, then $\lambda_t$ is the intensity of $Z$.
\end{proposition}
\noindent{\bf Proof}. Let $\{S_n\}_{n\geq 1}$ be a sequence of localizing
$\mc{F}_t$-stopping times qualifying $Z_t-\int_0^t\lambda_s ds$ as a local
martingale. The strategy used here is the same as for proposition \ref{prop1};
the class $\mc{H}$ of processes $C_t$ satisfying
\[\E\left(\int_0^{t\wedge S_n}C_sZ(ds)\right)=\E\left(\int_0^{t\wedge
S_n}C_s\lambda_s ds\right)\] for any $n\geq 1$ is shown to contain all bounded,
$\mc{F}_t$ predictable processes using the monotone class theorem. For any
$0\leq s\leq t$ and $n\geq 1$, by the martingale property of the stopped
processes
\begin{align*}
    &\E\left(Z_{t\wedge S_n}-\int_0^{t\wedge S_n}\lambda_sds\bigg|\mc{F}_s\right)=Z_{s\wedge S_n}-\int_0^{s\wedge S_n}\lambda_udu\\
    \Rightarrow\qquad&\E\left(\int_{s\wedge S_n}^{t\wedge S_n}Z(du)\bigg|\mc{F}_s\right)=\E\left(\int_{s\wedge S_n}^{t\wedge S_n}\lambda_u du\bigg|\mc{F}_s\right)\\
    \Rightarrow\qquad&\E\left(\int_0^{t\wedge S_n}\1{A}\1{u\in(s,\infty)}Z(du)\right)=\E\left(\int_0^{t\wedge S_n}\1{A}\1{u\in(s,\infty)}\lambda_udu\right)
\end{align*} 
for any $A\in\mc{F}_s$. So $\mc{H}$ contains indicators of the $\pi$-system 
\[\mc{K}=\{(s,\infty)\times A\subseteq(0,\infty)\times\Omega:0\leq s\leq
t,\;A\in\mc{F}_s\}\] which generates $\mc{P}(\mc{F}_t)$. By precisely the same
reasoning as in the proof of proposition \ref{prop1}, the monotone class theorem
guarantees $\mc{H}$ contains all bounded and $\mc{F}_t$-predictable processes.
Then, taking any nonnegative, $\mc{F}_t$-predictable process $C_t$, one obtains 
\begin{align*}
    \E\left(\int_0^{t\wedge S_n}C_s Z(ds)\right)=\E\left(\int_0^{t\wedge S_n}C_s\lambda_sds\right)\quad\Rightarrow\quad\E\left(\int_0^\infty C_sZ(ds)\right)=\E\left(\int_0^\infty C_s\lambda_sds\right)
\end{align*}
by taking $n,t\rightarrow+\infty$ successively and invoking the monotone
convergence theorem each time.\hfill{$\qed$}
\begin{theorem}[Spatial Thinning; Chevallier et al.
    (2015) theorem B.11]\label{thm1} Let $\Pi$ be a $\mc{F}_t$-adapted standard Poisson
    process on $\mbb{R}_+^2$. If $\lambda_t$ is nonnegative and
    $\mc{F}_t$-predictable with $\int_0^t\lambda_s(\omega)ds<K_t(\omega)$ and $\E(K_t)<\infty$ for all $t\geq
    0$, then the
    point process $Z$ defined by
    \begin{align*}
        Z(A)=\int_A\int_{\mbb{R}_+}\1{y\in[0,\lambda_t]}\Pi(dy\times dt),\quad A\in\mc{B}(\mbb{R}_+)
    \end{align*}
    admits $\lambda_t$ as a $\mc{F}_t$-predictable stochastic intensity process.
\end{theorem}
\noindent{\bf Proof}. For each $k\in\mbb{N}$, define $\Pi^k$ the restriction of
$\Pi$ to $\mbb{R}_+\times[0,k]$ so that $\Pi^k(A\times dt)=\int_A\1{y\in[0,k]}\Pi(dy\times
dt)$ $\forall A\in\mc{B}(\mbb{R}_+)$. Then each $\Pi^k$ can be viewed as a
$\mc{F}_t$-adapted MPP with mark space $(Y_k:=[0,k],\mc{Y}_k:=\mc{B}([0,k]))$
and intensity kernel $dy$. Corresponding to these MPPs, define a sequence of
point processes $\{Z^k\}_{k\geq 1}$ with
\begin{align*}
    Z^k(A)=\int_A\int_{\mbb{R}_+}\1{y\in[0,\lambda_t]}\Pi^k(dy\times dt),\quad A\in\mc{B}(\mbb{R}_+).
\end{align*}
At the same time, for any mark $y\in \mbb{R}_+$,
$\{(s,\omega)\in\mbb{R}_+\times\Omega:\lambda_s(\omega)\geq
y\}\in\mc{P}(\mc{F}_t)$ (since $\lambda_t$ is $\mc{F}_t$-predictable) such that
the set
\begin{align*}
    \mc{D}_k&:=\bigcap_{n\in\mbb{N}}\bigcup_{q\in\mbb{Q}_+}\{(s,\omega):\lambda_s(\omega)\geq q\}\times\left([0,q+\tfrac{1}{n}]\cap Y_k\right)
\end{align*}
is contained in
$\mc{P}(\mc{F}_t)\otimes\mc{Y}_k=:\widetilde{\mc{P}}_k(\mc{F}_t)$. For a more
workable form, it is also the case that
\[\mc{D}_k=\{(s,\omega,y)\in\mbb{R}_+\times\Omega\times
Y_k:\lambda_s(\omega)\geq y\}.\] To see this, take a point $(s,\omega,y)$ in the
latter of these, and a sequence $\{q_n\}_{n\geq 1}\subset\mbb{Q}_+$ with the
property $|q_n-y|\leq\tfrac{1}{n}$ and $q_n\nearrow y$ as $n\rightarrow+\infty$.
Then $\forall n\geq 1$, $\lambda_s(\omega)\geq y\geq q_n$ and
$y\in[0,q_n+\tfrac{1}{n}]$. Conversely, if $(s,\omega,y)\in\mc{D}_k$ under the
first definition, then to each $n\geq 1$, $\exists q_n\in\mbb{Q}_+$ with
$\lambda_s(\omega)\geq q_n$ and $y\in[0,q_n+\tfrac{1}{n}]\cap Y_k$. In the worst
case where $y>q_n$ for every $n$, the sequence $q_n\rightarrow y$ as
$n\rightarrow+\infty$ so still $\lambda_s(\omega)\geq y$.\\[5pt]
With $\mc{D}_k\in\widetilde{\mc{P}}_k(\mc{F}_t)$, it is clear that
$\1{y\in[0,\lambda_t(\omega)]}(t,\omega,y)$ is
$\widetilde{\mc{P}}_k(\mc{F}_t)$-measurable for any $k$, but also
$\int_0^t\int_{Y_k}\1{y\in[0,\lambda_s]}dyds<K_t$ almost surely.
Proposition \ref{prop2} thus furnishes a sequence of processes $X^k_t$ with
\begin{align*}
    X^k_t:=\int_0^t\int_{Y_k}\1{y\in[0,\lambda_s]}\left[\Pi^k(dy\times ds)-dyds\right]&=Z_t^k-\int_0^t\int_{Y_k}\1{y\in[0,\lambda_s]}dyds\\
    &=Z^k_t-\int_0^t\min(\lambda_s,k)ds
\end{align*}
a $\mc{F}_t$-local martingale. That is, there exists a localizing sequence
$\{S_n\}_{n\geq 1}$ of $\mc{F}_t$-stopping times so that $X^k_{t\wedge S_n}$ is
a $\mc{F}_t$-martingale $\forall n\geq 1$. Sending $k\rightarrow+\infty$,
\begin{align*}
    \lim_{k\rightarrow\infty}X^k_{t\wedge S_n}&=\lim_{k\rightarrow\infty}\left(\int^{t\wedge S_n}_0\int_{\mbb{R}_+}\1{y\in[0,k]}\1{y\in[0,\lambda_s]}\Pi(dy\times ds)-\int_0^{t\wedge S_n}\min(\lambda_s,k)ds\right)\\
    &=\int_0^{t\wedge S_n}\int_{\mbb{R}_+}\1{y\in[0,\lambda_s]}\Pi(dy\times ds)-\int_0^{t\wedge S_n}\lambda_sds
\end{align*}
which is just $Z_{t\wedge S_n}-\int_0^{t\wedge S_n}\lambda_sds$, which by
analogy shall be called $X_{t\wedge S_n}$. To be explicit, the first term is
obtained as the limiting measure of a sequence of increasing sets, and the
second with the monotone convergence theorem. For any $n\geq 1$, the stopped
process $X_{t\wedge S_n}$ also satisfies the martingale property
\[\lim_{k\rightarrow\infty}\E\left(X^k_{t\wedge
S_n}|\mc{F}_s\right)=\lim_{k\rightarrow\infty}X^k_{s\wedge
S_n}\quad\Rightarrow\quad\E\left(X_{t\wedge S_n}|\mc{F}_s\right)=X_{s\wedge
S_n}\] using the monotone convergence theorem to justify the limit-integral
interchange. Further, take any $t\geq 0$ and define
$A_\lambda(t,\omega):=\{(s,y):s\in[0,t],\;y\leq\lambda_s(\omega)\}$. Then
$\int\1{A_\lambda(t,\omega)}dsdy<K_t$ $\as$, so
\begin{align*}
    \E\left(|X_{t\wedge S_n}|\right)&\leq \E\left(\int_0^{t\wedge S_n}\int_{\mbb{R}_+}\1{y\in[0,\lambda_s]}\Pi(dy\times ds)\right)+\E\left(\int_0^{t\wedge S_n}\lambda_sds\right)\\
    &\leq\E\left(\Pi(A_\lambda(t,\omega))\right)+\E(K_t)
\end{align*}
and the expected value
$\E(\Pi(A_\lambda(t,\omega)))\leq\E(\Pi([0,t]\times[0,\tfrac{K_t}{t}]))=K_t<\infty$
$P$-$\as$, since $\Pi$ is standard Poisson and thus spatially homogenous. But
now $X_{t\wedge S_n}$ is a $\mc{F}_t$-martingale for all $n\geq 1$, so $X_t$ is
a local martingale. By proposition \ref{prop3}, $\lambda_t$ must be the
intensity of $Z$.\hfill{$\qed$}
\begin{remark}\label{rem2}
    In \cite[theorem B.11]{Chevallier_Caceres_Doumic_Reynaud_Bouret_2015}, which
    the proof of theorem \ref{thm1} closely follows, the authors only assume
    $\int_0^t\lambda_sds<\infty$ $P$-almost surely, rather than
    $\int_0^t\lambda_s(\omega)<K_t(\omega)$ for some $K_t$ locally integrable.
    However, the former is insufficient to certify the local integrability of
    $X_t$, which is necessary to establish $X_t$ as a local martingale.
\end{remark}
\indent The theoretical treatment thus far is sufficient to motivate and
contextualize various neuroscientific use-cases of point processes. For physiological modeling
(as opposed to data-driven approaches \cite{Pillow_Paninski_Uzzell_Simoncelli_Chichilnisky_2005}, \cite{Pillow_Shlens_Paninski_Sher_Litke_Chichilnisky_Simoncelli_2008})
this consists largely of a class of multivariate, coupled processes called nonlinear Hawkes (NH) networks.
This coupling is often referred to as 'self-excitation' in the mathematical
literature, but is not to be confused with the neuroscience parlance, as these
models easily admit inhibitory interactions as well. As for CB networks, let
$\mc{N}$ index a set of $N\in\mbb{N}$ units, and $\mc{G}=(\mc{N},\mc{E})$ some network topology.
To define a stochastic intensity at each
$i\in\mc{N}$, fix functions $\phi^i:\mbb{R}\rightarrow\mbb{R}_+$ and
$h^i_j:\mbb{R}_+\rightarrow\mbb{R}$, where $j$ ranges over $\mc{N}$. In the
interest of notational expediency, denote $\phi:=(\phi^i)_{i\in\mc{N}}$ and
$\mc{H}:=(h^i_j)_{i,j\in\mc{N}}$.
\begin{definition}[Nonlinear Hawkes Network; \cite{Bremaud_Massoulié_1996}, \cite{Chevallier_Caceres_Doumic_Reynaud_Bouret_2015}]\label{def10} With $\mc{N}$,
    $\mc{G}$, and a weight matrix $w=(w^i_j)_{i,j\in\mc{N}}\in\mbb{W}$, a
    $\mbb{R}_+^N$-point process $Z=(Z^i)_{i\in\mc{N}}$ is called a {\it
    nonlinear Hawkes network} if $\exists\phi,\mc{H}$ (as above) so that each
    $Z^i$ admits a stochastic intensity $\lambda^i_t$ of the form
    \begin{align*}
        \lambda_t^i:=\phi^i\left(\sum_{j\in\mc{G}(i)}\int_{0}^{t^-}w^i_jh^i_j(t-s)Z^j(ds)\right)
    \end{align*}
    where $\phi^i$ is called an {\it excitation function} and the $h^i_j$ are
    called $(i,j)$-{\it synaptic kernels}. Note that each of the $\lambda^i_t$
    are $\mc{F}^Z_t:=\bigvee_{i\in\mc{N}}\mc{F}^{Z^i}_t$-predictable.
\end{definition}
\begin{remark}
    Distinct from definition (\ref{def9}), the integral here is defined
    $\int_{0}^{t^-}w^i_jh^i_j(t-s)Z^j(ds):=\sum_{k\geq
    1}w^i_jh^i_j(t-T_k)\1{T_k<t}$ so as to ensure the predictability of the
    $\lambda^i_t$. This notation is unconventional, and shall be implicit when
    the resulting process is to be predictable.
\end{remark}
Throughout this report, the $\phi^i$ are assumed to be Lipschitz, nondecreasing
and bounded so as to include common classes of neuronal excitation profiles
\cite{Hodgkin_1948}, \cite{Izhikevich_2007} and bestow $\int_0^t\lambda^i_s$
with its requisite integrability for $t\geq 0$. Typically these are also
selected to satisfy $\phi^i>0$ to prevent the extinction of spiking activity.
Without loss of expressivity, the elements of $\mc{H}$ are also assumed
Lipschitz and integrable. Common choices resembling the spike-response
characteristics of general dynamic synapses (equation (\ref{eq27})) are the
alpha and double exponential profile \cite{Ermentrout_Terman_2010}. For each
$j\in\mc{N}$, fix time constants $\tau^j_r,\tau^j_d>0$ with $\tau^j_r\neq
\tau^j_d$ and synaptic polarities $q^j\in\{-1,1\}$. Then a phenomenological
synaptic kernel could be given explicitly by
\begin{align}
    h^i_j(t)=q^js^j(t),\quad s^j(t)=\begin{cases}
        \frac{\tau^j_r\tau^j_d}{\tau^j_r-\tau^j_d}(e^{-\tau^j_dt}-e^{-\tau^j_rt}),\quad&\text{(double exponential profile)}\\
        (\tau^j_d)^2te^{-\tau^j_dt},\quad&\text{(alpha profile)}.
    \end{cases}\label{eq14}
\end{align}
Polarities determine whether the synapse $j\rightarrow i$ is excitatory or
inhibitory. For the double exponential profile, $\tau^j_r$ characterizes the
rise time of the current, and $\tau^j_d$ the decay, while for the alpha profile
$\tau^j_d$ characterizes both. The time course of either synapse can be obtained
as the solution of a system of impulse-forced differential equations:
\begin{align}
    \frac{ds^j(t)}{dt}=-\frac{s^j(t)}{\tau^j_d}+\hat{s}^j(t),\quad\text{where}\quad
    \frac{d\tilde{s}^j(t)}{dt}=-\frac{\tilde{s}^j(t)}{\tau^j_r}+Z^j(dt).\label{eq28}
\end{align}
The alpha synapse is then the result of selecting $\tau^j_r=\tau^j_d$. Using the
dynamical representation (\ref{eq28}), an equivalent formulation for intensity
is $\lambda^i_t=\phi^i(\sum_{j\in\mc{G}(i)}w^i_jq^js^j(t))$. Justifying a
similar form for the recurrent kernels $h^i_i$ from a modeling standpoint is not
as straightforward. They can be used to purposefully mimic slower dynamical
phenomena elicited by local spiking (such as spike frequency adaptation
\cite{Kistler_Gerstner_Hemmen_1997}). One possibility for this could be
\begin{align}
    h^i_i(t)=-w^i_i(\beta^i_d)^2te^{-\beta^it}\label{eq15}
\end{align}
an alpha profile with weight $w^i_i\geq 0$, and a small characteristic time
$0<\beta^i<\min_{j\in\mc{N}}\{\min(a^j,b^j)\}$ to model an accumulating
inhibition with repetitive spiking. Of course, for mathematical purposes a
particular choice of $h^i_i$ is of no consequence. \\[5pt]
\indent Missing from the classical NH network is a period of absolute
refractoriness where spikes from a single unit are separated by a minimum
duration $\delta>0$. To introduce such an effect to definition (\ref{def10}),
define the $\mc{F}^{Z^i}_t$-predictable age processes 
\begin{align}
    S^i_t:=t-\sup\{T\in Z^i: T<t\},\quad i\in\mc{N}
\end{align}
specifying the time elapsed since the last spike, not including the present.
With this, the NH network intensities may be refined into what has been
called an age-dependent or generalized NH process, studied in \cite{Chevallier_2017}, \cite{Chevallier_Caceres_Doumic_Reynaud_Bouret_2015}. Letting
$\Phi^i:\mbb{R}_+\times\mbb{R}\rightarrow\mbb{R}_+$, define for $i\in\mc{N}$ the
modified intensity \cite{Chevallier_2017}:
\begin{align}
    \lambda^i_t:=\Phi^i\left(S^i_t,\;\;\sum_{j\in\mc{G}(i)}\int_0^{t^-}h^i_j(t-s)Z^j(ds)\right).\label{eq16}
\end{align} 
For the use-case of (\ref{eq16}) in mind, let $\psi^i:\mbb{R}_+\rightarrow
[0,1]$ be nondecreasing, $\lim_{t\rightarrow T}\psi^i(t)=1$ for some $T<\infty$,
and set $\Phi^i(t,x):=\phi^i(x)\psi^i(t)$; one can take
$\psi^i(t):=\1{t\geq\delta}$ for a particularly simple choice
($\delta\approx$1ms in real neurons, for example). The age-dependent intensity
(\ref{eq16}) is a strict generalization since, setting $\psi^i\equiv 1$, the non
age-dependent case (\ref{def10}) is recovered. This age-dependent model was used
in \cite{Borovkov_Decrouez_Gilson_2014} to study the stability of a spike
configuration process, and a mean-field limit for $N\rightarrow\infty$ was
obtained by \cite{Chevallier_2017}.\\[5pt]
\indent Summarizing, a NH network (in either the sense of (\ref{def10}) or
(\ref{eq16})) is via a stochastic intensity, a probabilistic description of an
idealized spiking process. The intensity is specified by a triple
($\Phi$,$\mc{H}$,$\mc{G}$), where $(\Phi$, $\mc{H}$) neatly collapse the role of
intrinsic and extrinsic conductances (respectively) into a probabilistic
description. The next (and final) type of uncontrolled model attempts to bridge
the mechanistic (CB) and probabilistic (NH) approaches, giving rise to a rich
class of biologically interesting and mathematically tractable models.
\section{Stochastic Realization of Integrate and Fire Networks}\label{sec6}
\indent Point process models defined purely in terms of an intensity process are probabilistic
in the sense that solutions are defined in only a weak sense, as opposed to strong solutions
which are progressively measurable in some driving noise process. This does not preclude the possibility
that a functional model (a stochastic differential equation) could not give rise to the same probabilistic flow,
and in fact such models can be interpreted as a kind of \textit{integrate-and-fire} (IF) type neuron model \cite[part II]{Gerstner_Kistler_Naud_Paninski_2014}.
Much like CB models, IF models strive for a mechanistic description of neuronal membrane dynamics.
However, simple variants actually predate CB models by nearly half a century
\cite{Brunel_van_Rossum_2007} inspired by the apparent threshold-character of
neural activity, which is treated as a fixed boundary in the voltage space for
each unit. Beneath this value, potentials evolve continuously under some
mechanistic law, called the subthreshold (or "integrate") regime. Above it, the
voltage is discontinuously reset to a subthreshold value -- whereupon it is held
for a fixed (absolutely refractory) duration $\delta>0$ before resuming a
continuous evolution. These periodic interruptions mark the moment of each spike
precisely, and can be used to formulate synaptic interactions via kernel
functions in precisely the same manner as for NH networks. In this way, IF
networks are regarded as a middleground between biophysical CB networks (via the
subthreshold dynamics) and the point-based evolution of NH models. Despite
admitting points, IF models can only be described with an intensity process in
a heuristic sense
\cite{Kass_Amari_Arai_Brown_Diekman_Diesmann_Doiron_Eden_Fairhall_Fiddyment_et_al._2018}
or as a renewal process under extremely harsh assumptions on the admissable
dynamics \cite{Gerwinn_Macke_Bethge_2009},
\cite{Buonocore_Caputo_D’Onofrio_Pirozzi_2015} rendering the general case (and in particular, large, heterogeneous networks)
largely inaccessible to the theory of point processes. One method for resolving this is to replace 
hard thresholds with a probabilistic
description using spatial thinning (theorem \ref{thm1}) \cite[eq. 7]{Jahn_Berg_Hounsgaard_Ditlevsen_2011},
\cite[eq. 1.1]{Jabin_Zhou_2023} which can be interpreted as so-called escape noise \cite{Plesser_Gerstner_2000}
where the threshold is relaxed to approximate the effect of diffusive membrane noise.
In the sequel, this new class is shown to be a superset of NH networks, and is simultaneously able to
approximate any hard threshold model in the classical paradigm with an
appropriate choice for the intensity. This new description defines
a jump-diffusion process with state dependent intensities, which shall be called a soft
IF (SIF) network. This model is thus a compelling platform for bridging neuroscience and
stochastic control; incorporating mechanistic and probabilistic features to
model the hybrid nature of spiking in a mathematically structured framework.
Leading up to this, a general formulation of stochastic IF networks in
continuous time are presented following the exposition in
\cite{Cessac_Viéville_2008}, \cite{Cessac_2011} and
\cite[ch.5]{Gerstner_Kistler_Naud_Paninski_2014}. These and NH models are then
compared with the SIF formalism.\\[5pt]
\indent Let $\mc{N}$ index a collection of $N\in\mbb{N}$ units and fix a network
topology $\mc{G}=(\mc{N},\mc{E})$ on $\mc{N}$. For $i\in\mc{N}$, let $v^i_t$ be
the voltage trajectory of unit $i$ over $\mbb{R}_+$ giving rise to spike times
$T^i=\{T^i_m\}_{m=0}^{M^i}$, where $M^i\in\mbb{N}\cup\{+\infty\}$ is the random
total number of spikes. These $M^i$ are at most countable, since the absolute
refractory duration $\delta>0$ will guarantee a positive minimal spike
separation $\inf_{m\geq 0}\{T^i_{m+1}-T^i_m\}\geq\delta>0$. Fixing a threshold
$\vartheta^i\in\mbb{R}$, spikes are recursively defined for $m\geq 0$
\begin{align}
    T^i_{m+1}:=\inf\{t\geq T^i_m+\delta: v^i_t\geq\vartheta^i\}\label{eq31}
\end{align}
and $T^i_0:=0$, $v^i_0<\vartheta^i$ by way of initialization. Now define the
reset potential of $i$ to be a value $r^i<\vartheta^i$ so that for $k\geq 0$,
$v^i_{T^i_k+\delta}=r^i$, and let $B_t:=(B^i_t)_{i\in\mc{N}}$ a $N$-dimensional
standard Brownian motion adapted to the filtration $\mc{F}_t$. Network
interactions are expressed using a collection of point measures
$Z=(Z^i)_{i\in\mc{N}}$ where
\begin{align}
    Z^i(A)=\sum_{m=1}^{M^i}\delta_{T^i_m}(A)\label{eq20}
\end{align}
and $\delta_x$ is the Dirac measure concentrated at a point $x\in\mbb{R}_+$. In
particular, by (\ref{eq31}), $\mc{F}^Z_t\subseteq\mc{F}_t$. Then the interspike
excursion over $T^i_k+\delta\leq t < T^i_{k+1}$ of unit $i$ evolves
(stochastically) as an It\^o process according to
\begin{align}
    \tau^idv^i_t=f^i(v_t,w^i,t,Z_t)dt+\sigma^idB^i_t\label{eq29}
\end{align}
with $\tau^i>0$ a time constant, $\sigma^i>0$ a fixed variance and
$w^i=(w^i_j)_{j\in\mc{N}}$ are the synaptic weights local to $i\in\mc{N}$. The
usual regularities on $f^i$ required for existence/uniqueness of a strong
solution are suspended until the subsequent, more workable model. The aggregated
voltage trajectories $v_t=(v^i_t)_{t\geq 0}$ then have dynamics given by
\begin{align}
    \tau dv_t=F(v_t,w,t,Z_t)dt+\Sigma dB_t\label{eq30}
\end{align}
with $\tau=\diag(\tau^i)_{i\in\mc{N}}$, $F:=(f^i)_{i\in\mc{N}}$,
$\Sigma=\diag(\sigma^i)_{i\in\mc{N}}$ and $w=(w^i)_{i\in\mc{N}}$. Brownian
motion models the superposition of a very large number of independent current
fluctuations arising from stochastic state transitions in gated channels
\cite{Destexhe_Mainen_Sejnowski_1994} \cite{Faisal_Selen_Wolpert_2008} and is
the sole source of spike time randomness when the thresholds $\vartheta^i$ are
fixed. Included in this class of hard threshold models are many popular
approaches generating highly realistic voltage trajectories which can sometimes
be heuristically derived from CB models
\cite{Fourcaud-Trocmé_Hansel_Van_Vreeswijk_Brunel_2003},
\cite{Jolivet_Lewis_Gerstner_2004}, \cite{Gerstner_Kistler_Naud_Paninski_2014}.
\begin{example}[Leaky and Exponential Integrate-and-Fire Networks]\label{ex2}
Let $\mc{H}$ be a matrix of synaptic kernels (just as for NH networks in
definition \ref{def10}) and $w\in\mbb{W}$ a weight matrix. Leaky
integrate-and-fire networks are given by equations (\ref{eq29}) and
(\ref{eq30}), but with
\begin{align*}
    f^i(v_t,w^i,t,Z_t)=-(v^i_t-E_\ell)+\sum_{j\in\mc{G}(i)}w^i_j\int_0^th^i_j(t-s)Z^j(ds)
\end{align*}
for $i\in\mc{N}$, where $\E_\ell\in\mbb{R}$ parameterize a CB-style leak current
\cite{Knight_1972}. A popular nonlinear alternative, called the exponential
integrate-and-fire model \cite{Fourcaud-Trocmé_Hansel_Van_Vreeswijk_Brunel_2003}
includes an exponential positive feedback:
\begin{align*}
    f^i(v_t,w^i,t,Z_t)=-(v^i_t-E_\ell)+\tau_s^ie^{(v^i_t-\tilde{\vartheta}^i)/\tau^i_s}+\sum_{j\in\mc{G}(i)}w^i_j\int_0^th^i_j(t-s)Z^j(ds)
\end{align*}
where $\tau^i_s>0$ characterizes how rapidly the voltage increases near the
onset of spiking, and $\tilde{\vartheta}^i>\tau^i_s+r^i$ approximates the
maximum stable voltage of an isolated unit receiving a constant input current
(the current achieving this maximum is called  a rheobase)
\cite{Gerstner_Kistler_Naud_Paninski_2014}. Without resets, trajectories diverge
in finite time \cite{Touboul_2009} such that the model is robust to any choice
of $\vartheta^i$ sufficiently large. It has been shown that such an exponential
nonlinearity empirically approximates the input-output characteristics of a
class of real neurons
\cite{Badel_Lefort_Brette_Petersen_Gerstner_Richardson_2008}. Additionally, in
the limit $\tau^i_s\rightarrow 0$, one recovers the leaky, linear model with
rheobase $\tilde{\vartheta}^i=\vartheta^i$
\cite{Fourcaud-Trocmé_Hansel_Van_Vreeswijk_Brunel_2003}. Depending on the form
of the synaptic kernel, extrinsic interactions can also be supported by an
expanded state space (along the lines of (\ref{eq28})).
% In both models, and by an abuse of notation, $N^j_t=\sum_{T^j\in
% N^j([0,t])}\delta(t-T^j)$, where $\delta$ here is the Dirac delta function.
\end{example}
\begin{remark}
    As a point of convention (and convenience) taking $r^i:=0$ and
    $\vartheta^i:=1$ $\forall i\in\mc{N}$ is without loss of generality since
    one can always study the affine transformation $v^i_t\mapsto
    (v^i_t-r^i)/\vartheta^i$. 
\end{remark}
Much of the literature cited in this section was concerned with motivating
particular IF models as reductions of more complex biophysical ones and/or
validating such models empirically. However, there are relatively fewer results on
how these relate to probabilistic models. For example, it is unclear how
one might certify a process $\lambda^i_t$ as the intensity of $Z^i_t$ from the
dynamics (\ref{eq30}) and spiking (\ref{eq31}) without resorting to estimation.
In \cite{Jabin_Zhou_2023}, an alternative formulation is considered which
dispenses with the hard threshold criterion (\ref{eq31}) in favor of a
probabilistic mechanism based on spatial thinning. For this, assign to each $i\in\mc{N}$ a
standard $\mbb{R}^2_+$-Poisson process $\Pi^i$ and intensity process
$\lambda^i_t=\Phi^i(S^i_t,v^i_t)=\phi^i(v^i_t)\psi^i(S^i_t)$, where
$S^i_t:=t-\sup\{T\in Z^i:T<t\}$ and $\Phi^i$ is as for age-dependent NH networks
in (\ref{eq16}). It is always assumed that $\psi^i(t)=0$ for all
$t\in[0,\delta]$, where $\delta>0$ is an absolute refractory duration. Then each
unit is equipped with the point measure $Z^i$ where
\begin{align}
    Z^i(A):=\int_A\int_{\mbb{R}_+}\1{y\leq \lambda^i_t}\Pi^i(dy\times ds)\label{eq32}
\end{align}
for $A\in\mc{B}(\mbb{R}_+)$. By
theorem (\ref{thm1}), $Z^i$ admits $\lambda^i_t$ as a $\mc{F}^{Z}_t$-predictable
intensity (where $Z=(Z^i)_{i\in\mc{N}}$) provided $\int_0^t\lambda^i_sds$ is
bounded above by an integrable process for any $t\geq 0$ (this condition is quite easily satisfied by
simply requiring $\Phi^i<\infty$). The spiking process (\ref{eq32}) is the only
substantive alteration needed to formulate the SIF network model from the IF
model (\ref{eq29}).
\begin{definition}[Soft-Threshold Generalized Integrate-and-Fire
    Networks]\label{def21} Let $\mc{H}$ be a matrix of synaptic kernels and
    $w\in\mbb{W}$ a weight matrix. For $i\in\mc{N}$, the trajectory $v^i_t$ can
    be written in integral form as 
    \begin{align*}
        v^i_t=v^i_0+\int_0^tf^i(v_s)ds+\int_0^t\sigma^idB^i_s+\sum_{j\in\mc{G}(i)}w^i_j\int_0^th^i_j(t-s)Z^j(ds)-\int_0^tv^i_sZ^i(ds)
    \end{align*}
    where $f^i$, $\sigma^i$, $B^i_t$ are just as for IF models. The third term
    encodes synaptic perturbations directly to the voltage, and the final term
    induces the post-spike reset.
\end{definition}
Note that the current-based synaptic interactions from example (\ref{ex2}) have
been reinterpreted as voltage responses in (\ref{def21}) (dimensionless, in
either case). These dynamics are said to be of jump-diffusion type, for the
combined influence of a diffusive Wiener process and the discrete poisson-driven
jumps. For ease of notation, one can equivalently write
\begin{align}
    v^i_t=v^i_0+\int_0^tf^i(v_s)ds+\int_0^t\sigma^idB^i_s+\sum_{j\in\mc{G}(i)}w^i_j\int_0^th^i_j(v^i_s,t-s)Z^j(ds)\label{eq33}
\end{align}
with the understanding that $i\in\mc{G}(i)$, $w^i_i=1$ and
$h^i_i(v^i_s,t-s)=-v^i_s$. While the synaptic kernel representation above is
nice to illuminate the connection to NH networks, it is more mathematically
convenient add some number of synaptic state variables for each unit to generate
synaptic responses. This idea was described in equation (\ref{eq28}) for double
exponential and alpha kernels. As an example, (\ref{eq33}) might be re-expressed
in differential form
\begin{align}
    dv^i_t&=\left(f^i(v_t)+\sum_{j\in\mc{G}(i)}w^i_js^j_t\right)dt+\tilde{\sigma}^idB^i_s-v^i_tZ^i(dt)\label{eq34}\\
    ds^i_t&=-s^i_t/\tau_r^i+Z^i(dt).\label{eq35}
\end{align}
In this case the complete network may then be written generically as
\begin{align}
    dx_t=f(x_t, w)dt+\sigma dB_s+C(x_t)Z(dt)\label{eq36}
\end{align}
which is more familiar in the mathematical literature. Note that with such
dynamic synapses, the weights influence the drift directly; without them the
voltage responses are singular and weight dependence shifts to the jump
coefficient. State dependence of the jump coefficient is needed here to (at
least) implement the resets. Kernel-free representations like (\ref{eq36}) will
be the starting point for the next chapter.\\[5pt]
\indent The following result establishes how SIF networks could be considered a
unification of IF and NH networks, making them a particularly appealing
candidate for a pilot formulation of neuronal networks as stochastic teams. Note
that accents are used to distinguish objects belonging to SIF versus NH models
throughout. 
\begin{theorem}\label{thm2} Any NH network admits a realization as a SIF.
    Further, fixing thresholds $(\vartheta^i)_{i\in\mc{N}}$ and approximation bounds $\varepsilon>0$,
    and $\delta>\tilde{\delta}>0$, for each $i\in\mc{N}$ there exists an intensity process $\lambda^i_t$
    so that $P(Z^i((T,T+\tilde{\delta}])=1)>1-\varepsilon$ whenever $T$ is a $\vartheta^i$-crossing.
\end{theorem}
\noindent {\bf Proof}. Fixing a NH network $(\Phi,\mc{H})$ with resulting
intensities $\lambda^i_t$, one needs only to choose
$(\hat{\Phi},\hat{\mc{H}})=(\Phi,\mc{H})$, $\hat{f}^i\equiv 0$,
$\hat{\sigma}^i=0$, $\hat{v}^i_0=0$,
$\hat{\lambda}^i_t=\hat{\Phi}^i(\hat{S}^i_t,\hat{v}^i_t)$ and eliminate the
reset term for each $i\in\mc{N}$ of a SIF network to immediately obtain sample
path equivalence; to each $\omega\in\Omega$ and any $t\in\mbb{R}_+$,
\begin{align*}
    Z^i_t(\omega)&=\int_0^t\int_{\mbb{R}_+}\1{y\leq \Phi\left(S^i_s,\;\sum_{j\in\mc{G}(i)}w^i_j\int_0^{s^-}h^i_j(s-r)Z^j(\omega, dr)\right)}\Pi^i(\omega,dy\times ds)\\
    &=\int_0^t\int_{\mbb{R}_+}\1{y\leq \hat{\Phi}^i(\hat{S}^i_s,\hat{v}^i_s)}\Pi^i(\omega,dy\times ds)
\end{align*}
which is just $\hat{Z}^i_t(\omega)$ from (\ref{eq32}) associated to unit $i$ of
the constructed SIF.\\[5pt]
\noindent For the second claim, set all SIF components excluding $\hat{\Phi}$
and fix approximation bounds $\varepsilon,\tilde{\delta}>0$. Setting SIF
intensities to satisfy the absolute refractory condition $\hat{\Phi}^i(s,v)=0$
whenever $0\leq s<\delta$ implies that $\hat{Z}^i((T,T+\tilde{\delta}])\leq 1$
almost surely. To see this, define
$\xi=\inf\{s\in(T,T+\tilde{\delta}]:\hat{Z}^i_s-\hat{Z}^i_T=1\}$ and assume that
this set is nonempty (since otherwise $\hat{Z}^i((T,T+\tilde{\delta}])=0$). Then 
\begin{align*}
    \hat{Z}^i((T,T+\tilde{\delta}))=\hat{Z}^i_\xi-\hat{Z}^i_T+\hat{Z}^i((\xi,T+\tilde{\delta}])=1+\int_\xi^{T+\tilde{\delta}}\int_{\mbb{R}_+}\1{y\leq \hat{\Phi}^i(s-\xi,\hat{v}^i_s)}\Pi^i(dy\times ds)
\end{align*}
where  $\hat{Z}^i_{\xi}-\hat{Z}^i_T=1$ since $\hat{Z}^i_t$ is right continuous.
But $s-\xi<\delta$ for $s\in(\xi,T+\tilde{\delta}]$, so the second term is zero
almost surely. Henceforth imposing this absolute refractoriness, let
$\mc{A}=\{(s,y)\in\mbb{R}_+^2:s\in(T,T+\tilde{\delta}],\;y\leq\hat{\Phi}^i(\hat{S}^i_s,\hat{v}^i_s)\}$,
and $\mu$ the Lebesgue measure on $\mc{B}(\mbb{R}_+^2)$. Then
\begin{align*}
    P(\hat{Z}^i((T,T+\tilde{\delta}])=1)=P(\hat{Z}^i((T,T+\tilde{\delta}])\geq 1)=P(\Pi^i(\mc{A})\geq 1)=1-e^{-\mu(\mc{A})}
\end{align*}
since $P(\Pi^i(\mc{A})=0)=e^{-\mu(\mc{A})}$. Finally, choose $C>0$ so that
$e^{-\tilde{\delta}C}<\varepsilon$ and set the intensities
$\hat{\Phi}^i(\hat{S}^i_t,\hat{v}^i_t)=C$ whenever $t\in(T,T+\tilde{\delta}]$,
with $\hat{\Phi}^i(\hat{S}^i_t,\hat{v}^i_t)=0$ otherwise (for any such $T$).
Then from the preceding equalities:
\[P(\hat{Z}^i((T,T+\tilde{\delta}])=1)=1-e^{-\mu((T,T+\tilde{\delta}]\times[0,C])}=1-e^{-\tilde{\delta}C}>1-\varepsilon\]
such that unit $i$ spikes once within time $\tilde{\delta}$ of crossing the hard
threshold $\vartheta^i$ with arbitrarily high probability, and remains quiescent
otherwise, almost surely.\hfill{$\qed$}\\[5pt]
\indent Theorem \ref{thm2} establishes NH networks as a degenerate case of SIF
and shows that the latter can be designed to approximate hard threshold behavior
characteristic of traditional IF models. Acknowledging that hard thresholds
have long been considered an idealism \cite{Blair_Erlanger_1933} (and all the
more considering the microscale \cite{Faisal_Selen_Wolpert_2008} and dynamical
underpinnings \cite{Rinzel_Ermentrout_1998} of spike generation) the benefit of
this result is as a unifying generalization of two popular strategies utilizing
distinct modeling ethoses. Furthermore, the combination of mechanistic and
probabilistic dynamical features results in a jump-diffusion-type model, the
class of which has received significant attention in recent decades, primarily
for their applications in finance \cite{Jang_2007}. As such, the SIF model will be equivalently
referred to as a \textit{neural jump-diffusion}, which is perhaps more technically evocative (while SIF might be preferred from
a computational neuroscientist's perspective in light of theorem \ref{thm2}). For their expressivity,
mathematical tractibility and formal connection to various neuroscientific
models, SIF networks are proposed as a central platform for the development of an
optimal control theory of neurocomputation.
\section{Synthesis of Discrete Time Neural Jump-Diffusion Networks}\label{sec7}
As a final point of preparation for the introduction of control, this section deals with the synthesis of a discrete time variant of the uncontrolled SIF network from the previous section.
Discrete time models are desirable for primarily technical reasons, at once allowing us to more readily apply
results from large scale decentralized stochastic control (see $\S$\ref{sec3}) and 
simplifying the process of modeling putative neurocomputations as control objectives. The substantive adjustment
enabling this is an alternative means of thinning (proposition \ref{prop4}) in order to apply existing
approximation schemes. 
\subsection{Preliminary Model Restructuring}\label{subsec2}
In the previous section, example (\ref{eq28}) and equations
(\ref{eq34}-\ref{eq36}) describe how more realistic network interactions can be
realized by the inclusion of impulse-driven synaptic state variables. In
particular, it was seen that the synaptic weights can be relocated to the drift
coefficient by introducing synaptic filters. That is, for unit $i\in\mc{N}$, the
local dynamics can be expressed as
\begin{align}
    dx^i_t&=f^i(x_t,w^i_t)dt+\sigma^idB^i_t+C^i(x^i_{t-})Z^i(dt),\label{eq48}
\end{align}
where $x^i_{t-}:=\lim_{s\nearrow t}x^i_s$. Equation (\ref{eq48}) corresponds to
(\ref{eq36}), so the purely local jump term just implements potential resets and
initiates outgoing synaptic currents through a state variable $s^i_t$
(\ref{eq35}). Such stateful synapses obfuscate the fact that interactions are
fundamentally mediated through the point measure $Z=(Z^i)_{i\in\mc{N}}$, so
henceforth the simpler dynamics
\begin{align}
    dx^i_t&=f^i(x^i_t)dt+\sigma^idB^i_t+C^i(x^i_{t-},w^i_t)Z(dt)\label{eq49}
\end{align}
are considered. The usual voltage process $v^i_t$ has been replaced with a
generic state $x^i_t$ to evoke the possibility of an augmented state space (to
include time, refractory periods, etc.). Additionally, the local jump
coefficient $C^i(x^i_{t-},w^i_t)$ is a linear functional on $\mbb{R}^k$ given by 
\[C^i(x^i_{t-},w^i_t)=(w^i_1(t)C^i_1(x^i_{t-}),w^i_2(t)C^i_2(x^i_{t-}),\dots,w^i_N(t)C^i_N(x^i_{t-})).\]
The minimum specification for $C^i$ is that it implements local voltage resets,
in which case if $x^i_t=(v^i_t,\dots)\in\mbb{R}^d$ is a finite state vector, then
$C^i_i(x)=(-v,0,0,\dots, 0)\in\mbb{R}^d$ for $i\in\mc{N}$, $x\in\mbb{X}^i$. When
$i\neq j$, $C^i_j$ can be used to specify the polarity of $j$ and distinguish
excitatory from inhibitory subpopulations (e.g. $C^i_j(x)\equiv
c_j\in\{-1,1\}$).\\[5pt]
\indent Existing discrete time approximation schemes for jump-diffusions typically
call for an alternative thinning formulation expressing the point measures $Z^i$
directly in terms of a driving Poisson MPP, rather than a spatial process over
$\mbb{R}_+^2$ (the particular complication arising with the latter is that
$\Pi(\mbb{R}_+\times dt)$ is not locally finite). To realize this for the SIF,
first note that the target intensity $\Phi^i(S^i_t,x^i_{t-})$ may be expressed
with $\Phi^i(x^i_{t-})$ by augmenting the state process $x^i_t$ with $S^i_t$
given by
\begin{align}
    dS^i_t=dt-S^i_{t-}Z^i(dt)\label{eq50}
\end{align}
so that absolute refractoriness can be expressed in a purely state dependent
manner. Following the construction in \cite[\S 3]{Glasserman_Merener_2003}, take
$\Pi$ to be a MPP on $Y\times\mbb{R}_+$, with mark space $Y=(0,1)\times\mc{N}$,
and intensity kernel
$\lambda_t(dy\times\{i\})=:\lambda^i_t(dy)=\frac{\lambda}{N}dy$ for
$i\in\mc{N}$. In particular, $\lambda>0$ is a global driving rate parameter, and
the marks are marginally uniform on $(0,1)$ and $\mc{N}$. Recalling that
$(\Phi^i)_{i\in\mc{N}}$ are uniformly bounded, $\lambda$ is assumed large enough
to satisfy
\begin{align}
    \Phi(x^i)<\frac{\lambda}{N},\quad \forall i\in\mc{N},\;x^i\in\mbb{X}^i\label{eq51}.
\end{align}
Then a new state dependent thinning function
$\theta:Y\times\mbb{X}\rightarrow\{0,1\}^{N}$ can be assigned to $\Pi$
directly: for $y\in Y$ with $y=(y^\prime,\{i\})$, define
\begin{align}
    \theta(y,x)=(\theta^j(y^\prime,x^j)\1{j=i})_{j\in\mc{N}},\quad\text{where}\quad \theta^j(y^\prime,x^j)=\1{y^\prime<\frac{N}{\lambda}\phi^j(x^j)}\label{eq52}
\end{align}
which can be used to selectively admit points from the $\Pi$ as spikes in each
of the $Z^i$. In the spatial thinning method, each unit has a private driving Poisson
measure with a local state dependent thinning function $\1{y\leq
\Phi^i(x^i_{t-})}$. The alternative conception here produces $N$ point
processes from a single Poisson MPP by using $(x_t)_{t\geq 0}$ to partition $Y$
via $(\theta^i)_{i\in\mc{N}}$ over time. Specifically, spikes are generated
according to the measure
\begin{align}
    Z_t:=\int_0^t\int_{(0,1)\times\mc{N}}\theta(y,x_{s-})\Pi(dy\times ds),\quad\text{and}\quad Z^i_t:=\int_0^t\int_0^1\theta^i(y,x^i_{s-})\Pi^i(dy\times ds)\label{eq53}
\end{align}
where $\Pi^i(dy\times ds)=\Pi(dy\times\{i\}\times
ds)=\sum_{j\in\mc{N}}\1{j=i}\Pi(dy\times\{j\}\times ds)$, and
$(\Pi^i)_{i\in\mc{N}}$ is a set of independent Poisson MPPs, since they are
defined as measures over disjoint domains. It can be shown that $Z^i_t$ defined
this way has predictable intensity $\upsilon^i_t=\Phi^i(x^i_{t-})$, just as for
spatial thinning.
\begin{proposition}[Glasserman \& Merener, (2003)]\label{prop4}
    If $(\Phi^i)_{i\in\mc{N}}$ and $\Pi$ satisfy (\ref{eq51}), and
    $\Phi^i(x^i_{t-})$ is $\mc{F}_t$-predictable for $i\in\mc{N}$, then $Z^i_t$
    defined as in (\ref{eq53}) admits stochastic intensity process given by
    $\lambda^i_t=\Phi^i(x^i_{t-})$.
\end{proposition}
\noindent{\bf Proof}. The $\theta^i$ are measurable functions of a predictable
process, and so are themselves $\mc{F}_t$-predictable. Furthermore, for $t\geq
0$ and $i\in\mc{N}$, 
\begin{align*}
    \E\left(\int_0^t\int_{(0,1)}\theta^i(y,x^i_{s-})\frac{\lambda}{N}dyds\right)=\E\left(\int_0^t\int_0^{\frac{N}{\lambda}\Phi^i(x^i_{s-})}\frac{\lambda}{N}dyds\right)&=\E\left(\int_0^t\Phi^i(x^i_{s-})ds\right)
    <\frac{\lambda}{N}t
\end{align*}
which is finite. Then, by \ref{prop2}, the process
\begin{align*}
    X_t:=\int_0^t\int_{(0,1)}\theta^i(y,x^i_{s-})\left[\Pi^i(dy\times ds)-\frac{\lambda}{N}dyds\right]=Z^i_t-\int_0^t\int_{(0,1)}\theta^i(y,x^i_{s-})\frac{\lambda}{N}dyds
\end{align*}
is a martingale. Using the same computation as above, this process can be
rewritten
\begin{align*}
    X_t=Z^i_t-\int_0^t\Phi^i(x^i_{s-})ds
\end{align*}
where now the fact that $Z^i_t-\int_0^t\Phi^i(x^i_{s-})ds$ is martingale
certifies $\upsilon^i_t=\Phi^i(x^i_{t-})$ as the stochastic intensity of the
spiking process $Z^i_t$ via proposition \ref{prop3}.\hfill{$\qed$}\\[5pt]
\indent Incorporating this new representation into the local dynamics yields
\begin{align}
    dx^i_t=f^i(x^i_i)dt+\sigma^idB^i_t+\int_{(0,1)\times\mc{N}}C^i(x^i_{t-},w^i_t)\theta(y,x_{t-})\Pi(dy\times dt)
\end{align}
whence it is convenient to define a new jump coefficient. For
$y=(y^\prime,\{k\})\in (0,1)\times\mc{N}$, let
\begin{align}
    \Theta^i(x^i_{t-},w^i_t,y):=C^i(x^i_{t-},w^i_t)\theta(y,x^i_{t-})=\sum_{j\in\mc{N}}w^i_j(t)C^i_j(x^i_{t-})\theta^j(y^\prime,x^j_{t-})\1{j=k}.\label{eq54}
\end{align}
Adopting the usual convention $\Theta=(\Theta^i)_{i\in\mc{N}}$, the network
dynamics are given by
\begin{align}
    dx_t=f(x_t)dt+\sigma dB_t+\int_{(0,1)\times\mc{N}}\Theta(x_{t-},w_{t},y)\Pi(dy\times ds)\label{eq55}
\end{align}
such that $\Theta$ now distributes points directly from $\Pi$ to each unit in a
manner consistent with the network spiking process, as dictated by the
excitability profiles $(\Phi^i)_{i\in\mc{N}}$. In this new form the network is
amenable to classical results in the theory of jump-diffusions, such as
conditions for the existence of a unique, strong solution.
\begin{theorem}[Ikeda \& Watanabe (1989)]\label{thm4} Let
    $(\Omega,\mc{F},(\mc{F}_t)_{t\geq 0},P)$ be a filtered probability space,
    and consider the following stochastic differential equation
    \begin{align*}
        dX_t=b(X_t)dt+\sigma(X_t)dB_t+\int_\Gamma q(X_{t-},\gamma)\Pi(d\gamma\times dt)\tag{$\ast$}
    \end{align*}
    with drift $b:\mbb{R}^d\rightarrow\mbb{R}^d$, diffusion
    $\sigma:\mbb{R}^d\rightarrow\mbb{R}^{d\times r}$ and jump
    $q:\mbb{R}^d\times\Gamma\rightarrow\mbb{R}^d$ coefficients measurable,
    $(B_t)_{t\geq 0}$ an $\mc{F}_t$-adapted $\mbb{R}^r$-valued standard Wiener
    process, and $\Pi$ a marked Poisson process on $\Gamma\times\mbb{R}_+$ with
    time-homogeneous intensity kernel $\lambda(d\gamma)$, finite on $\Gamma$. If
    these coefficients satisfy the linear growth constraint
    \begin{align*}
        \|b(x)\|^2+\|\sigma(x)\|^2+\int_\Gamma\|q(x,\gamma)\|^2\lambda(d\gamma)\leq M_1(1+\|x\|^2),\quad\forall x\in\mbb{R}^d
    \end{align*}
    for some $M_1>0$, and the Lipschitz condition
    \begin{align*}
        \|b(x)-b(y)\|^2+\|\sigma(x)-\sigma(y)\|^2+\int_\Gamma\|q(x,\gamma)-q(y,\gamma)\|^2\lambda(d\gamma)\leq M_2\|x-y\|^2,\quad\forall x,y\in\mbb{R}^d
    \end{align*}
    for some $M_2>0$, then there exists a unique, strong solution to ($\ast$).
\end{theorem}
For a proof, see \cite[ch.4 theorem 9.1]{Ikeda_Watanabe_1989}. The hypotheses of
theorem \ref{thm4} hold for the model (\ref{eq55}) provided they hold for $f^i$,
$C^i_j$, and $w^i_j$ for $i,j\in\mc{N}$, treating $t$ as a (deterministic) state
variable. For clarity, a strong solution in this context is a c\'adl\'ag process
$(x_t)_{t\geq 0}$ adapted to reference filtration $\mc{F}_t$, for which there
exists an $\mc{F}_t$-Wiener process $B_t$ and Poisson MPP $\Pi_t$ satisfying
(\ref{eq55}) almost surely. Of course, this is to say nothing of the
existence/uniqueness of a strong solution to a controlled neural jump-diffusion.
At this stage there are two possible directions to proceed from the uncontrolled
model (\ref{eq55}) to a stochastic team.\\[5pt]
\indent First, control, information structure and cost could be introduced in
the continuous time description, and from there one could try to derive an
approximating discrete time control problem. Illustrating this program, the
dynamics would be controlled by a collection of $\mbb{U}^i$-valued process
$(u^i_t)_{t\geq 0}$ depending causally on some local information process
$\mc{I}^i_t$. Causality is in the sense that $u^i_t=\gamma^i_t(\mc{I}^i_t)$ for
an admissable (measurable) policy $(\gamma^i_t)_{t\geq 0}\in\Gamma^i$ and the
information $\mc{I}^i_t$ is $\mc{F}_t$-predictable for $i\in\mc{N}$, $t\geq 0$.
Preempting the discussion in a later section, this could manifest in
(\ref{eq55}) as
\begin{align*}
    dx_t=f(x_t)dt+\sigma dB_t+\int_{(0,1)\times\mc{N}}\Theta(x_{t-},u_t,y)\Pi(dy\times ds)
\end{align*}
for example. Associated to this would be an optimal cost
\begin{align*}
    J^\ast(x_0)=\inf_{\gamma\in\Gamma}J(x_0,\gamma)
\end{align*}
where $\Gamma=\prod_{i\in\mc{N}}\Gamma^i$, and now the hope is to find a
$\gamma^\ast\in \Gamma$ realizing this infimum -- assuming it exists in the
first place. Then one seeks an approximating discrete time control system
parameterized by $h>0$ so that 
\begin{align*}
    x^h_{t+1}=F^h(x^h_t,u^h_t,z^h_t,\Pi^h_t)
\end{align*}
for some $\iid$ noise process $(z^h_t)_{t\in\mbb{Z}_+}$, so that
$(x^h_t,u^h_t,z^h_t,\Pi^h_t)_{t\in\mbb{Z}_+}$ converges (in some sense) to the
controlled jump-diffusion as $h\rightarrow 0$. Crucially, a discrete time
optimal policy $\gamma^{h,\ast}=\argmin_{\gamma\in\Gamma^h}J^h(x_0,\gamma)$
should be near-optimal for the continuous time model; for any $\varepsilon>0$,
$\exists h>0$ so that 
\begin{align*}
    J(x_0,\gamma^{h,\ast})<J^\ast(x_0)+\varepsilon.
\end{align*}
\indent Alternatively, from (\ref{eq55}) one could first obtain an uncontrolled
(i.e. dynamical) approximation to the drift-diffusion dynamics, and then
formulate the control problem -- suspending any claim that the discretized
controlled system solves any continuous time one. The expectation is that one
would later certify that the discrete time model devised indeed solves a
controlled jump-diffusion in the sense vaguely described above. Such a result
could follow by extending previous work for approximating decentralized
diffusions \cite[\S 5.4, theorem 5.5]{Pradhan_Yüksel_2023} to jump-diffusions
with controlled jump/drift coefficients \cite{Kushner_2000}. This alternative
approach is favorable since it avoids the distracting complications of a
continuous time setting while attempting to formulate and analyze
neuroscientifically meaningful problems.
\subsection{Discrete Time Dynamical Approximation}
The assumptions of theorem \ref{thm4} are not only sufficient for the existence
of a unique strong solution, but also for first order strong convergence results
invoked here \cite{Bruti-Liberati_Nikitopoulos-Sklibosios_Platen_2006}. Fix a
finite time horizon $T>0$, and consider the problem of approximating $x^i_t$ for
$t\in[0,T]$ at finitely many points $0=t_1<t_2<\cdots<t_M=T$. A number
$h\in(0,1)$ is called a maximum step size if $P(t_{m+1}-t_m<h)=1$ for $m<M$.
With such an $h$, $x^{i,h}_{t_m}=x^{i,h}_m$ shall denote an approximating
discrete time process, and $x^{i,h}_t=x^{i,h}_{t_m}$ for $t\in[t_m,t_{m+1})$ for
$1\leq m<M-1$ its constant interpolation on $[0,T]$.
\begin{definition}[Uniform Order $\kappa$ Strong Convergence]\label{def24} An
    approximation $x^{i,h}=(x^{i,h}_m)_{m=1}^M$ converges strongly uniformly
    with order $\kappa$ to the solution of (\ref{eq55}) if $\exists \alpha,
    h^\ast>0$ with
    \begin{align*}
        \E\left(\sup_{0\leq s\leq T}\|x^i_{s}-x^{i,h}_s\|^2\right)\leq \alpha h^{2\kappa}
    \end{align*}
    whenever the maximum step size $h\in(0,h^\ast)$, and where $\alpha$ is
    independent of $h$.
\end{definition}
Definition (\ref{def24}) implies that $x^{i,h}$ remains pathwise close to the
true solution by taking $h$ small. Perhaps the simplest approach to achieve a
strongly convergent approximation is with a simple Eulder scheme on an
equidistant grid
\cite[$\S$3]{Bruti-Liberati_Nikitopoulos-Sklibosios_Platen_2006}. Specifically,
let $M_T=T/h$ and $t_m=mh$ for $0\leq m\leq M_T$ be the discretization points of
$[0,T]$ and define the approximation $x^{i,h}$:
\begin{align}
    x^{i,h}_{m+1}=x^{i,h}_m+f^i(x^{i,h}_m)h+\sigma^i\sqrt{h}z^i_m+\int_{t_m}^{t_{m+1}}\int_{(0,1)\times\mc{N}}\Theta(x^{h}_{m},w^i_m,y)\Pi(dy\times ds)\label{eq56}
\end{align}
for $0\leq m\leq M_T-1$ and with $(z^i_m)_{m\in\mbb{Z}_+}$ an $\iid$ standard
Gaussian sequence. Denoting the marks generated by $\Pi$ with a random sequence
$((y_n,k_n))_{n\geq 1}\subset (0,1)\times\mc{N}$, and
$\Pi_t=\Pi((0,1)\times\mc{N}\times [0,t])$, the synaptic interaction term can be
written
\begin{align}
    \int_{t_m}^{t_{m+1}}\int_{(0,1)\times\mc{N}}\Theta(x^h_m,w^i_m,y)&\Pi(dy\times ds)\notag\\
    &=\1{\Pi_{t_{m+1}}-\Pi_{t_m}\geq 1}\left(\sum_{\ell=\Pi_{t_m}+1}^{\Pi_{t_{m+1}}}\sum_{j\in\mc{N}}w^i_j(t_m)C^i_j(x^{i,h}_{t_m})\theta^j(y_\ell,x^j_{t_m})\1{j=k_\ell}\right)\notag{}\\
    &=\sum_{j\in\mc{N}}w^i_j(t_m)C^i_j(x^{i,h}_{t_m})Z^{j,h}_{t_m}
\end{align}
where
$Z^{j,h}_{t_m}:=\int_{t_m}^{t_{m+1}}\int_{(0,1)}\theta^j(y,x^{j,h}_{t_m})\Pi^j(dy\times
dt)$ approximates $Z^j$ by thinning over the increment $(t_m,t_m+h]$ with the
constant approximant $x^{j,h}_{t_m}$. In particular,
\begin{align*}
    \E\left(\int_{t_m}^{t_{m+1}}\int_{(0,1)}\theta^j(y,x^{j,h}_{t_m})\Pi^j(dy\times dt)\right)=\E\left(\int_{t_m}^{t_m+h}\int_{(0,1)}\theta^j(y,x^{j,h}_{t_m})\frac{\lambda}{N}dydt\right)=\Phi^j(x^{i,h}_{t_m})h
\end{align*}
so $Z^{j,h}_{t_m}$ is a Poisson random variable with rate
$\Phi^j(x^{j,h}_{t_m})h$. As long as no spikes impinge on unit $i$, the
approximation is identical to the typical Euler-Maruyama scheme for diffusive
processes, and in fact inherits the same order of strong convergence.
\begin{theorem}[Bruti-Liberati \& Platen (2005)]\label{thm13}
    Let $x^{i,h}$ be obtained via the Euler scheme (\ref{eq56}) for some
    $h\in(0,1)$, and that model (\ref{eq55}) satisfies the growth and Lipschitz
    conditions of theorem (\ref{thm4}). If
    \begin{align*}
        \E(\|x^i_0\|^2)<\infty,\quad\text{and}\quad \E(\|x^i_0-x^{i,h}_0\|^2)\leq \alpha h
    \end{align*}
    for some $\alpha>0$, then $x^{i,h}$ converges strongly uniformly with order
    $\kappa=1/2$ to the strong solution $x^i_t$ of (\ref{eq55}) on $[0,T]$.
\end{theorem} 
For a detailed proof, see \cite[theorem 6.1]{Bruti-Liberati_Platen_2005} or the
more comprehensive reference \cite[theorem 6.4.1]{Platen_Bruti_Liberati_2010}.
This has been a rather expedient route to a discrete time model for
(\ref{eq55}), which may be rewritten
\begin{align}
    x^i_{t+1}=F^i(x^i_t,w^i_t,z^i_t,Z_t).\label{eq57}
\end{align}
Now, $((x^i_t, w^i_t))_{t\in\mbb{Z}_+}$ forms the state-weight profile,
$(z^i_t)_{t\in\mbb{Z}_+}$ is an $\iid$ standard Gaussian sequence, and
$Z_t=(Z^j_t)_{j\in\mc{N}}$ is a Poissonian noise with state dependent rate so
that $\E(Z^i_t)=\Phi^i(x^i_t)h$ for $i\in\mc{N}$. To be explicit, $F^i$ is
defined according to (\ref{eq56}):
\begin{align}
    F^i(x^i_t,w^i_t,z^i_t,Z_t)&=x^i_t+f^i(x^i_t)h+\sigma^i\sqrt{h}z^i_t+\sum_{j\in\mc{N}}w^i_{j,t}C^i_j(x^i_t)Z^i_t\\
    &=x^i_t+f^i(x^i_t)h+\sigma^i\sqrt{h}z^i_t+\sum_{j\in\mc{N}}w^i_{j,t}C^i_j(x^i_t)\int_{\Delta_t}\int_{(0,1)}\theta^j(y,x^j_t)\Pi^j(dy\times ds)\label{eq63}
\end{align}
for $i\in\mc{N}$, where $\Delta_t$ is the region of time lying between discretization points $t$
and $t+1$ for any $t\in\mbb{Z}_+$. Then by a mild abuse notation, (\ref{eq57}) can
be expressed as $x^i_{t+1}=F^i(x^i_t,w^i_t,x^{-i}_t,z^i_t,\Pi_t)$
where in this context, $\Pi_t$ denotes the restriction of $\Pi$ to
$(0,1)\times\mc{N}\times\Delta_t$ such that $(\Pi_t)_{t\geq 0}$ are independent, and can be regarded as identically distributed
by mapping the points on $\Delta_t$ to $\Delta_t-t=(0,h]$. In case the original continuous time model had stateful synapses (as in (\ref{eq48}))
the discrete time local dynamics could instead be expressed in the general form
\begin{align}
    x^i_{t+1}&=x^i_t+\left(f^i(x^i_t)+\sum_{j\in\mc{N}}w^i_{j,t}s^j_t\right)h+\sigma^i\sqrt{h}z^i_t+C^i_i(x^i_t)\int_\Delta\int_{(0,1)}\theta^i(y,x^i_t)\Pi^i(dy\times ds)
\end{align}
or $x^i_{t+1}=F^i(x^i_t,w^i_t,x^{-i}_t,z^i_t,\Pi^i_t)$, with $\Pi^i_t$ the restriction of $\Pi$ to $(0,1)\times\{i\}\times\Delta_t$, where all notation has been coopted from
the stateless synapse model. In particular, coupling between units has been consolidated to the state variable, which may be more mathematically expedient in some scenarios.
\section{Summary and Remarks}
The purpose of this chapter was to ground the subsequent development of discrete
time controlled models in existing dynamical models of computational
neuroscience. We broadly distinguish single neuron models as being either
probabilistic (point processes, SIF units) or mechanistic (CB, IF models) (a
distinction also noted in
\cite{Kass_Amari_Arai_Brown_Diekman_Diesmann_Doiron_Eden_Fairhall_Fiddyment_et_al._2018})
and review some merits of each with respect to their biological realism and mathematical structure.
The main limitation of CB models is their lack of a precisely defined spiking process, which is needed
to develop computational objectives commensurate with spike coding. Point process models are a natural
solution for realizing mathematically tractable spiking processes, but a more rigorous mathematical characterization of these (while classical)
is largely absent from the neuroscience literature. For this reason, we included a concise overview of point process theory, with an emphasis on
the central role of stochastic intensities and the notion of thinning as a modeling tool. Two such thinning formulations are introduced: spatial thinning (theorem \ref{thm1}) and MPP thinning (proposition \ref{prop4}).
The former is used occasionally in the more mathematically-oriented modeling literature \cite{Giraudo_Sacerdote_1997}, \cite{Jahn_Berg_Hounsgaard_Ditlevsen_2011}, \cite{Fournier_Löcherbach_2016}, \cite{Jabin_Schmutz_Zhou_2024}
while the latter never appears, and is instead introduced here for its ubiquity in the existence, uniqueness and discrete time approximation results. However, spatial thinning may be more appropriate for future developments, since MPP thinning may not
support spiking in a population as $N\rightarrow\infty$.\\[5pt]
\indent Our uncontrolled model of choice, the SIF (or neural jump-diffusion) has appeared throughout the last couple decades, but usually without any membrane diffusion component \cite{Cormier_Tanré_Veltz_2021}, \cite{Fournier_Löcherbach_2016}. This is sometimes justified
using the concept of escape noise, where a stochastic intensity subsumes the contribution of diffusive noise to spike time variability \cite{Plesser_Gerstner_2000}. Instead, our model is nearly identical to (and is strongly inspired by) that of \cite{Jabin_Zhou_2023}.
In order to make this choice as compelling as possible from a modeling perspective, theorem \ref{thm2} established neural jump-diffusions as a stochastic generalization of
traditional IF methods (for which a stochastic intensity characterization is difficult to obtain) and self-exciting multivariate point processes (which lack a state process for modeling mechanistic contributions to spiking). Finally, we present a strongly converging discrete time approximation for the uncontrolled dynamics
under MPP thinning. Acknowledging the option to formulate control in continuous time before obtaining a controlled discrete time model, we instead opt to work backwards, modeling and some preliminary research on the discrete time case before considering a robustness analysis for the ensuing error.
\chapter{Controlled Network Models: Discrete Time Neural Stochastic Teams}
This section deals with the task of imbuing neural jump-diffusions with all the elements of control.
Each unit in the network is distinguished as an individual DM by specifying local dynamics (the topic of the previous chapter) along with information and action variables.
However, rather than specify a set of local objectives, the population acts cooperatively to optimize a single objective. This task structure is what characterizes the controlled network as a
\textit{stochastic team}, and we refer the reader to appendix \ref{app2} for a formal definition. Moreover, team problems map onto the
neuroscientific notion of population coding, whereby neuronal representation and computation are considered to reside in the concerted activity of many cells.
As discussed at the end of $\S$\ref{subsec2}, we choose to focus on the discrete time setup while acknowledging that neurocomputations are continuous time phenomena.\\[5pt]
\indent Ultimately, elevating neural jump-diffusion networks to a stochastic team problem is another exercise in modeling. However, and unlike the previous chapter,
the literature supporting this effort is sparse -- neurons are rarely modeled as controllers and networks, it would seem, have never been modeled as a stochastic team.
In \cite{Ahmadian_Packer_Yuste_Paninski_2011}, \cite{Iolov_Ditlevsen_Longtin_2014} the problem of identifying an optimal stimulus to control spike-timing in a single IF neuron was considered. More recently,
\cite{Sepulchre_2022}, \cite{Schmetterling_Burghi_Sepulchre_2022}, \cite{Burghi_Sepulchre_2023} consider adaptive control of CB units, regarding maximal intrinsic conductance parameters as control-defining, but
only for small circuits and without specifying any cost function to guide controller design. The stochastic team formulation of this chapter is inspired by these works, and by the biophysical principles discussed through $\S$\ref{sec4} and $\S$\ref{sec1}.

\section{Team Actions and Information Structure}
\indent First we shall specify (i) what components of the model are to serve as
a control substrate, and (ii) the information to be made available at each of
the controllers. The principle hypothesis of this project holds that neurons act
to regulate their local dynamics using local information. In the framework
presented by \cite{Sepulchre_2022}, CB units have agency over their intrinsic
maximal conductance parameters, which in neuroscientific parlance can be
interpreted as a model of \textit{neuromodulation}. Of course, intrinsic
conductances are not a necessary feature of neural jump-diffusions (and IF
models, at large) so we adopt the -- philosophically similar -- position that
neurons update their extrinsic maximal conductances, or synaptic weights,
instead. We note that both neuromodulation (intrinsic conductance control) and synaptic plasticity (extrinsic conductance control) are important mechanisms of learning
in neuronal networks. However, and unlike intrinsic conductances, extrinsic conductances are innate in our neural
jump-diffusion formulation.\\[5pt]
\indent Recalling that the connection strength from unit $j$ to $i$
is denoted by $w^i_j$ taking value on an interval $\mbb{W}^i_j$, a
crucial step is to make the simple notational switch
\begin{align*}
    u^i_j:=w^i_j,\quad\text{and}\quad\mbb{U}^i_j:=\mbb{W}^i_j,\quad i,j\in\mc{N}
\end{align*}
signifying a new role for the synaptic weights as a control substrate, whence
the local $\mbb{U}^i=\prod_{j\in\mc{N}}\mbb{U}^i_j$ and network (or global, joint)
control spaces $\mbb{U}=\prod_{i\in\mc{N}}\mbb{U}^i$ are defined. Correspondingly, the dynamics become
\begin{align}
    x^i_{t+1}=F^i(x^i_t,u^i_t,z^i_t,Z_t),\quad\text{or}\quad x^i_{t+1}=F^i(x^i_t,x^{-i}_t,u^i_t,z^i_t,\Pi_t)\label{eq61}
\end{align}
where $x^{-i}_t:=(x^j_t)_{j\in\mc{N},\,j\neq i}$. This setup
shall be called \textit{weight}, or \textit{synaptic control}. Since these now determine the
network connectivity, some notion of agreement with a constraining network
topology $\mc{G}=(\mc{N},\mc{E})$ is needed.
\begin{definition}[Commensurateness]
    For an arbitrary network topology $\mc{G}$ and control space
    $\mbb{U}\subset\mbb{R}_+^{N^2}$, $\mbb{U}$ is called {\it
    $\mc{G}$-commensurate} and $\mc{G}$ is called {\it $\mbb{U}$-commensurate}
    if and only if
    \begin{align*}
        (i,j)\notin\mc{E}\longleftrightarrow \mbb{W}^i_j=\{0\},\quad i,j\in\mc{N}.
    \end{align*}
\end{definition}
The above simply says that the only admissable value of $u^i_j$ is $0$ whenever
$j\notin\mc{G}(i)$. For modeling purposes, some further structure can be
imposed on $\mbb{U}$. For $i,j\in\mc{N}$, $i\neq j$, we suppose that
\begin{align}
    \mbb{U}^i_j=[-\overline{w}^i_j,-\underline{w}^i_j]\cup[\underline{w}^i_j,\overline{w}^i_j]\label{eq93}
\end{align}
with $\underline{w}^i_j=\overline{w}^i_j=0$ if and only if $j\notin\mc{G}(i)$.
Self-connections are taken to be $\mbb{U}^i_i=\{1\}$, with $i\in\mc{G}$ for
$i\in\mc{N}$ so as to seemlessly include voltage resets amongst the network
interactions. The symmetry of this set about zero permits connections to be inhibitory, but without restriction
on whether any individual DM can transmit a mix of excitatory and inhibitory signals. This form defies so-called \textit{Dale's law} (misleadingly named, for it is not a law in the typical sense)
which suggests that a given biological neuron will only emit a single neurotransmitter species (and hence, all outgoing synapses will be of the same polarity) \cite{Osborne_1979}, \cite{Barranca_Bhuiyan_Sundgren_Xing_2022}.
Later, it may be necessary to introduce Dale's law by dividing the team into excitatory and inhibitory subpopulations, only permitting DMs to control the magnitude of incoming synapses.\\[5pt]
\indent Under these restrictions, the set of such $\mc{G}$-commensurate
control spaces can be classified further, and two subtypes are expected to be of
particular theoretical/experimental utility.
\begin{definition}[Maximal and Strict Network Topologies]
    For a $\mc{G}$-commensurate control space $\mbb{U}$, the set $\mbb{U}^i_j$
    is called {\it strict} if $\underline{w}^i_j>0$, and {\it maximal}
    otherwise. The space $\mbb{U}$ itself is then said to be strict (maximal)
    iff $\mbb{U}^i_j$ is strict (maximal) $\forall (i,j)\in\mc{E}$, $i\neq j$.
\end{definition}
Such a $\mc{G}$-maximal space $\mbb{U}$ is one where networks are permitted to
effectively dissolve connections through their policies. This setup could be
useful when the relationship between network topology and computation is of
interest, as well as when one wants to remain agnostic about connectivity.
Without belaboring the point prematurely, if $\mbb{U}$ is $\mc{G}$-maximal, then
$\exists (u_t)_{t\geq 0}\subset\mbb{U}$ and a corresponding time $T\geq 0$ and
subtopology $\mc{G}^\prime\subsetneq\mc{G}$ (in the sense that
$\mc{E}^\prime\subsetneq\mc{E}$) so that $(u_t)_{t\geq T}\subset\mbb{U}^\prime$,
where $\mbb{U}^\prime$ is $\mc{G}^\prime$-maximal. In this case, the minimal
stable topology corresponding to an optimal control (and implicitly the
prescribed task) would be of great theoretical value. Conversely, strict
topologies can be used to fix a connectivity and study other structural
properties of a solution. To avoid further esoterica prematurely, all instances
of $\mbb{U}$ in the context of weight control are taken to be $\mc{G}$-maximal
with respect to the complete (dense) graph topology $\mc{G}$, such that $(i,j)\in\mc{N}$ for all $i,j\in\mc{N}$.
In any case, the joint action space $\mbb{U}$ for the team is a compact subset of a real Euclidean space.
\begin{remark}[Regularity of Dynamics]\label{rem1}
    The thinning functions $(\theta^i)_{i\in\mc{N}}$ used in (\ref{eq61}) (and hence the $F^i$) are discontinuous in the state and mark space. This impedes the application of frameworks for studying
    large populations (\cite[assumption 3]{Bauerle_2023} and \cite[assumption 2.5.i]{Sanjari_Saldi_Yüksel_2023}) and robustness results for continuous time approximation (\cite[assumption 2.2]{Kushner_2000}).
    However, provided $\Phi^i:\mbb{X}^i\rightarrow[0,\frac{\lambda}{N})$ is continuous for $i\in\mc{N}$, a sequence of continuous thinning functions $(\tilde{\theta}^i_n)_{i\in\mc{N},n\geq 1}$ can be used
    to approximate the offending $Z^i_t$ terms. For example, setting
    \begin{align}
        \tilde{\theta}_n^i(y,x^i):=\1{y\leq \frac{N}{\lambda}\Phi^i(x^i)}+\left(-n\left(y-\frac{N}{\lambda}\Phi^i(x^i)\right)+1\right)\1{y\in(\frac{N}{\lambda}\Phi^i(x^i),\frac{N}{\lambda}\Phi^i(x^i)+\frac{1}{n}]},\quad i\in\mc{N},\;n\geq 1
    \end{align}
    one can define for each $t\geq 0$ an approximation $\tilde{Z}^{i,n}_t$ for the spike process $Z^i_t$ via
    \begin{align}
        \tilde{Z}^{i,n}_t:=\int_{\Delta_t}\int_{(0,1)}\tilde{\theta}^i_n&(y,x^i_t)\Pi^i(dy\times ds)\\
        &=Z^i_t-\int_{\Delta_t}\int_{(0,1)}\left(n\left(y-\frac{N}{\lambda}\Phi^i(x^i_t)\right)+1\right)\1{y\in(\frac{N}{\lambda}\Phi^i(x^i_t),\frac{N}{\lambda}\Phi^i(x^i_t)+\frac{1}{n}]}\Pi^i(dy\times ds)
    \end{align}
    in the sense that the expected error can be made arbitrarily small:
    \begin{align}
        \E(|Z^i_t-\tilde{Z}^{i,n}_t|)&=\E\left(\int_{\Delta_t}\int_{(0,1)}\left(n\left(y-\frac{N}{\lambda}\Phi^i(x^i)\right)+1\right)\1{y\in(\frac{N}{\lambda}\Phi^i(x^i),\frac{N}{\lambda}\Phi^i(x^i)+\frac{1}{n}]}\Pi^i(dy\times ds)\right)\notag{}\\
        &\leq 2\E\left(\int_{\Delta_t}\int_{(0,1)}\1{y\in(\frac{N}{\lambda}\Phi^i(x^i),\frac{N}{\lambda}\Phi^i(x^i)+\frac{1}{n}]}\frac{\lambda}{N}dyds\right)\notag{}\\
        &\leq \frac{2\lambda h}{nN}\notag.
    \end{align}
    Further, provided $C^i_j$ are bounded and continuous for $i,j\in\mc{N}$, the observation that $\Pi^i((0,1)\times\Delta_t)<\infty$ almost surely implies that for $(x^i_k)_{k\geq 1}\subset\mbb{X}^i$ with $x^i_k\rightarrow x^i$ as $k\rightarrow\infty$,
    \begin{align*}
        \lim_{k\rightarrow\infty}\int_{\Delta_t}\int_{(0,1)}u^i_jC^i_j(x^i_k)\tilde{\theta}^{j}_n(y,x^i_k)\Pi^j(dy\times ds)=\int_{\Delta_t}\int_{(0,1)}u^i_jC^i_j(x^i)\tilde{\theta}^j_n\Pi^j(dy\times ds)
    \end{align*}
    almost surely, by the dominated convergence theorem. In this way, the approximating dynamics $\tilde{F}^i$ (utilizing $\tilde{\theta}^i_n)$ are shown to be continuous for $i\in\mc{N}$, $n\geq 1$. A much more
    technical analysis would be required to characterize how the error induced by the $(\tilde{F}^i)_{i\in\mc{N}}$ propagates over time; that is, to characterize how
    $\E(\|x_t-\tilde{x}_t\|^2)$ grows in $t\in\mbb{Z}_+$. Queries of this kind shall be left to future work.
\end{remark}
\indent Following the central thesis, two decentralized information structures (ISs) shall be considered for
neuronal network models. First, and most conservative, is completely decentralized
information:
\begin{align}
    \mc{I}^i_t:=\begin{cases}
        \{x^i_{0:t},u^i_{0:t-1}\},\quad&\text{$t\geq 1$}\\
        \{x^i_0\},\quad&\text{$t=0$}
    \end{cases},\quad i\in\mc{N}\label{eq58}
\end{align}
such that units only track their local state-action profile. For now it shall be
assumed that information is of perfect recall where units do not forget
variables they previously had access to. This IS is largely one of parsimony in
the sense that units have access to a strict subset of the information available
under comparable structures. In particular, this can be considered a minimal information structure, providing a lower estimate on
the optimal performance of the team on a given task. That is, a larger IS would specify a superset of policies to optimize over, resulting in a nondecreasing performance \cite[\S7.2, proposition 7.2.1]{Yuksel_Basar_2024}.
For the second IS, we augment the completely decentralized IS by allowing units to track the \textit{empirical state distribution}. For this property, we call this a \textit{mean-field sharing} IS, and it turns out that
for a certain class of stochastic teams, it is sufficient to define a (near) optimal control (this is the main result of \cite{Sanjari_Saldi_Yüksel_2024}, and will be expounded upon in $\S$\ref{sec3}).
\begin{definition}[Empirical Measure/Mean Field]\label{def25} Suppose
    $\mbb{X}^i=\mbb{X}^j$ for $i,j\in\mc{N}$, writing $\mbb{X}:=\mbb{X}^i$.
    For $x\in\mbb{X}^N$ so that
    $x=(x^j)_{j\in\mc{N}}$, the measure $\mu[x]\in\mc{P}(\mbb{X})$ defined by
    \begin{align*}
        \mu[x](dx)=\frac{1}{N}\sum_{j\in\mc{N}}\delta_{x^j}(dx)
    \end{align*}
    where $\delta_{x^j}$ is the Dirac measure centered at $x^j\in\mbb{X}$, is
    called the {\it empirical measure} associated to the
    finite state $x\in\mbb{X}$. The set of such measures is denoted with $\mc{P}^N(\mbb{X})$. That is,
    \begin{align*}
        \mc{P}^N(\mbb{X}):=\left\{\mu\in\mc{P}(\mbb{X}):\mu=\mu[x]\;\text{for some}\;x\in\mbb{X}^N\right\}.
    \end{align*}
\end{definition}
Henceforth, the condition that $\mbb{X}^i=\mbb{X}^j=:\mbb{X}$ for $i,j\in\mc{N}$ is assumed, with the network state space denoted $\mbb{X}^N$.
With this, the mean field-sharing IS is defined by the local information variables
\begin{align}
    \mc{I}^i_t:=\begin{cases}
        \{x^i_{0:t},u^i_{0:t-1},\mu_{0:t}\},\quad&\text{$t\geq 1$}\\
        \{x^i_0,\mu_0\},\quad&\text{$t=0$}
    \end{cases},\quad i\in\mc{N}\label{eq59}
\end{align}
where $\mu_t:=\mu[v_t]$ for $t\geq 0$. Importantly, we suppose that only the voltage is conveyed via the mean field -- synapse and
refractory states are not conveyed in this manner. While this IS allows units to track the voltage distribution over the network, units cannot
associate any particular voltage signal to an individual unit. From a biological perspective, it may be reasonable to regard mean-field sharing as modeling
\textit{local field potentials} of the brain. That is, the activity of neurons in a small region of neural tissue give rise to an extracellular current which
perturbs, and is even thought to coordinate, nearby neurons (see \cite{Herreras_2016} for a recent review of this phenomenon). 
\section{Neurocomputation as a Control Objective}
\indent As per the principle hypothesis, control objectives are to be designed to model putative neurocomputational objectives. However, proposing a compelling model of neurocomputation \textit{a priori} is markedly more challenging than the other elements of our model
since so little is known about it -- let alone in sufficient technical detail to afford a clear mathematical translation. As a neuroscientific precursor to cost functions, one would minimally need to fix some notion for what the network {\it should} be doing, and the
process variables to which this prescription applies. To the latter, we adopt the view presented in \cite{Jaeger_Noheda_vanderWiel_2023}: that the computation-defining (i.e., cost-defining) process should be at least physically measurable by other system components. In neural tissue,
this perhaps makes spiking activity the most natural candidate for such a process. As for the prescriptive component (i.e., what the network should be doing) we remain largely agnostic at this time, and instead present a particular class of cost functions considered in \cite[ch.6]{Yuksel_Basar_2024},\cite{Sanjari_Saldi_Yüksel_2024},
and under which the existence of decentralized optimal controls was established for large stochastic teams. Following this thread, an important direction for our future work would be to characterize the computations realizable with this class of objectives.\\[5pt]
\indent Once again adopting the relatively uncontroversial view that spike times form the basis of a neural code, define the spiking process $Z_t=(Z^i_t)_{i\in\mc{N}}$
and stagewise costs $c_t,\tilde{c}_t:\mbb{R}_+^{N}\times\mbb{U}\rightarrow\mbb{R}_+$ for $0\leq t\leq T-1$ and $T\in\mbb{N}$
to produce a finite horizon cost criterion:
\begin{align}
    J_T(x_0,\gamma):=\E^\gamma_{x_0}\left(\sum_{t=0}^{T-1}c_t(Z_t,u_t)+c_T(Z_T)\right)=\E^\gamma_{x_0}\left(\sum_{t=0}^{T-1}\tilde{c}_t(x_t,u_t,\Pi_t)+\tilde{c}_T(x_T,\Pi_T)\right),\label{eq60}
\end{align}
where one could synonymously define a discounted cost $J_\beta$ (with $\beta\in(0,1)$, and $c_t\equiv\beta^tc$ for some $c:\mbb{R}^N_+\times\mbb{U}\rightarrow\mbb{R}$) or average cost $J_A$ (where $c_t\equiv\frac{1}{T}c$ instead). For any of these (and complementing the IS (\ref{eq59}) for the study of large
populations of agents \cite{Sanjari_Saldi_Yüksel_2024}) one can apply a set of permutation invariant or \textit{exchangeable} stagewise costs, which are needed to establish equivalence with an infinite population analog in \cite{Bauerle_2023} and \cite{Sanjari_Saldi_Yüksel_2023}. In this case, rather than viewing only $Z_t$ as cost defining, a slightly
more liberal view is adopted, allowing for dependence upon the empirical voltage distribution of the system via a time-homogeneous stagewise cost $\boldsymbol{c}:\mbb{X}^N\times\mbb{U}\rightarrow\mbb{R}_+$. To wit,
\begin{align}
    \boldsymbol{c}(x_t,u_t,\Pi_t):=\frac{1}{N}\sum_{j\in\mc{N}}c(x^j_t,u^j_t,\mu[v_t],\Pi_t)\label{eq64}
\end{align}
which can be simply plugged into the finite horizon $J_T$ (or $J_\beta,$ $J_A$) as in (\ref{eq60}), for example. For neural stochastic teams, our main contention is that such a cost should only depend on the state process $x^i_t$ through its spiking activity $Z^i_t$, or through some pairwise comparison of the voltages via the empirical measure. That is,
for \ref{eq64} we could define something to the effect of
\begin{align*}
    c(x^i_t,u^i_t,\mu[v_t],\Pi_t):=\tilde{c}\left(Z^i_t,u^i_t,\int_\mbb{V}g(v^i_t,v)\mu[v_t](dv)\right)=\tilde{c}\left(Z^i_t,u^i_t,\frac{1}{N}\sum_{j\in\mc{N}}g(v^i_t,v^j_t)\right),\quad i\in\mc{N}
\end{align*}
for a function $g:\mbb{V}^2\rightarrow\mbb{R}$, where $\mbb{V}$ is the
projection of $\mbb{X}$ into the voltage axis. Such a cost structure could be
used to incentivize membrane potential coupling, as is observed in biological
networks (see \cite{Denève_Machens_2016} for a discussion of when/how such
phenomena might arise) or even penalize it to discourage the system from
establishing a regime of high coordination, which can be characteristic of some
pathological brain states \cite{Uhlhaas_Singer_2006}.\\[5pt]
\indent In this framework of neural stochastic teams, the cost is to be viewed
as a platform for testing hypotheses -- an independent variable for proposing
sufficient (computational) conditions for biologically-expected behaviors to
arise. Thus our goal is never to fix a particular cost. The general formulations
of this section are only an initial articulation of what costs we might like to
work with, given some existing work on the control of large scale stochastic
systems \cite{Bauerle_2023},\cite{Sanjari_Saldi_Yüksel_2024}. We will revisit
exchangeable costs in $\S$\ref{sec3}.
\section{Summary and Remarks}
To conclude this chapter, let us summarize the neural stochastic team model.
Consider a population of units indexed $\mc{N}$, and with a prescribed strict
network topology $\mc{G}$. For this, define the state space $\mbb{X}^{N}$ and
action space $\mbb{U}$, commensurate with $\mc{G}$. With initial state $x_0\sim
\nu\in\mc{P}(\mbb{X}^N)$, the system evolves according to the discrete dynamics
\begin{align}
    x_{t+1}=F(x_t,u_t,z_t,\Pi_t)&=(F^i(x_t,u^i_t,z^i_t,\Pi_t))_{i\in\mc{N}}\notag\\
    &=\left(x^i_t+f^i(x^i_t)h+\sigma^i\sqrt{h}z^i_t+\sum_{j\in\mc{N}}\int_{\Delta_t}\int_{(0,1)}u^i_jC^i_j(x^i_t)\theta^j(y,x^j_t)\Pi^j(dy\times ds)\right)_{i\in\mc{N}}
\end{align}
where $h>0$ is a small discretization parameter corresponding to
$\Delta_t:=[t,t+h)$, $(z^i_t)_{t\in\mbb{Z}_+,i\in\mc{N}}$ is a collection of
$\iid$ standard Gaussian random variables, and $\Pi_t$ denotes the restriction
of the MPP $\Pi$ with intensity $\lambda>0$ and mark space $(0,1)$ to
$(0,1)\times\mc{N}\times\Delta_t$. For $i\in\mc{N}$, the thinning function
$\theta^i:(0,1)\times\mbb{X}\rightarrow\{0,1\}$ then defines the spiking process
for unit $i$ via 
\begin{align}
    Z^i_t=\int_{\Delta_t}\int_{(0,1)}\theta^i(y,x^i_t)\Pi^i(dy\times ds)=\int_{\Delta_t}\int_{(0,1)}\1{y\leq\frac{N}{\lambda}\Phi^i(x^i_t)}\Pi^i(dy\times ds)
\end{align}
where the excitation profile $\Phi^i:\mbb{X}\rightarrow \mbb{R}_+$ satisfies
$\Phi^i<\frac{\lambda}{N}$. At a minimum, the intrinsic dynamics
$f^i:\mbb{X}\rightarrow\mbb{X}$ should incorporate a leak term $-v^i_t$, and the
jump coefficient $C^i_i(x^i_t):=-v^i_t$ is fixed (so as to implement post-spike
resents) for $i\in\mc{N}$. Then, the action sequence
$(u_t)_{t\in\mbb{Z}_+}=(u^i_t)_{t\in\mbb{Z}_+,i\in\mc{N}}$ is determined
according to a policy $\gamma=(\gamma^i_t)_{t\in\mbb{Z}_+,i\in\mc{N}}$ so that
the mapping $\gamma^i_t:\mc{I}^i_t\rightarrow\mbb{U}^i$ is measurable, with
$(\mc{I}^i_t)_{t\in\mbb{Z}_+,i\in\mc{N}}$ either a completely decentralized
(\ref{eq58}) or mean field-sharing (\ref{eq59}) IS. With
$\Gamma=\prod_{i\in\mc{N}}\Gamma^i$ the space of all such policies, the network
seeks to infimize a cost function:
\begin{align}
    J^\ast(\gamma)=\inf_{\gamma\in\Gamma}J(\nu,\gamma)=\inf_{\gamma\in\Gamma}\E^\gamma_\nu\left(\mc{L}(x,u,\Pi)\right)
\end{align}
where the loss function $\mc{L}$ over the entire system trajectory can be
expressed in terms of a sequence of stagewise costs $(c_t)_{t\in\mbb{Z}_+}$.
Characteristically, $c_t:\mbb{R}^{N}_+\times\mbb{U}\rightarrow\mbb{R}_+$ are
assumed to depend on $x_t,\Pi_t$ only through $Z_t$ at each $t\in\mbb{Z}_+$ in
accordance with spike coding. Optionally, each $c_t$ might admit an exchangeable
decomposition of the form (\ref{eq64}) incorporating the empirical measure
$\mu[v_t]$. The most idiosyncratic features of this stochastic team problem
(relative to the broader literature in stochastic control) appear to be (i) the
state dependent spiking process, obtained from the original jump-diffusion
dynamics in continuous time (but see remark \ref{rem1}) and (ii) the fact that
the network topology itself is subject to control, at least in the
$\mc{G}$-maximal regime.\\[5pt] 
\indent As we remarked at the outset of this chapter, stochastic team models of
neuronal networks are an entirely new contribution, building upon a relatively
sparse literature viewing individual neurons as controllers. Optimal control has
been considered for single neurons previously, where the goal was to optimize a
stimulus to realize a precise spike time for a single unit
\cite{Iolov_Ditlevsen_Longtin_2014}. Our model builds on this as another
instance of an optimal control perspective. A more substantial point of
similarity is that spike times are considered to determine cost realizations,
and one of our main assertions is that this should remain the case -- at least
for finite networks of spiking neurons. In essence, the performance of the
network should be assessed in terms of its transmittable, or outgoing activity,
as suggested in the perspective article \cite{Jaeger_Noheda_vanderWiel_2023} on
computation in nonconventional physical systems. Then, rather than the intrinsic
conductance control models of \cite{Sepulchre_2022}, we regard extrinsic
synaptic weights (i.e., the density of synaptic membrane receptors between two
neurons) as being controlled. Membrane composition control -- encapsulating the
inclusion of either intrinsic or extrinsic maximal conductance parameters -- is
a biophysically principled perspective to adopt for controlled network models,
but there is no reason to prefer one kind over the other. That is, both
neuromodulatory and synaptic plasticity mechanisms are important for neuronal
function, and it is only that the CB setup needed for a clear neuromodulatory
interpretation of control renders the spiking process undefined. Further, this
point is not addressed in \cite{Burghi_Sepulchre_2023},\cite{Sepulchre_2022} so
our preference is for weight control in the neural jump-diffusion formulation,
especially since recent work has characterized a limiting infinite population
dynamics for them \cite{Jabin_Poyato_Soler_2021},\cite{Jabin_Schmutz_Zhou_2024}.
This and similarly relevant results on large scale decentralized stochastic
control  shall be described in the following chapter to outline some key future
directions for this research.

\chapter{Large Scale Decentralized Stochastic Control}\label{ch4}
Early examinations of sensory cortex conducted by Mountcastle
\cite{Mountcastle_1957} and Hubel and Wiesel \cite{Hubel_Wiesel_1962}
established a hypothesis of brain modularity following the observation that the
cerebral cortex is organized as a tiling of radially-redundant columns. These
columns share a common general structure and composition
\cite{Mountcastle_1997}, which inspired Wilson and Cowan
\cite{Wilson_Cowan_1973} to derive a mean-field model describing the flow of an
infinite population of neurons. Since then, neural field theory
\cite{Coombes_2014}, \cite[part III]{Gerstner_Kistler_Naud_Paninski_2014} has
established itself as a classical technique in neuroscientific modeling, with
modern applications seeking to provide mechanistic explanations of the kinds of
macroscale (electroencephalography, magnetoencephalography) observations
available to clinicians -- see \cite{Nunez_1974},
\cite{Sanz-Leon_Knock_Spiegler_Jirsa_2015}, for example. In decentralized
stochastic control, an identical tradition exists, where one seeks to
approximate the optimal control for a very large population of agents using the
solution to an infinite population analogue \cite{Bauerle_2023},
\cite{Sanjari_Saldi_Yüksel_2023}, \cite[ch. 6]{Yuksel_Basar_2024}. In both
fields, the limiting system is often obtained under some assumption of symmetry,
or exchangeability, where the agents are homogeneous in their local dynamics and
interact via an empirical distribution on the local state and action spaces.
However, our model insists that interactions obey a (controlled) network
topology, which induces a heterogeneity in the agent population. As a result,
the system is non-exchangeable, which precludes much of the existing theory. In
this final chapter, we turn our attention away from modeling, and examine some
recent literature from the theories of neural fields and many-agent
decentralized stochastic control which will undoubtedly guide the future
developments of this project. 
\section{Mean-Field Dynamics for Non-Exchangeable Networks}\label{sec2}
In a recent vein of literature, Jabin et al. \cite{Jabin_Poyato_Soler_2021},
\cite{Jabin_Zhou_2023}, have considered mean-field dynamics for non-exchangeable
networks of integrate-and-fire neuron models, which are almost identical to the
continuous time dynamical models presented earlier in \ref{eq33}. In particular,
for a network of $\mc{N}=\{1,2,\dots,N\}$ neurons, the local dynamics they
consider are given by
\begin{align}
    dx^{i,N}_t=f(x^{i,N}_t)dt+\frac{1}{N}\sum_{j=1}^N w^{i,N}_{j}\int_{\mbb{R}_+}\1{y\leq \Phi(x^{j,N}_t)}\Pi^{j,N}(dy\times dt)-x^{i,N}_{t-}\int_{\mbb{R}_+}\1{y\leq \Phi(x^{i,N}_t)}\Pi^{i,N}(dy\times dt)\label{eq67}
\end{align}
where the initial conditions
$(x^{i,N}_0)_{i\in\mc{N}}\sim\nu_0^N\in\mc{P}(\mbb{X})$ are independent of
$(\Pi^{i,N})_{i\in\mc{N}}$. The differences between this model and the
(pre-control, continuous time) jump-diffusion model of chapter \ref{chap1} are
that the local drifts $f$ and excitation functions $\Phi$ are identical between
units, and the jump coefficients $C^i_j\equiv 1$ for $i\neq j$, but
$C^i_i=-x^{i,N}_{t-}$ for $i\in\mc{N}$. This version omits a diffusive noise,
but see \cite{Jabin_Zhou_2023} where a similar result is obtained for a
jump-diffusion variant, albeit using more technical methods. Lastly, it is
assumed that both $f:\mbb{R}\rightarrow\mbb{R}$ and
$\Phi:\mbb{R}\rightarrow\mbb{R}_+$ are continuously differentiable.\\[5pt]
\indent One of the key theoretical tools employed to study the limiting dynamics
as $N\rightarrow\infty$ is an appropriate notion for the limiting network
topology, which is readily available in graphon theory \cite{Lovasz_2012}.
Graphons (a portmanteau of graph and function) were developed originally for the
study of limits of dense graph sequences, although convergence notions have been
developed for less-than-dense graphs as well (see \cite{Jabin_Zhou_2023},
\cite{Borgs_Chayes_Cohn_Zhao_2018}) and have further proven critical for some
recent advancements in the theory of graphon mean field games
\cite{Caines_Huang_2019}, \cite{Fabian_Cui_Koeppl_2023} (where they are also
employed to resolve infinite populations of heterogenously interacting agents).
\begin{definition}[Graphons and Digraphons \cite{Jabin_Poyato_Soler_2021}, $\S$ 2.1]
    A measurable function $W:[0,1]^2\rightarrow\mbb{R}$ is called a
    \textit{graphon} iff for any $u,v\in[0,1]$, both $W(u,v)=W(v,u)$ and $0\leq
    W(u,v)\leq 1$. If $W$ is not symmetric and only satisfies $-1\leq W(u,v)\leq
    1$, then $W$ is called a \textit{digraphon}. We shall denote the space of
    digraphons with $\mc{W}$.
\end{definition}
Provided that the directed, weighted network topology
$(w^{i,N}_j)_{i,j\in\mc{N}}$ for an $N$-unit network (\ref{eq67}) satisfies
$w^{i,N}_j\sim\mc{O}(1/N)$ (that is, $\exists M>0$, $N^\ast\geq 1$ so that
$|w^{i,N}_j|\leq M\frac{1}{N}$ for $N\geq N^\ast$) it can be associated to a
digraphon by first specifying a spatial embedding on the unit interval.
\begin{definition}[Almost Everywhere Partition \cite{Jabin_Poyato_Soler_2021}, $\S$ 1.5]\label{def28}
    The collection $(E^{i,N})_{i\in\mc{N}}$ is called an \textit{almost
    everywhere partition} of $[0,1]$ iff for $i\in\mc{N}$,
    $E^{i,N}\subset[0,1]$, $\mu(E^{i,N})=\frac{1}{N}$ and
    $\1{[0,1]}(x)=\sum_{i\in\mc{N}}\1{E^{i,N}}(x)$ for almost every $x\in[0,1]$.
\end{definition}
For such a partition $E^N=(E^{i,N})_{i\in\mc{N}}$, a digraphon associated to the
weight matrix $(w^{i,N}_j)_{i,j\in\mc{N}}$ is a function
$W^{E^N,N}:[0,1]^2\rightarrow[0,1]$ given by
\begin{align}
    W^{E^N,N}(u,v):=\sum_{i,j\in\mc{N}}w^{i,N}_j\1{E^{i,N}}(u)\1{E^{j,N}}(v).
\end{align}
Note that any such digraphon representation can be regarded as equivalent by
relating the partition $(E^{i,N})_{i\in\mc{N}}$ to another
$(\tilde{E}^{i,N})_{i\in\mc{N}}$ through an invertible, measure-preserving
transformation $\mc{T}:[0,1]\rightarrow[0,1]$ so that for any $i\in\mc{N}$,
$\mc{T}(E^{i,N})=\tilde{E}^{j,N}$ for some $j\in\mc{N}$. Additionally, Jabin et
al. in \cite{Jabin_Poyato_Soler_2021} remark that for any permutation
$\sigma\in\mc{S}(\mc{N}):=\{\sigma:\mc{N}\rightarrow\mc{N}|\,\sigma\text{ is
invertible }\}$, a trajectory $(X^{\sigma(i),N}_t)_{i\in\mc{N},t\geq 0}$ with
connectivity $(w^{\sigma(i),N}_{\sigma(j)})_{i,j\in\mc{N}}$ is a solution to
(\ref{eq67}) if and only if $(X^{i,N}_t)_{i\in\mc{N},t\geq 0}$ is with
$(w^{i,N}_j)_{i,j\in\mc{N}}$. Thus, the spatial embedding for the network can be
considered arbitrary, and the sense in which $W^{E^N,N}$ converges should
reflect this insensitivity. One appropriate choice for this is to bestow
$\mc{W}$ with the \textit{cut metric}.
\begin{definition}[Cut Metric
    {\cite[ch.8]{Lovasz_2012},\cite{Gao_Caines_2019}}]\label{def27} For any
    digraphon $W\in\mc{W}$, the \textit{cut norm}
    $\|\cdot\|_\square:\mc{W}\rightarrow\mbb{R}_+$ is given by
    \begin{align*}
        \|W\|_\square:=\sup_{A,B\subseteq [0,1]}\left|\int_{A\times B}W(x,y)dxdy\right|.
    \end{align*}
    In analogy to the set of permutations on finite discrete sets, define
    $\mc{S}_{[0,1]}$ to be the set of measure preserving bijections on $[0,1]$:
    \[\mc{S}_{[0,1]}:=\{\psi:[0,1]\rightarrow[0,1]:\mu(\psi^{-1}(A))=\mu(A),\;\forall
    A\in\mc{B}([0,1])\;\text{and $\psi$ is invertible}\}.\] Then for
    $W,V\in\mc{W}$, the \textit{cut metric} is given by
    \begin{align*}
        d_\square(W,V):= \inf_{\psi\in\mc{S}_{[0,1]}}\|W^\psi-V\|_{\square}
    \end{align*}
    where $W^\psi(x,y):=W(\psi(x),\psi(y))$ for all $\psi\in\mc{S}_{[0,1]}$,
    $x,y\in[0,1]$.
\end{definition}
In fact the cut metric is only a pseudometric (since, for example,
$d_\square(W,W^\psi)=0$ for any $\psi\in\mc{S}_{[0,1]}$) but is a true metric on
a corresponding set of equivalence classes, call it $\widetilde{\mc{W}}$.
Further, $(\widetilde{\mc{W}},d_\square)$ is compact \cite[\S5, theorem
5.1]{Lovász_Szegedy_2007} (see \cite[\S8.2]{Lovasz_2012} for further remarks on
this equivalence). We next restate the consequent lemma, used in \cite[\S2.1,
lemma 1]{Jabin_Schmutz_Zhou_2024} to obtain the limit dynamics for (\ref{eq67}).
\begin{theorem}[{\cite[\S2.1, lemma 2]{Jabin_Schmutz_Zhou_2024}}]\label{thm5}
    Let $w^N=(w^{i,N}_j)_{i,j\in\mc{N}}$ and $(w^N)_{N\geq 1}$ define a sequence
    of network topologies, and suppose these are uniformly bounded such that
    $\sup_{N\geq 1}\max_{1\leq i,j\leq N}|w^{i,N}_j|<\infty$. Then, there exists
    a subsequence $(w^{N_k})_{k\geq 1}$, a digraphon $W\in\widetilde{\mc{W}}$
    and a sequence of almost-everywhere partitions $(E^{N_k})_{k\geq 1}$ such
    that the digraphons
    \begin{align*}
        W^{E^{N_k},N_k}(u,v):=\sum_{1\leq i,j\leq N_k}w^{i,N_k}_j\1{E^{i,N_k}}(u)\1{E^{j,N_k}}(v)
    \end{align*}
    satisfy $d_\square(W^{E^{N_k},N_k},W)\rightarrow 0$ as $k\rightarrow\infty$.
\end{theorem}
In particular, the limit in this case does not depend on the precise topologies
of the network sequence, only on their $\mc{O}(1/N)$ scaling behavior as $N$
grows large. This discussion is sufficient as a conceptual basis for presenting
the main result of \cite{Jabin_Schmutz_Zhou_2024}, which characterizes the
limiting dynamics for neural networks of the form (\ref{eq67}). That is, that
the so-called \textit{extended empirical measures} given by
\begin{align}
    \mu^N_t(v,dx):=\sum_{j\in\mc{N}}\delta_{x^{j,N}_t}(dx)\1{E^{j,N}}(v),\quad N\geq 1,\;t\geq 0,\;v\in[0,1]\label{eq71}
\end{align}
converge in some sense (see \cite[\S2.2]{Jabin_Schmutz_Zhou_2024}) to the
solution of the mean-field partial differential equation given by
\begin{align}
    \begin{cases}
        &\partial_t\mu_t(v,dx)+\partial_x\left((f(x)+h_t(v))\mu_t(v,dx)\right)+\Phi(x)\mu_t(v,dx)-r_t(v)\delta_0(dx)=0\\
        &r_t(v):=\int_\mbb{R}\Phi(x)\mu_t(v,dx)\\
        &h_t(v):=\int_{[0,1]}W(v,u)r_t(u)du\\
        &\mu_0(v,dx)=\nu_0(v,dx)
    \end{cases}\label{eq92}
\end{align}
where $W\in\widetilde{\mc{W}}$ is the digraphon limit obtained under the
boundedness assumption in theorem \ref{thm5}. The function-valued process
$r_t:[0,1]\rightarrow\mbb{R}_+$ specifies the typical rate of a unit according
to its location on $[0,1]$. In the same manner, $h_t:[0,1]\rightarrow\mbb{R}$
specifies the postsynaptic input received by a typical unit.\\[5pt]
\indent Aside from the relatively minor differences between this model and its
jump-diffusion variant, an important topic of subsequent research will be to
apply a similar graphon framework to the controlled, discrete time applications
we have in mind. At the forefront of this endeavor is to determine implications for the structure of an optimal weight control policy; is it possible to constrain the space of policies without loss as the population
grows large? This question will likely require more refined notions for graph convergence
(i.e., permitting nontrivial limits for sparse graph sequences. See \cite{Borgs_Chayes_Cohn_Zhao_2018} from the graphon literature, \cite{Caines_Huang_2024},\cite{Fabian_Cui_Koeppl_2023} for
recent results in the setting of control, and \cite{Jabin_Zhou_2023} for limits of sparse neural jump-diffusions) so as not to
discount the possible importance of network sparsity, which is a well-appreciated feature of neuronal networks \cite{Betzel_Bassett_2017}. 
\section{Optimal Control of Exchangeable Stochastic Teams}\label{sec3}
This final section highlights some recent work characterizing the structure of
optimal policies for finite and infinite stochastic teams, specifically that of
\cite{Sanjari_Saldi_Yüksel_2024}. It turns out that for symmetric teams, where
agents are sufficiently homogeneous in in their dynamics, the optimal solution
is approximable by policies utilizing a decentralized, mean-field sharing IS
(such as was proposed earlier in \ref{eq59}) provided the team is sufficiently
large \cite[theorem 4]{Sanjari_Saldi_Yüksel_2024}. Further, these policies are
symmetric -- each agent uses the same policy, and these are conditionally
independent such that the joint policy is rendered permutation invariant. In the
context of weight control, and considering the ideas outlined in \ref{sec2}
utilizing graphon theory, this invariance could permit a similar program for
establishing convergence to an infinite network, but now with a
\textit{controlled} graphon topology. It is in the spirit of this possible
direction that we review the setup and results of
\cite{Sanjari_Saldi_Yüksel_2024}.
\subsection{Problem Description and Comparison with Neural Teams}\label{subsec1}
\indent To begin, let us specify the form of the problem and assumptions imposed
in \cite{Bauerle_2023}, \cite{Sanjari_Saldi_Yüksel_2024} and contrast these with
neural teams. For a collection of $N$ DMs indexed by $\mc{N}$, the state
dynamics are to be given by
\begin{align}
    x^i_{t+1}=f(x^i_t,u^i_t,\mu^N_t,z^i_t),\quad i\in\mc{N},\;t\geq 0,\label{eq75}
\end{align}
where $\mu^N_t:=\mu[x_t]\in\mc{P}^N(\mbb{X})$ is the state empirical measure
$\mu[x_t]=\frac{1}{N}\sum_{j\in\mc{N}}\delta_{x^j_t}$ (see definition
\ref{def25}) and $(z^i_t)_{t\geq 0}$ is an $\iid$ noise process independent of
the initial data $x^i_0\sim\nu_0$ for each $i\in\mc{N}$. The state and action
spaces associcated to each DM are identical and denoted with $\mbb{X}$,
$\mbb{U}$ respectively. Such dynamics are called
\textit{symmetric} to allude to the fact that all local states evolve under the
same $f$, which determines a single stochastic kernel $\mc{T}$ (see \ref{eq68})
such that
\begin{align}
   P(x_{t+1}\in A|x_t,u_t,\mu^N_t)=\prod_{i\in\mc{N}}\mc{T}(A^i|x^i_t,u^i_t,\mu^N_t)\label{eq80} 
\end{align}
for $A=\prod_{i\in\mc{N}}A^i\in\mc{B}(\mbb{X}^N)$ and $A^i\in\mc{B}(\mbb{X})$
for $i\in\mc{N}$. We denote the form of $\mc{T}$ with the membership
$\mc{T}\in\mc{P}(\mbb{X}|\mbb{X}\times\mbb{U}\times\mc{P}(\mbb{X}))$, and equip
this (as with all spaces of probability measures considered) with the topology
of weak convergence.\\[5pt]
\indent The information structures considered in
\cite{Sanjari_Saldi_Yüksel_2024} are centralized
\begin{align}
    \mc{I}^{i,CEN}_t:=\begin{cases}
        \{x_{0:t},u_{0:t-1}\},\quad&\text{$t\geq 1$}\\
        \{x_0\},\quad&\text{$t=0$}
    \end{cases},\quad i\in\mc{N}\label{eq69}
\end{align}
and (decentralized) mean-field sharing 
\begin{align}
    \mc{I}^{i,DEC}_t:=\begin{cases}
        \{x^i_{0:t},u^i_{0:t-1},\mu^N_{0:t}\},\quad&\text{$t\geq 1$}\\
        \{x^i_0,\mu^N_0\},\quad&\text{$t=0$}
    \end{cases},\quad i\in\mc{N}.\label{eq70}
\end{align}
A Policy can then be viewed as a sequence of stochastic kernels
$\gamma^i_t\in\mc{P}(\mbb{U}|\mc{I}^i_t)$, where $\mc{I}^i_t$ is given by one of
(\ref{eq69}), (\ref{eq70}) and $u^i_t\sim\gamma^i_t(\cdot|\mc{I}^i_t)$ for
$i\in\mc{N}$, $t\geq 0$. Note that a measurable, deterministic policy
$\gamma^i_t:\mc{I}^i_t\rightarrow\mbb{U}$ can be realized by a singular kernel
$\tilde{\gamma}^i_t\in\mc{P}(\mbb{U}|\mc{I}^i_t)$ by setting
$\tilde{\gamma}^i_t(A|\mc{I}^i_t):=\1{\gamma^i_t(\mc{I}^i_t)\in A}$ for
$A\in\mc{B}(\mbb{U})$. Both finite and infinite horizon costs are considered in
\cite{Sanjari_Saldi_Yüksel_2024}, but we shall focus on the former, setting a
finite time horizon $T\in\mbb{Z}_+$. With a measurable, stagewise, single-agent
cost function $c:\mbb{X}\times\mbb{U}\times\mc{P}(\mbb{X})\rightarrow\mbb{R}_+$,
the finite horizon discounted cost is given by
\begin{align}
    J^N_T(\gamma)=\mbb{E}^{\gamma}_{\nu_0}\left(\frac{1}{N}\sum_{i=1}^N\sum_{t=0}^{T-1}\beta^t c(x^i_t,u^i_t,\mu^N_t)\right)
\end{align}
where the discount factor $\beta\in(0,1)$ and
$\gamma=(\gamma^i_{0:T-1})_{i\in\mc{N}}$. This cost is said to be
\textit{exchangeable}, since for any permutation $\sigma\in\mc{S}(\mc{N})$, the
cost over the sample path
$\bs{c}(x_{0:T},u_{0:T},\mu^N_{0:T}):=\frac{1}{N}\sum_{i=1}^N\sum_{t=0}^{T-1}\beta^tc(x^i_t,u^i_t,\mu^N_t)$
satisfies
\begin{align}
    \bs{c}(x^\sigma_{0:T},u^\sigma_{0:T},\mu^N_t)=\bs{c}(x_{0:T},u_{0:T},\mu^N_{0:T})\label{eq74}
\end{align}
where
$x^\sigma_{0:t}:=(x^{\sigma(1)}_{0:t},x^{\sigma(2)}_{0:t},\dots,x^{\sigma(N)}_{0:t})$
for any $t\geq 0$. For an infinite population, where $\mc{N}=\mbb{N}$, a joint
policy $\gamma=(\gamma^i_{0:T-1})_{i=1}^\infty$ induces the discounted cost
given by
\begin{align}
    J_T(\gamma)=\limsup_{N\rightarrow\infty}\mbb{E}^\gamma_{\nu_0}\left(\frac{1}{N}\sum_{i=1}^N\sum_{t=0}^{T-1}\beta^t c(x^i_t,u^i_t,\mu^N_t)\right).
\end{align}
In either case, policies $\gamma^{\ast,N}$, $\gamma^\ast$ are globally optimal
for their respective problems if and only if
\begin{align}
    J^N_T(\gamma^{\ast,N})=\inf_\gamma J^N_T(\gamma),\quad\text{or}\quad J_T(\gamma^\ast)=\inf_\gamma J_T(\gamma)
\end{align}
where the infima are taken over some to-be-defined spaces of admissable
policies. Team problems with symmetric dynamics (\ref{eq75}) and exchangeable costs are themselves called exchangeable (i.e., an \textit{exchangeable team}). Only three assumptions are imposed on this setup:
\begin{enumerate}[(i)]
    \item The state $\mbb{X}$ and action $\mbb{U}$ spaces are compact.
    \item The stagewise cost
    $c:\mbb{X}\times\mbb{U}\times\mc{P}(\mbb{X})\rightarrow\mbb{R}_+$ is jointly
    continuous and bounded.
    \item The dynamics
    $f(\cdot,\cdot,\cdot,z):\mbb{X}\times\mbb{U}\times\mc{P}(\mbb{X})\rightarrow\mbb{X}$
    are jointly continuous for all disturbances $z$.
\end{enumerate}

\indent Let us now remark on the specific incompatibilities of neural team
problems with the finite population setup. For simplicity, we assume that the
state consists only of the voltage and that the intrinsic dynamics (drift and
excitation functions) are symmetric: for all $i\in\mc{N}$,
\begin{align}
    v^{i,N}_{t+1}=v^{i,N}_t+f(v^{i,N}_t)+\sigma\sqrt{h}z^i_t+\frac{1}{N}\sum_{j\in\mc{N}\setminus\{i\}}u^{i,N}_j&\left(\int_{\Delta_t}\int_{(0,1)}\1{y\leq \Phi(v^{j,N}_t)}\Pi^{j,N}(dy\times ds)\right)\notag\\
    &-v^{i,N}_t\min\left(1,\int_{\Delta_t}\int_{(0,1)}\1{y\leq\Phi(v^{i,N}_t)}\Pi^{i,N}(dy\times ds)\right),\label{eq72}
\end{align}
where $\Pi^{j,N}$ is a standard marked Poisson process with intensity
$\lambda^N\geq\sup_{x\in\mbb{X}}N\Phi(x)$ and mark space $(0,1)$. Reducing the
state to just the voltage has the benefit of making the proposed mean-field
sharing information structure (\ref{eq59}) coincide with $\mc{I}^{i,DEC}_t$ in
(\ref{eq70}). However, two incongruencies are immediately apparent. First, the
thinning function is discontinuous in the state, such that both the dynamics and
stagewise costs (\ref{eq64}) are discontinuous and we fail to satisfy
assumptions (ii) and (iii). Second, the network interactions cannot be expressed
as a function of the empirical distribution due to the DM identity dependence of
$\Pi^j$, $u^i_j$ for $j\in\mc{N}$.\\[5pt]
\indent Some adjustments are needed to remediate these issues. On the more
technical side, it may be feasible to approximate the spiking process with a
thinning formulation which is continuous in the state (see remark \ref{rem1}).
Further, the requirement that $\lambda^N\rightarrow\infty$ as
$N\rightarrow\infty$ means that the driving process for an infinite population
cannot be represented by an MPP, necessitating a spatial thinning variant (where
the new "mark space" $\mbb{R}_+$, is no longer finite). This form of thinning is
less common in the literature on jump-diffusions, which is why we switched to
the MPP thinning of proposition \ref{prop4}. More substantively, in light of
\cite{Jabin_Schmutz_Zhou_2024}, we could attempt to extend the structural
results in \cite{Sanjari_Saldi_Yüksel_2024} to non-exchangeable systems by
considering interactions which may be expressed through an extended empirical
measure $\tilde{\mu}^N_t$ (\ref{eq71}) associated to an almost everywhere
partition $E^N=(E^{i,N})_{i\in\mc{N}}$. Let us briefly explore how this might be
done for (\ref{eq72}). For fixed $N\geq 1$, define the (local) graphon control
and a spatial embedding for the driving Poisson process:
\begin{align}
    \widetilde{u}^{i,N}(\xi):=\sum_{j=1}^Nu^{i,N}_j\1{E^{j,N}}(\xi),\quad \widetilde{\Pi}^{N}(\xi,dy\times ds):=\sum_{j=1}^N\Pi^{j,N}(dy\times ds)\1{E^{j,N}}(\xi)
\end{align}
where $\xi\in[0,1]$ specifies a spatial location. Then, given
$u^{i,N}_j\in\mbb{U}=[0,\overline{w}]$ for $i\in\mc{N}$, the joint control
$u^N=(u^{i,N})_{i\in\mc{N}}$ defines a digraphon $\widetilde{u}^N$ given by
\begin{align}
    \widetilde{u}^N(\xi,\zeta):=\sum_{i=1}^N\widetilde{u}^{i,N}(\xi)\1{E^{i,N}}(\zeta)=\sum_{1\leq i,j\leq N}u^{i,N}_j\1{E^{j,N}}(\xi)\1{E^{i,N}}(\zeta)
\end{align}
where $\xi,\zeta\in[0,1]$. We also need to separate the influence of local
spiking from presynaptic spiking which may be accomplished by defining an
interaction function:
\begin{align}
    \Xi^{i,N}(v^{i,N}_t,u^{i,N}_t,\xi,\widetilde{\Pi}^N,v):=\sum_{\ell=1}^N\Xi^{i,N}_\ell(v^{i,N}_t,u^{i,N}_t,\xi,\widetilde{\Pi}^N,v)\1{E^{\ell,N}}(\xi)
\end{align}
where 
\begin{align}
    \Xi^{i,N}_\ell(v^{i,N}_t,u^{i,N}_t,\xi,\widetilde{\Pi}^N,v):=\begin{cases}
        \left(\int_{\Delta_t}\int_{(0,1)}\1{y\leq\Phi(v)}\widetilde{\Pi}^N(\xi,dy\times ds)\right),\quad& i\neq\ell\\
        -N\left(\frac{v^{i,N}_t}{u^{i,N}_i}\right)\min\left(1,\int_{\Delta_t}\int_{(0,1)}\1{y\leq\Phi(v^{i,N}_t)}\widetilde{\Pi}^N(\xi,dy\times ds)\right),\quad& i=\ell.
    \end{cases}\label{eq73}
\end{align}
With these objects, the spike dependence of (\ref{eq72}) may be expressed in
terms of the extended empirical measure
$\widetilde{\mu}^N_t(\xi,dx)=\sum_{j=1}^N\delta_{x^{j,N}_t}(dx)\1{E^{j,N}}(\xi)$
as follows:
\begin{alignat*}{2}
    &\int_0^1\int_\mbb{V}\widetilde{u}^{i,N}(\xi)\Xi^{i,N}(x^{i,N}_t,u^{i,N}_t,\xi,\widetilde{\Pi}^N,v)\widetilde{\mu}^N_t(\xi,dv)d\xi\\
    &=\int_0^1\int_\mbb{V}\widetilde{u}^{i,N}(\xi)\Xi^{i,N}(x^{i,N}_t,u^{i,N}_t,\xi,\widetilde{\Pi}^N,v)\left(\sum_{j=1}^N\delta_{v^{j,N}_t}(dv)\1{E^{j,N}}(\xi)\right)d\xi\\
    &=\sum_{j=1}^N\int_0^1\left(\sum_{k=1}^N
    u^{i,N}_k\1{E^{k,N}}(\xi)\right)\left(\sum_{\ell=1}^N\Xi^{i,N}_\ell(v^{i,N}_t,u^{i,N}_t,\xi,\widetilde{\Pi}^N,v^{j,N}_t)\1{E^{\ell,N}}(\xi)\right)\1{E^{j,N}}(\xi)d\xi\\
    &=\sum_{j=1}^N\int_{E^{j,N}}u^{i,N}_j\Xi^{i,N}_j(v^{i,N}_t,u^{i,N}_t,\xi,\widetilde{\Pi}^N,v^{j,N}_t)d\xi\\
    &=\frac{1}{N}\sum_{j\in\mc{N}\setminus\{i\}}u^{i,N}_j\left(\int_{\Delta_t}\int_{(0,1)}\1{y\leq
    \Phi(v^{j,N}_t)}\Pi^{j,N}(dy\times ds)\right)\\
    &\qquad\qquad\qquad-\frac{1}{N}u^{i,N}_i\left(N\left(\frac{v^{i,N}_t}{u^{i,N}_i}\right)\min\left(1,\int_{\Delta_t}\int_{(0,1)}\1{y\leq\Phi(v^{i,N}_t)}\Pi^{i,N}(dy\times
    ds)\right)\right)
\end{alignat*}
where this final expression is precisely that of (\ref{eq72}), and the
appearance of $\frac{1}{N}$ results from $\int_{E^{j,N}}d\xi=\frac{1}{N}$ since
$\Xi^{i,N}$ is constant with varying $\xi\in E^{j,N}$ for $j\in\mc{N}$. Even
still, the post-spike resets induce a dynamical heterogeneity, seen from
(\ref{eq73}) by the fact that $\Xi^{i,N}\neq\Xi^{j,N}$ if $i\neq j$. Overcoming
this will be the subject of subsequent research, and will likely draw further
inspiration from \cite{Jabin_Zhou_2023},\cite{Jabin_Schmutz_Zhou_2024}.
\subsection{Exchangeable and Symmetric Policies for Finite and Infinite Teams}
To begin, lets review the classes of policies studied by
\cite{Sanjari_Saldi_Yüksel_2024} for finite symmetric teams. For the centralized
information structure (\ref{eq69}), an admissable joint policy
$\gamma^N=(\gamma^N_t)_{t=0}^{T-1}$ where
$\gamma^N_t=(\gamma^{i,N}_t)_{i\in\mc{N}}$ is a set of stochastic kernels such
that
\begin{align}
    \gamma^N_t\in\mc{P}\left(\mbb{U}^N\,\bigg|\,\prod_{k=0}^t(\mbb{X}^N\times\mc{P}(\mbb{X}))\right),\quad 0\leq t\leq T-1.
\end{align}
The centralized history process supporting these is given by
$h_t=(x_{0:t},\mu^N_{0:t})$, which is permissable under centralized information
since both $(u_t)_{t\geq 0}$ and $(\mu^N_t)_{t\geq 0}$ are adapted to the team
state process. Denote the set of such $\gamma^N$ with $\Gamma^N$. The sets of
Markovian and stationary subsets are subsequently defined
\begin{align}
    \Gamma^N_M&:=\left\{\gamma^N\in\Gamma^N:\gamma^N_t(\cdot|h_t)=\gamma^N_t(\cdot|x_t,\mu^N_t),\quad\forall h_t\in\prod_{k=0}^t(\mbb{X}^N\times\mc{P}(\mbb{X}))\right\}
\end{align}
and
\begin{align}
    \Gamma^N_S&:=\left\{\gamma^N\in\Gamma^N_M:\gamma^N_t=\gamma\;\text{for some $\gamma\in\mc{P}(\mbb{U}|\mbb{X}^N\times\mc{P}(\mbb{X}))$},\;\forall t\geq 0\right\}
\end{align}
respectively. The main idea is that under any policy in $\Gamma^N$, agents
select their policies randomly, but there is no restriction on how these
selections are allowed to covary. By constraining the permissible mechanisms of policy randomization, various classes
of structured team policies can be considered. 
\begin{definition}[Exchangeable Policies]\label{def29}
    A policy $\gamma^N\in\Gamma^N$ is called \textit{exchangeable} if for any $0\leq t\leq T-1$, $\sigma\in\mc{S}(\mc{N})$ and centralized history $h_t=(x_{0:t},\mu^N_{0:t})$, 
    we have
    \begin{align*}
        \gamma^{N}_t(u^\sigma_t\in\cdot|h^\sigma_t)=\gamma^N_t(u_t\in\cdot|h_t)
    \end{align*}
    where $h^\sigma_t=(x^\sigma_{0:t},\mu^N_{0:t})$. The set of exchangeable policies is denoted $\Gamma^N_{EX}$, while the sets Markovian and stationary exchangeable policies
    are denoted $\Gamma^N_{EX,M}$ and $\Gamma^N_{EX,S}$, respectively.
\end{definition}
Exchangeable policies can be regarded as those where the conditional distribution of the joint action
is invariant under permutations of the identities of the DMs. While exchangeable policies utilize centralized information,
in general, \cite{Sanjari_Saldi_Yüksel_2024} define two subsets which are decentralized, using the mean-field information structure
(\ref{eq70}) and which are later shown to be at least near-optimal for symmetric teams operating under an exchangeable cost criterion.
\begin{definition}[Symmetric Policy Classes]\label{def31}
    A randomized joint policy $\gamma^N\in\Gamma^N$ is said to be \textit{conditionally independent symmetric} (or \textit{symmetric with common randomness}) if for all $0\leq t\leq T-1$ we have
    \begin{align}
        \gamma^N_t(u_t\in \cdot|h_t)=\int_0^1\prod_{j=1}^N\widetilde{\gamma}_t^N(u^j_t\in \cdot|h^j_t,z)\eta_t(dz)\label{eq76}
    \end{align}
    where $\eta_t\in\mc{P}([0,1])$ is a \textit{common randomness} independent of any exogenous noise, $h^i_t:=(x^i_{0:t},\mu^N_{0:t})$ is the decentralized
    history variable available to unit $i\in\mc{N}$ at time $t$, and
    \[\widetilde{\gamma}^N_t\in\mc{P}\left(\mbb{U}\;\bigg|\left(\prod_{k=0}^t\mbb{X}\times\mc{P}(\mbb{X})\right)\times[0,1]\right)\]
    is a stochastic kernel common amongst agents generating (conditionally independent) local actions. The set of such policies is denoted $\Gamma^N_{\mathit{CO},\mathit{SYM}}$.
    If instead we had
    \begin{align}
        \gamma^N_t(u_t\in\cdot|h_t)=\prod_{j=1}^N\widetilde{\gamma}^N_t(u^j_t\in\cdot|h^j_t)\label{eq77}
    \end{align}
    such that the shared local kernel $\widetilde{\gamma}^N_t$ is conditionally independent of any noise given the local history, then $\gamma^N$ is called \textit{independent symmetric} (or
    \textit{symmetric with private randomness}). The set of such policies shall be denoted with $\Gamma^N_{PR,\mathit{SYM}}$
\end{definition}
It is apparent from definition $\Gamma^N_{PR,\mathit{SYM}}\subseteq\Gamma^N_{CO,\mathit{SYM}}\subseteq\Gamma^N_{EX}$. An important consequence of symmetric policies is that the decentralized information structure $\mc{I}^{1:N,\mathit{DEC}}_{0:T-1}$
is sufficient to propagate $(x^i_t,\mu^N_t)$ forward in time. That is, by adopting a policy $\gamma^N\in\Gamma^N_{PR,\mathit{SYM}}$ or $\gamma^N\in\Gamma^N_{CO,\mathit{SYM}}$, $\{(x^i_t,\mu^N_t),u^i_t\}_{0\leq t\leq T}$ is a controlled Markov chain for all $i\in\mc{N}$.
The proof of this fact is quite instructive, and so shall be presented fully. 
\begin{lemma}[{\cite[lemma 1]{Sanjari_Saldi_Yüksel_2024}}]\label{lem2}
    For an exchangeable team with a joint randomized policy $\gamma^N\in\Gamma^N_{PR,\mathit{SYM}}$, we have
    \begin{align*}
        P((x^i_{t+1},\mu^N_{t+1})\in\cdot|x_{0:t},u_{0:t})=P((x^i_{t+1},\mu^N_{t+1})\in\cdot|x^i_t,u^i_t,\mu^N_t),\quad i\in\mc{N}.
    \end{align*} 
    If instead $\gamma^N\in\Gamma^N_{CO,\mathit{SYM}}$, then we have
    \begin{align*}
        P((x^i_{t+1},\mu^N_{t+1})\in\cdot|x_{0:t},u_{0:t},z)=P((x^i_{t+1},\mu^N_{t+1})\in\cdot|x^i_t,u^i_t,\mu^N_t,z),\quad i\in\mc{N}.
    \end{align*} 
    where $z$ is a $[0,1]$-valued common randomness.
\end{lemma}
\noindent\textbf{Proof.} We will show the result for the latter case where $\gamma^N\in\Gamma^N_{CO,\mathit{SYM}}$ such that (\ref{eq76}) holds for all
$t$, history variables $h_t$ and some $\tilde{\gamma}^N_t$ with noise distribution $\eta_t$. Following the original proof found in \cite[appendix B, lemma 1]{Sanjari_Saldi_Yüksel_2024}, our goal is to show the following chain of equalities:
\begin{align}
    P((x^i_{t+1},\mu^N_{t+1})\in\cdot|x^i_t,\mu^N_t,\widetilde{\gamma}^N_t,z)&=P((x^i_{t+1},\mu^N_{t+1})\in\cdot|x^i_t,\mu^N_t,\theta^N_t,\widetilde{\gamma}^N_t,z)\label{eq78}\\
    &=P((x^i_{t+1},\mu^N_{t+1})\in\cdot|x^i_t,\mu^N_t,\theta^N_t,x^{-i}_t,u_t,\widetilde{\gamma}^N_t,z)\label{eq82}\\
    &=P((x^i_{t+1},\mu^N_{t+1})\in\cdot|\mu^N_{0:t},\theta^N_{0:t},x_{0:t},u_{0:t},\widetilde{\gamma}^N_t,z)\label{eq83}\\
    &=P((x^i_{t+1},\mu^N_{t+1})\in\cdot|x_{0:t},u_{0:t},z)\label{eq84}
\end{align}
where $\theta^N_t=\mu[(x_t,u_t)]$. First, for $\mu\in\mc{P}(\mbb{X})$ define $\mc{M}(\mu):=\{x\in\mbb{X}^N:\mu[x]=\mu\}$. This induces a partition on $\mbb{X}^N$, where for any $x_1,x_2\in\mbb{X}^N$, $\mu[x_1]=\mu[x_2]$ if and only if $\exists\sigma\in\mc{S}(\mc{N})$ such that $x^\sigma_1=x_2$,
and we might say that $x_1$ and $x_2$ are \textit{empirically equivalent} to denote this relation.\\[5pt]
\indent At any rate, given $\mu^N_t\in\mc{P}(\mbb{X})$ and any $x_t\in M(\mu^N_t)$, we can find a
representative $\widetilde{x}_t\in\mbb{X}^N$ and $\sigma_x\in\mc{S}(\mc{N})$ so that $x_t=\widetilde{x}_t^{\sigma_x}$. Fixing the history process $h_k=(x_{0:k},\mu^N_{0:k})$ for $k=0,1,\dots,t$ arbitrarily, we associate $h^{\sigma_x}_{0:t}$ to $x_t$, $h_{0:t}$ to $\widetilde{x}_{t}$
and generate $u^i_t\sim\widetilde{\gamma}^N_t(\cdot|h^{\sigma_x(i)}_t,z)$ so that $\widetilde{u}^i_t:=u^{\sigma_x^{-1}(i)}_t\sim\widetilde{\gamma}^N_t(\cdot|h^i_t,z)$ for $i\in\mc{N}$. Then,
\begin{align*}
    \frac{1}{N}\sum_{i=1}^N\delta_{(x^i_t,u^i_t)}=\frac{1}{N}\sum_{i=1}^N\delta_{(\widetilde{x}^{\sigma_x(i)}_t,u^{i}_t)}=\frac{1}{N}\sum_{i=1}^N\delta_{(\widetilde{x}^i_t,\widetilde{u}^i_t)}=:\theta^N_t.
\end{align*}
That is, $\gamma^N\in\Gamma^{N}_{CO,\mathit{SYM}}$ has allowed us to associate to any realization of $\{(x_t,u_t,\mu^N_t)\}_{0:t}$ a fixed one $\{(\widetilde{x}_t,\widetilde{u}_t,\mu^N_t)\}_{0:t}$ using the same policy so that
$\mu[(x_t,u_t)]=\mu[(\widetilde{x}_t,\widetilde{u}_t)]=\theta^N_t$. Defining $\mc{Q}(\theta^N_t):=\{u\in\mbb{U}^N:\mu[(x,u)]=\theta^N_t\;\text{for some}\;x\in\mbb{X}^N\}$, we may alternatively say that $u_t\in\mc{Q}(\theta^N_t)$ --
all possible actions are empirically equivalent, implying (\ref{eq78}). Now, for any $\sigma\in\mc{S}(\mc{N})$,
\begin{align}
    P(x^i_{t+1}\in\cdot|x^\sigma_t,u^\sigma_t,\mu^N_t,\widetilde{\gamma}^N_t,z)&=P(x^i_{t+1}\in\cdot|x^{\sigma(i)}_t,u^{\sigma(i)}_t,\mu^N_t,\widetilde{\gamma}^N_t,z)\notag\\
    &=P(x^{\sigma(i)}_{t+1}\in\cdot|x^{\sigma(i)}_t,u^{\sigma(i)}_t,\mu^N_t,\widetilde{\gamma}^N_t,z)\label{eq79}\\
    &=P(x^{\sigma(i)}_{t+1}\in\cdot|x_t,u_t,\mu^N_t,\widetilde{\gamma}^N_t,z)\label{eq81}
\end{align}
where (\ref{eq79}) is a consequence of symmetric dynamics (\ref{eq80}). With this fact we obtain
\begin{align*}
    P(x_{t+1}\in\cdot|x^\sigma_t,u^\sigma_t,\mu^N_t,\widetilde{\gamma}^N_t,z)&=\prod_{i=1}^NP(x^i_{t+1}\in\cdot|x^\sigma_t,u^\sigma_t,\mu^N_t,\widetilde{\gamma}^N_t,z)\\
    &=\prod_{i=1}^NP(x^{\sigma(i)}_{t+1}\in\cdot|x_t,u_t,\mu^N_t,\widetilde{\gamma}^N_t,z)\\
    &=P(x^\sigma_{t+1}\in\cdot|x_t,u_t,\mu^N_t,\widetilde{\gamma}^N_t,z)
\end{align*}
and the second equality follows from (\ref{eq81}). In particular, since $x_{t+1}$ and $x^\sigma_{t+1}$ are empirically equivalent,
we have
\[P(\mu^N_{t+1}\in\cdot|x_t,u_t,\mu^N_t,\widetilde{\gamma}^N_t,z)=P(\mu^N_{t+1}\in\cdot|x^\sigma_t,u^\sigma_t,\mu^N_t,\widetilde{\gamma}^N_t,z),\forall \sigma\in\mc{S}(\mc{N})\]
or equivalently, $P(\mu^N_{t+1}\in\cdot|x_t,u_t,\mu^N_t,\widetilde{\gamma}^N_t,z)$ is invariant for all $x_t\in\mc{M}(\mu^N_t)$, $u_t\in\mc{Q}(\theta^N_t)$. Of course, for any $i\in\mc{N}$ it is
also the case that $x^i_{t+1}$ is independent of $x^{-i}_t$, $u_t$ given $x^i_t$, $\mu^N_t$ and $\widetilde{\gamma}^N_t$, and these two facts together permit us to condition on $x^{-i}_t$, $u_t$ in (\ref{eq82}).
Subsequently, (\ref{eq83}) holds since $x_{t+1}$ is independent of $\mu^N_{0:t-1}$, $\theta^N_{0:t-1}$, $x_{0:t-1}$ and $u_{0:t-1}$ given $x_t$, $u_t$, and (\ref{eq84}) holds since $x_{0:t}$, $u_{0:t}$ determine $\mu^N_{0:t}$, $\theta^N_{0:t}$, respectively,
and $x_{t+1}$ is independent of $\widetilde{\gamma}^N_t$ given $u_t$.\hfill{$\qed$}\\[5pt]
\indent The main effect of policy symmetry in the above proof is encapsulated by (\ref{eq82}), permitting DMs to
infer the state-action empirical measure $\theta^N_t$ using the decentralized information structure.\\[5pt]
\indent We now proceed to classes of policies for infinite teams, setting $\mc{N}=\mbb{N}$ \cite[\S3.2]{Sanjari_Saldi_Yüksel_2024}.
For the centralized information structure (\ref{eq69}), an admissable, jointly randomized policy $\gamma=(\gamma_t)_{0\leq t\leq T-1}$ where $\gamma_t=(\gamma^i_t)_{i=1}^\infty$ for $0\leq t\leq T-1$ is
a set of stochastic kernels such that
\begin{align}
    \gamma_t\in\mc{P}\left(\mbb{U}^\infty\bigg|\prod_{k=0}^t\mbb{X}^\infty\right)
\end{align}
where $\mbb{K}^\infty:=\prod_{i=1}^\infty\mbb{K}$ for $\mbb{K}\in\{\mbb{X},\mbb{U}\}$. In this case, the history process is given by $h^\infty_t:=x^\infty_{0:t}$, where
we now distinguish $x^\infty_t=(x^i_t)_{i\in\mc{N}}$ for $0\leq t\leq T-1$. Likewise, a Markovian policy is one where $\gamma_t(u^\infty_t\in\cdot|h^\infty_t)=\gamma_t(u^\infty_t\in\cdot|x^\infty_t)$ for $0\leq t\leq T-1$,
and this policy is further said to be stationary if it is time invariant. The sets of these are denoted $\Gamma_M$, $\Gamma_S$, respectively. We next present infinite population analogues of definitions \ref{def29} and \ref{def31}.
\begin{definition}[Infinitely-Exchangeable Policies]\label{def32}
    A policy $\gamma\in\Gamma$ is called \textit{infinitely-exchangeable} if for any $N\geq 1$, and $\mc{S}(\{1,2,\dots,N\})$ we have
    \begin{align*}
        \gamma_t(u^{\sigma,\infty}_t\in\cdot|h^{\sigma,\infty}_t)=\gamma_t(u^\infty_t\in\cdot|h^\infty_t),\quad 0\leq t\leq T-1
    \end{align*}
    where $h^{\sigma,\infty}_t:=(x^{\sigma(1)}_{0:t},x^{\sigma(2)}_{0:t},\dots,x^{\sigma(N)}_{0:t},x^{N+1}_{0:t},x^{N+2}_{0:t},\dots)$, and $u^{\sigma,\infty}_t$ is defined analogously. The set of such policies is denoted $\Gamma_{EX}$, with Markovian and stationary subsets
    $\Gamma_{EX,M}$, $\Gamma_{EX,S}$, respectively.
\end{definition}
\begin{definition}[Infinite Symmetric Policy Classes]\label{def33}
    A randomized joint policy $\gamma\in\Gamma$ is said to be \textit{conditionally independent symmetric} (or \textit{symmetric with common randomness}) if for all $0\leq t\leq T-1$
    we have
    \begin{align*}
        \gamma_t(u^\infty_t\in\cdot|h^\infty_t)=\int_0^1\prod_{j=1}^\infty\widetilde{\gamma}_t(u^j_t\in\cdot|h^j_t,z)\eta(dz)
    \end{align*}
    where $\eta\in\mc{P}([0,1])$ is a \textit{common randomness} independent of any exogenous noise and $h^i_t:=(x^i_{0:t},\mu_{0:t})$ 
    is the decentralized history available to DM $i\in\mc{N}$ at time $t$, but now $\mu_t:=\mc{L}(x^i_t|z)\in\mc{P}(\mbb{X}|[0,1])$ is the conditional
    distribution of any DM given the common randomization. Just as for finite teams, the common stochastic kernel $\widetilde{\gamma}_t$ is such that
    \begin{align*}
        \widetilde{\gamma}_t\in\mc{P}\left(\mbb{U}\bigg|\left(\prod_{k=0}^t\mbb{X}\times\mc{P}(\mbb{X})\right)\times[0,1]\right).
    \end{align*}
    The set of such policies is denoted $\Gamma_{CO,\mathit{SYM}}$. If instead we had
    \begin{align*}
        \gamma_t(u^\infty_t\in\cdot|h^\infty_t)=\prod_{j=1}^\infty\widetilde{\gamma}_t(u^j_t\in\cdot|h^j_t)
    \end{align*}
    then $\gamma$ is called \textit{independent symmetric} (or \textit{symmetric with private randomness}). The set of such policies shall be denoted with $\Gamma_{PR,\mathit{SYM}}$.
\end{definition}
For either the finite or infinite team problem, a randomized policy (along with
an initial distribution on the states) induces a probability measure on the
space of possible state-action trajectories $(x_{0:T},\mu_{0:T},u_{0:T-1})$
(where $\mu_t\in\mc{P}^N(\mbb{X})$ in the finite case) known as a
\textit{strategic measure} \cite{Yüksel_Saldi_2017}. Equipping the space of such
measures with the topology of weak convergence, \cite{Sanjari_Saldi_Yüksel_2024}
exploits the convexity/compactness of stategic measures induced by
$\Gamma^N_{EX}$, $\Gamma_{EX}$ to demonstrate their optimality for exchangeable
team problems. To present these findings, we will next recapitulate the basic
notions of strategic measures induced by the above policy classes from
\cite[\S5]{Sanjari_Saldi_Yüksel_2024}.
\subsection{Strategic Measures for Finite and Infinite Exchangeable Teams}
For finite exchangeable teams with $\mc{N}=\{1,2,\dots,N\}$, denote the set of
possible trajectories up to time $0\leq t\leq T-1$ with
\begin{align}
    \mbb{T}^N_t:=\left(\prod_{k=0}^{t}\mbb{X}^N\times\mbb{U}^N\times\mc{P}^N(\mbb{X})\right),\quad\text{and}\quad\mbb{T}^N_T:=\left(\prod_{k=0}^{T-1}\mbb{X}^N\times\mbb{U}^N\times\mc{P}^N(\mbb{X})\right)\times(\mbb{X}^N\times\mc{P}^N(\mbb{X}))
\end{align}
and consider $\mc{P}(\mbb{T}^N_T)$ equipped with the topology of weak convergence.
Fixing $\gamma^N\in\Gamma^N$ and initial state uncertainty $x^i_0\sim\nu_0$ for
$i\in\mc{N}$, a strategic measure $P^N_{\gamma^N}\in\mc{P}(\mbb{T}^N_T)$ is induced by
$\gamma^N$ if for every $0\leq t\leq T$, $\widetilde{h}^N_{t-1}\in\mbb{T}^N_{t-1}$
and continuous, bounded functional
$g:\mbb{T}^N_{t-1}\times\mbb{X}^N\times\mc{P}(\mbb{X})\rightarrow\mbb{R}$ the following manipulations hold:
\begin{align*}
    &\int_{\mbb{T}^N_{t-1}\times\mbb{X}^N\times\mc{P}(\mbb{X})} g(\widetilde{h}^N_{t-1},x_t,\mu^N_t)P^N_{\gamma^N}(d\widetilde{h}^N_{t-1},dx_t,d\mu^N_t)\\
    &=\int_{\mbb{T}^N_{t-1}\times\mbb{X}^N\times\mc{P}(\mbb{X})}g(\widetilde{h}^N_{t-1},x_t,\mu^N_t)P(d\mu^N_t|x_t,\widetilde{h}^N_t)P^N_{\gamma^N}(d\widetilde{h}^N_t,dx_t) \\
    &=\int_{\mbb{T}^N_{t-1}\times\mbb{X}^N\times\mc{P}(\mbb{X})}g(\widetilde{h}^N_{t-1},x_t,\mu^N_t)P(d\mu^N_t|x_t,\widetilde{h}^N_t)P(dx_t|\widetilde{h}^N_t)P^N_{\gamma^N}(d\widetilde{h}^N_{t-1})\\
    &=\int_{\mbb{T}^N_{t-1}}P^N_{\gamma^N}(d\widetilde{h}^N_{t-1})\int_{\mbb{X}^N}g(\widetilde{h}^N_{t-1},x_t,\mu^N_t)P(d\mu^N_t|x_t)\prod_{i=1}^N\mc{T}(dx^i_t|x^i_{t-1},u^i_{t-1},\mu^N_{t-1})
\end{align*}
where the final step follows since $\mu^N_t$ is determined by $x_t$ and the dynamics are symmetric with the shared state transition kernel $\mc{T}$ (\ref{eq80}). Denote the set of such measures with
$L^N$. We proceed by characterizing subsets of $L^N$ which correspond to exchangeable and symmetric policies.
\begin{lemma}\label{lem3}
Let $\gamma\in\Gamma^N$, and consider the induced strategic measure $P^N_{\gamma}\in L^N$. Then $P^N_\gamma\in L^N_{EX}\subseteq L^N$, where $L^N_{EX}$ is defined
\begin{align}
    L^N_{EX}:=\left\{P^N_{\gamma}\in L^N:P^N_\gamma((x_{0:T},u_{0:T-1},\mu^N_{0:t})\in\cdot)=P^N_\gamma((x^\sigma_{0:T},u_{0:T-1}^\sigma,\mu^N_{0:T})\in\cdot),\;\forall\sigma\in\mc{S}(\mc{N})\right\}
\end{align} \
if and only if $\gamma\in\Gamma^N_{EX}$.
\end{lemma}
\noindent\textbf{Proof.} We will prove that $\gamma\in\Gamma^N_{EX}$
$\Rightarrow$ $P^N_\gamma\in L^N_{EX}$. Indeed, assuming the team acts under a
policy $\gamma\in\Gamma^N_{EX}$, we denote
$P_k:=P((u_{0:k},x_{0:k},\mu^N_{0:k})\in\cdot)$ for $0\leq k\leq T$ and find
that for $1\leq t\leq T-1$
\begin{align}
    P_t&=P(u_t\in\cdot|u_{0:t-1},x_{0:t},\mu^N_{0:t})P(u_{0:t-1},x_{0:t},\mu^N_{0:t})\notag\\
    &=P(u^\sigma_t\in\cdot|u^\sigma_{0:t-1},x^\sigma_{0:t},\mu^N_{0:t})P((x_t,\mu^N_t)\in\cdot|u_{0:t-1},x_{0:t-1},\mu^N_{t-1})P_{t-1}\\
    &=P(u^\sigma_t\in\cdot|u^\sigma_{0:t-1},x^\sigma_{0:t},\mu^N_{0:t})\left[P(\mu^N_t\in\cdot|x_t)\prod_{i=1}^N\mc{T}(x^i_t\in\cdot|u_{0:t-1},x_{0:t-1},\mu^N_{0:t-1})\right]P_{t-1}\label{eq85}\\
    &=P(u^\sigma_t\in\cdot|u^\sigma_{0:t-1},x^\sigma_{0:t},\mu^N_{0:t})\left[P(\mu^N_t\in\cdot|x^\sigma_t)\prod_{i=1}^N\mc{T}(x^{\sigma(i)}_t\in\cdot|u^\sigma_{0:t-1},x^\sigma_{0:t-1},\mu^N_{0:t-1})\right]P_{t-1}\label{eq86}\\
    &=P(u^\sigma_t\in\cdot|u^\sigma_{0:t-1},x^\sigma_{0:t},\mu^N_{0:t})P((x^\sigma_t,\mu^N_t)\in\cdot|u^\sigma_{0:t-1},x^\sigma_{0:t-1},\mu^N_{t-1})P_{t-1}\label{eq87}
\end{align}
where (\ref{eq85}) once again follows from the symmetric dynamics, (\ref{eq86})
by simply rearranging the product (and using the fact that $x^i_{t-1}$,
$u^i_{t-1}$ and $\mu^N_{t-1}$ determine $x^i_t$), and (\ref{eq87}) by undoing
the manipulations used to obtain (\ref{eq85}). Thus, showing that
$(x_{0:t-1},u_{0:t-1},\mu^N_{0:t-1})$ is exchangeable would yield the result,
since
\begin{align*}
    P((x_{0:T},u_{0:T-1},\mu^N_{0:T})\in\cdot)&=P((x_T,\mu^N_T)\in\cdot|x_{0:T-1},u_{0:T-1},\mu^N_{0:T-1})P_{T-1}\\
    &=P((x^\sigma_T,\mu^N_T)\in\cdot|x^\sigma_{0:T-1},u_{0:T-1}^\sigma,\mu^N_{0:T-1})P_{T-1}.
\end{align*}
By applying the backward recursion (\ref{eq87}) to $P_{t-1}$, the result follows
the fact that $(x_0,\mu^N_0)$ is exchangeable.\hfill{$\qed$}\\[5pt]
\indent Thus, both $L^N$ and $L^N_{EX}$ correspond to policies utilizing
possibly centralized information. Less formally,
\cite{Sanjari_Saldi_Yüksel_2024} defines strategic measures corresponding to
$\Gamma^N_{CO,\mathit{SYM}}$ as
\begin{align*}
    L^N_{CO,\mathit{SYM}}:=\left\{P^N_\gamma\in L^N:P^N_\gamma((x_{0:T},u_{0:T-1},\mu^N_{0:T})\in\cdot)=\int_{(\cdot)\times[0,1]^T}\prod_{i=1}^N\widetilde{P}^N_\gamma(dx^i_{0:T},du^i_{0:T-1},d\mu^N_{0:T}|z)\eta(dz)\right\}
\end{align*}
where $\eta\in\mc{P}([0,1]^T)$ is the joint (across time) distribution on a
common policy randomization mechanism (see (\ref{eq76})) and
$\widetilde{P}^N_\gamma\in\mc{P}(\mbb{T}_{T-1}\times\mbb{X}\times\mc{P}^N(\mbb{X})|[0,1]^T)$
is a strategic measure on unit $i\in\mc{N}$, with
$\mbb{T}_t:=\prod_{k=0}^t(\mbb{X}\times\mbb{U}\times\mc{P}^N(\mbb{X}))$ for $1\leq
t\leq T-1$. That is, for every $1\leq t\leq T$ and $g:\mbb{T}_{t-1}\times\mbb{X}\times\mc{P}(\mbb{X})\rightarrow\mbb{R}$ continuous and bounded, we have
\begin{align}
    \int_{\mbb{T}_{t}\times\mbb{X}\times\mc{P}(\mbb{X})}g(h^i_{t})\widetilde{P}^N_\gamma(dh^i_{t}|z)
    &=\int_{\mbb{T}_{t-1}}\widetilde{P}^N_\gamma(d\widetilde{h}^i_{t-1})\int_\mbb{X}g(h^i_{t})P(dx^i_t,\mu^N_t|x^i_{t-1},u^i_{t-1},\mu^N_{t-1},z)\label{eq88}
\end{align}
where $\widetilde{h}^i_{t-1}\in\mbb{T}_{t-1}$ is the local trajectory up to time $t-1$. Note that in (\ref{eq88}), $\mu^N_t$ (which is determined by the global state $x_t$) can be inferred by DM $i$ under decentralized information by
invoking lemma \ref{lem2}. Similarly, strategic measures corresponding to $\Gamma^N_{PR,\mathit{SYM}}$ are defined
\begin{align*}
    L^N_{PR,\mathit{SYM}}:=\left\{P^N_\gamma\in L^N:P^N_\gamma((x_{0:T},u_{0:T-1},\mu^N_{0:T})\in\cdot)=\int_{(\cdot)}\prod_{i=1}^N\widetilde{P}^N_\gamma(dx^i_{0:T},dx^i_{0:T-1},d\mu^N_{0:T})\right\}
\end{align*}
for some local strategic measure $\widetilde{P}^N_\gamma\in\mc{P}^N(\mbb{T}_{T-1}\times\mbb{X}\times\mc{P}^N(\mbb{X}))$ common to each DM. Thus, (\ref{eq88}) holds for $\widetilde{P}^N_\gamma$ in this context as well,
albeit without conditioning on any common randomness.\\[5pt]
\indent For completeness, we remark that with any strategic measure $P^N_\gamma\in L^N$ (associated to policy $\gamma\in\Gamma^N$) the finite horizon cost may be rewritten
\begin{align}
    J^N_T(\gamma)=\widetilde{J}^N_T(P^N_\gamma):=\int\frac{1}{N}\sum_{i=1}^N\sum_{t=0}^{T-1}\beta^tc(x^i_t,u^i_t,\mu^N_t) P^N_\gamma(dx_{0:T},du_{0:T-1},d\mu^N_{0:T}).\label{eq91}
\end{align} 
\indent Let us now state how these sets of strategic measures can be extended to infinite exchangeable teams \cite{Sanjari_Saldi_Yüksel_2024}. For $0\leq t\leq T-1$, define the trajectory variables
\begin{align}
    \mbb{T}^\infty_t:=\left(\prod_{k=0}^t(\mbb{X}\times\mbb{U})^\infty\times\mc{P}(\mbb{X})\right),\quad\text{and}\quad \mbb{T}^\infty_T:=\mbb{T}^\infty_{T-1}\times(\mbb{X}^\infty\times\mc{P}(\mbb{X}))
\end{align}
and denote with $L$ the set of strategic measures $P_\gamma\in\mc{P}(\mbb{T}_T)$. The set of exchangeable strategic measures corresponding to policies in $\Gamma_{EX}$ is defined
\begin{align}
    L_{EX}:=\{P_\gamma\in\mc{P}(\mbb{T}_T):P_\gamma((x^\infty_{0:T},u^\infty_{0:T-1},\mu_{0:T})\in\cdot)=P_\gamma&((x^{\sigma,\infty}_{0:T},u^{\sigma,\infty}_{0:T-1},\mu_{0:T})\in\cdot),\notag\\
    &\forall\sigma\in\mc{S}(\{1,\dots,N\}),\;N\geq 1\}
\end{align}
and those corresponding to $\Gamma_{CO,\mathit{SYM}}$ by
\begin{align}
    L_{CO,SYM}:=\left\{P_\gamma\in\mc{P}(\mbb{T}_T):P_\gamma((x^\infty_{0:T},u^\infty_{0:T},\mu_{0:T})\in\cdot)=\int_{(\cdot)\times[0,1]^T}\prod_{i=1}^\infty\widetilde{P}_\gamma(dx^i_{0:T},du^i_{0:T-1},d\mu_{0:T}|z)\eta(dz)\right\}.
\end{align}
\indent The fact that these sets do indeed correspond to their respective policy classes ($\Gamma_{EX}$ and $\Gamma_{CO,\mathit{SYM}}$, respectively) follows from the Ionescu-Tulcea extension theorem (see \ref{thm6}) \cite[p.508]{Saldi_Yüksel_2022}.
Recall that under policies in $\Gamma_{CO,\mathit{SYM}}$, $\mu_t=\mc{L}(x^i_t|z)$ is the conditional law on any local state given the common noise for all $t$ due to policy symmetry, and $\mu_t$ can propagate locally with mean-field sharing (lemma \ref{lem2}). It is not clear at the outset if the same can be said for policies in $\Gamma_{EX}$.
For policies in this larger class, $\mu_{0:T}$ is a weaker object called the \textit{directing measure} of the system \cite{Sanjari_Saldi_Yüksel_2024}. Definitionally, we say $\mu_{0:T}$ is a directing measure for $x^\infty_{0:T}$ if for any $A=A^1\times A^2\times\cdots$ with each $A^i\in\mc{B}(\mbb{X})$, the marginal probability
on $x^\infty_t$ is given by
\begin{align}
    P(x^\infty_t\in A)=\int_{\mc{P}(\mbb{X})}\prod_{i=1}^\infty\xi(A^i)P(\mu_t\in d\xi)\label{eq89}
\end{align}
which follows from the infinite exchangeability of the state process under exchangeable policies \cite[definition 2.6]{Aldous_Ibragimov_1985}. In particular,
this implies that local states in $x^\infty_t$ are conditionally $\iid$ given $\mu_t$ \cite[lemma 2.7]{Aldous_Ibragimov_1985} and policies are allowed to depend
on the full centralized information process. It is perhaps surprising then, that $L_{EX}=L_{CO,\mathit{SYM}}$, but not so for the finite case.
\begin{theorem}[{\cite[lemma 2]{Sanjari_Saldi_Yüksel_2024}}]\label{thm7}
    $L_{CO,\mathit{SYM}}=L_{EX}$, but $L_{CO,\mathit{SYM}}\subsetneq L_{EX}$.
\end{theorem}
By theorem \ref{thm7}, the directing measure $\mu_{0:T}$ resulting from an infinitely exchangeable policy $\gamma\in\Gamma_{EX}$ propagates under decentralized information, and $\mu_t=\mc{L}(x^i_t|z)$ for any $i\in\mc{N}$. The proof can be found
in \cite[theorem 3.2]{Sanjari_Saldi_Yüksel_2024}, but follows directly from a de Finetti-type theorem (found in \cite[theorem 1.1]{Kallenberg_2005}, for example) stating that infinite exchangeability is equivalent
to conditional independence -- granting some measurability precondition on the space of admissable policies.\\[5pt]
\indent The final set of strategic measures is that corresponding to policies in $\Gamma_{PR,\mathit{SYM}}$, which is given by
\begin{align}
    L_{PR,\mathit{SYM}}:=\left\{P_\gamma\in L:P((x^\infty_{0:T},u^\infty_{0:T-1},\mu_{0:T})\in\cdot)=\int_{(\cdot)}\prod_{i=1}^\infty\widetilde{P}_\gamma(dx^i_{0:T},du^i_{0:T-1},\mu_{0:T})\right\}.
\end{align}
we close out this section with a technical lemma \cite[lemma 3]{Sanjari_Saldi_Yüksel_2024}, which is critical for proving the existence of optimal policies within these subclasses of strategic measures.
\begin{lemma}[{\cite[lemma 3]{Sanjari_Saldi_Yüksel_2024}}]\label{lem4}
    If assumptions (i) and (iii) hold from $\S$\ref{subsec1}, then $L_{EX}$ and $L^N_{EX}$ paired with the topology of weak convergence are both convex and compact.
\end{lemma}
While we shall only sketch the proof of this result, the supporting ideas are sufficiently extensible to deserve some consideration. To wit, the compactness result in lemma \ref{lem4} follows from Prokhorov's theorem
\cite[1.12]{Prokhorov_1956},\cite[theorem 9.3.3]{Dudley_2002} and another characterizing the limit of a sequence of directing measures for exchangeable distributions \cite[proposition 7.20]{Aldous_Ibragimov_1985} neither of which are proven here.
\begin{definition}[Tightness]\label{def34}
    A family of probability measures $\mc{P}$ on a measurable Polish space $(S,\mc{B}(S))$ is called \textit{tight} if for any $\varepsilon>0$, there exists a compact set $K_\varepsilon\in\mc{B}(S)$ such that
    $P(K_\varepsilon)>1-\varepsilon$ for all $P\in\mc{P}$.
\end{definition}
Provided the subjacent Polish space $S$ is compact, any sequence $(P_n)_{n\geq 1}\subseteq\mc{P}(S)$ is tight since one can take $S$ itself to satisfy definition \ref{def34}. 
Further, the fundamental result of Prokhorov \cite[theorem 1.12]{Prokhorov_1956} states that tightness is necessary and sufficient for the concerning sequence to admit a weakly convergent subsequence. 
\begin{theorem}[Prokhorov's Theorem {\cite[theorem 1.12]{Prokhorov_1956}}]\label{thm8}
    Let $S$ be Polish, and consider $\mc{P}=(P_n)_{n\geq 1}\subseteq\mc{P}(S)$. Then $\mc{P}$ is tight if and only if $\exists\,(P_{n_k})_{k\geq 1}\subset\mc{P}(S)$ so that $P_{n_k}\rightarrow P\in\mc{P}(S)$ weakly as $k\rightarrow\infty$.
\end{theorem}
\begin{theorem}[{\cite[proposition 7.20]{Aldous_Ibragimov_1985}}]\label{thm9}
    Let $S$ be Polish, and $X=(X_n)_{n\geq 1}$ an infinitely exchangeable sequence of $S$-valued random variables, directed by a random measure $\nu\in\mc{P}(S)$ in the sense of (\ref{eq89}).  
    For another set of random variables $X^n$, suppose that either
    \begin{enumerate}[(i)]
        \item $X^n:=(X^n_i)_{i\geq 1}$ is infinitely exchangeable and directed by $\nu^n$
        \item $X^n:=(X^n_i)_{1\leq i\leq n}$ is exchangeable with $\mu[X^n]=:\nu^n$.
    \end{enumerate}
    Then, $\mc{L}(X^n)\rightarrow \mc{L}(X)$ weakly as $n\rightarrow\infty$ if and only if $\mc{L}(\nu^n)\rightarrow\mc{L}(\nu)$ weakly as $n\rightarrow\infty$.
\end{theorem}
Let us now sketch the proof that $L_{EX}$ is compact (as in lemma \ref{lem4}) following \cite[appendix D]{Sanjari_Saldi_Yüksel_2024}. Consider a sequence of strategic measures $(P_n)\subseteq L_{EX}$, and in particular
the marginals of these on $(x^{\infty,n}_{0:T},u^{\infty,n}_{0:T-1})$, call them $(P^\ast_n)_{n\geq 1}\subseteq\mc{P}((\prod_{t=0}^{T-1}(\mbb{X}\times\mbb{U})^\infty)\times\mbb{X}^\infty)$ which are infinitely exchangeable by virtue of $P_n\in L_{EX}$. 
Since $\mbb{X}$ and $\mbb{U}$ are compact, Tychonoff's theorem certifies the space of trajectories is too, so $(P^\ast_n)_{n\geq 1}$ is tight. By Prokhorov's theorem (theorem \ref{thm8}) we may extract a subsequence $(P^\ast_{n_k})_{k\geq 1}$ (with directing measures $\mu^{n_k}_{0:T}$) converging
weakly to the law of a trajectory $(x^\infty_{0:T},u^\infty_{0:T-1})$. In particular, every finite marginal converges, and so too do finite permutations:
\begin{align}
    \mc{L}(x^{0:N,n}_{0:T},u^{0:N,n}_{0:T-1})\underset{n\rightarrow\infty}{\longrightarrow}\mc{L}(x^{0:N}_{0:T},u^{0:N}_{0:T-1})\quad\Rightarrow\quad \mc{L}(x^{\sigma,0:N,n}_{0:T},u^{\sigma,0:N,n}_{0:T-1})\underset{n\rightarrow\infty}{\longrightarrow}\mc{L}(x^{\sigma,0:N}_{0:T},u^{\sigma,0:N}_{0:T-1})\label{eq90}
\end{align} 
for any $\sigma\in\mc{S}(\{1,2,\dots,N\})$, $N\geq 1$. By (\ref{eq90}), infinite exchangeability of the prelimit trajectories thus implies that the limiting distribution $\mc{L}(x^\infty_{0:T},u^\infty_{0:T-1})$ is too. Subseqently applying de Finetti's theorem \cite[theorem 3.1]{Aldous_Ibragimov_1985} followed by \cite[lemma 2.15]{Aldous_Ibragimov_1985}
(making reference to \cite[definition 2.6]{Aldous_Ibragimov_1985}) furnishes a directing measure for the limit trajectory $\mu_{0:T}$. Finally, theorem \ref{thm9} certifies that $\mc{L}(\mu^n_{0:T})\rightarrow\mc{L}(\mu_{0:T})$ as $n\rightarrow\infty$. Thus, we have extracted a convergent subsequence $(P_{n_k})_{k\geq 1}\subseteq L_{EX}$ with limit in $L_{EX}$, so
$L_{EX}$ is compact. An almost identical proof holds for $L^N_{EX}$, and convexity shall be left unshown (but see \cite[appendix D]{Sanjari_Saldi_Yüksel_2024}).
\subsection{Optimality of Decentralized Policies for Exchangeable Teams}
Building on the definitions and discussions from the preceding subsections, we now state some of the main results of \cite{Sanjari_Saldi_Yüksel_2024} to serve as inspiration for future development of neural teams. First is that for finite exchangeable team problems,
the search for optimal policies may be restricted to $\Gamma^N_{EX}$ without loss.
\begin{theorem}[{\cite[theorem 1]{Sanjari_Saldi_Yüksel_2024}}]\label{thm10}
    For a finite exchangeable team of size $N$ with finite time horizon $T$, we have
    \begin{align*}
        \inf_{P^N_\gamma\in L^N}\widetilde{J}^N_T(P^N_\gamma)=\inf_{P^N_\gamma\in L^N_{EX}}\widetilde{J}^N_T(P^N_\gamma).
    \end{align*}
    Further, under the base assumptions of section \ref{subsec1}, an infimizing policy $P^{N,\ast}_\gamma\in L^N_{EX}$ exists.
\end{theorem}
To sketch the proof, one first shows that for all $\sigma\in\mc{S}(\mc{N})$, $P^N_\gamma\in L^N$, $\widetilde{J}^N_T(P^N_\gamma)=\widetilde{J}^N_\gamma(P^{\sigma,N}_\gamma)$, where $P^{\sigma,N}_\gamma:=P^N_{\gamma^\sigma}$.
This holds due to the exchangeability of the stagewise costs (\ref{eq74}) and dynamical symmetry. Then, associate to $P^N_\gamma$ an exchangeable version $\widehat{P}^N_\gamma$ with policy $\widehat{\gamma}$ given by
\begin{align}
    \widehat{\gamma}^N_t(u_t\in\cdot|x_{0:t}):=\frac{1}{|\mc{S}(\mc{N})|}\sum_{\sigma\in\mc{S}(\mc{N})}\gamma^N_t(u^\sigma_t\in\cdot|x^\sigma_{0:t}),\quad 0\leq t\leq T-1.
\end{align}
It can be shown that the corresponding strategic measure $\widehat{P}^N_\gamma\in L^N_{EX}$ via convexity of $L^N$ \cite[theorem 2.4]{Yüksel_Saldi_2017}. Examining (\ref{eq91}), we see that
the cost function $\widetilde{J}^N_T$ is linear on $L^N$, and it follows that $\widehat{P}^N_\gamma$ achieves the same cost as $P^N_\gamma$, establishing the sufficiency of $L^N_{EX}$. By lemma \ref{lem4},
$L^N_{EX}$ is compact, so an infimizing policy exists if $\widetilde{J}^N_T$ is continuous on $L^N_{EX}$. But this is immediate from the definition of weak convergence since the cost is assumed bounded and continuous. Fixing $(P^N_{\gamma_n})_{n\geq 1}\subseteq L^N_{EX}$ such that $P^N_{\gamma_n}\rightarrow P^N_\gamma$ weakly as $n\rightarrow\infty$, we have
\begin{align*}
    \lim_{n\rightarrow\infty}\widetilde{J}^N_T(P^N_{\gamma_n})&=\lim_{n\rightarrow\infty}\int\frac{1}{N}\sum_{i=1}^N\sum_{t=0}^{T-1}\beta^tc(x^i_t,u^i_t,\mu^N_t)P^N_{\gamma_n}(dx_{0:T},du_{0:T-1},d\mu^N_{0:T})\\
    &=\int\frac{1}{N}\sum_{i=1}^N\sum_{t=0}^{T-1}\beta^tc(x^i_t,u^i_t,\mu^N_t)P^N_{\gamma}(dx_{0:T},du_{0:T-1},d\mu^N_{0:T})
\end{align*}
which is nothing more than $\widetilde{J}^N_T(P^N_\gamma)$. Thus, an infimizing policy of $\widetilde{J}^N_T$ exists in $L^N_{EX}$, which completes the proof.\\[5pt]
\indent It is possible to construct a measure-valued Markov decision process which is equivalent to the finite exchangeable team problem under centralized information (see \cite[\S3, theorem 3.3]{Bauerle_2023}).
Furthermore, dynamic programming recursions are well defined under our assumptions, and these lead to an optimal Markovian policy for the finite horizon problem \cite[theorem 3.5]{Bauerle_2023}. Using a similar idea as for the proof of theorem \ref{thm10},
the same recursions can be shown to yield an optimal strategic measure $P^{N,\ast}_\gamma$ induced by a now \textit{Markovian} exchangeable policy in $\Gamma^N_{EX,M}$ \cite[theorem 2]{Sanjari_Saldi_Yüksel_2024}. In this manner, it is possible
to approximate a solution to the infinite population problem which is symmetric and decentralized.
\begin{theorem}[{\cite[theorem 3]{Sanjari_Saldi_Yüksel_2024}}]\label{thm11}
    The sequence of optimal strategic measures $(P^{N,\ast}_\gamma)_{N\geq 1}\subseteq L^N_{EX}$ obtained via dynamic programming recursions subsequentially converges to a measure $P^\ast_\gamma\in L_{CO,\mathit{SYM}}$
    which is globally optimal for the infinite population team problem. Moreover, there exists another optimal strategic measure $\widetilde{P}^\ast_\gamma\in L_{PR,\mathit{SYM}}$ induced by a Markovian policy.
\end{theorem}
The proof of this fact can be found in \cite[appendix G]{Sanjari_Saldi_Yüksel_2024}. This is one of two potentially consequential results from \cite{Sanjari_Saldi_Yüksel_2024} with respect to neuroscience, stating that
decentralized policies with mean field sharing are optimal for infinite networks. This is not all theorem \ref{thm11} says, but it is already remarkable to think that such minimal information suffices. Of course, brains are not infinitely large, but it turns out
that very large networks utilizing decentralized information can achieve \textit{near-optimality}.
\begin{theorem}[{\cite[theorem 4]{Sanjari_Saldi_Yüksel_2024}}]\label{thm12}
    Let $P^\ast_\gamma\in L_{PR,\mathit{SYM}}$ be a symmetric and independent optimal solution for the infinite exchangeable team, and denote $P^{N,\ast}_\gamma$ the restriction of $P^\ast_\gamma$ to the first $N$ agents. Then $P^{N,\ast}_\gamma\in L^N_{PR,\mathit{SYM}}$, and there exists a nonnegative sequence $(\varepsilon_N)_{N\geq 1}$ with $\varepsilon_N\rightarrow 0$ as $N\rightarrow\infty$ so that
    \begin{align*}
        \widetilde{J}^N_{T}(P^{N,\ast}_\gamma)\leq \inf_{P^N_\gamma\in L^N_{EX}}\widetilde{J}^N_T(P^N_{\gamma})+\varepsilon_N,\quad N\geq 1.
    \end{align*}
\end{theorem}
\subsection{Summary and Remarks}
\indent This section has largely been a compilation of definitions, remarks and results from \cite{Sanjari_Saldi_Yüksel_2024}, at times infused with some of my own additional commentary for future reference. This coverage culminated in theorems \ref{thm11} and \ref{thm12},
which establish (near) optimality of decentralized, symmetric policies for (finite) infinite populations. Theorem \ref{thm12} is particularly consequential, implying that policies in $\Gamma^N_{PR,\mathit{SYM}}$ can perform arbitrarily well, provided the team is sufficiently large (without any sense of just how large for a desired level of performance).
In light of the principle hypothesis of this report, this is precisely the type of result one would like to demonstrate. It would suggest that the computational burden at each cell decreases as the population enlarges, and would further predict that local function (which is physically instantiated via cellular structure) should homogenize in large teams of neurons.
For instance, the cerebellum is comprised of more neurons than the rest of the brain, yet its cellular circuitry is remarkable homogeneous across much of its volume \cite{D’Angelo_Mazzarello_Prestori_Mapelli_Solinas_Lombardo_Cesana_Gandolfi_Congi_2011}. Similarly, the modular structure of cerebral cortex \cite{Mountcastle_1997}
might reflect a similar computational principle, albeit with homogeneity at the level of the individual modules rather than the neurons themselves.\\[5pt]
\indent The neural team model presented in chapter 3 of this report was designed with this end in mind,
supposing that neurocomputations can be formulated as exchangeable costs, and that networks broadcast a voltage mean-field to their constituents. However, as discussed in $\S$\ref{subsec1}, dynamical discontinuity and heterogeneity
are consequences of spike-coding and weight control -- both of which I have argued are essential constraints of neurocomputation. A major goal of my subsequent doctoral research will be to adopt a graphon-theoretic approach and pursue similar structural/optimality results for neural team problems as in \cite{Sanjari_Saldi_Yüksel_2024}. To conclude this chapter, I will sketch the idea for this program.\\[5pt]
\indent Following the example of \cite{Jabin_Zhou_2023},\cite{Jabin_Schmutz_Zhou_2024},
it is possible to obtain mean-field dynamics for non-exchangeable, spiking networks. Biologically, these can be interpretted as a cortical module, incorporating hundreds or thousands of units \cite{Mountcastle_1997}. Regarding each of these \textit{modules} as a DM in its own right,
we can then consider a network comprised of such modules, for which the dynamics could be sufficiently regular to regard this "network of networks" itself as something like an exchangeable team -- albeit now with a controlled graphon network topology \cite{Caines_Huang_2021}.
\chapter{Concluding Remarks}
The purpose of this report has been to organize a solid foundation for a
stochastic team-based approach to theoretical neuroscience. In particular, the
first-order problems for this project were to (i) motivate the need/utility of
such a program in the first place, and (ii) set about proposing a formal model.
We have addressed both of these, with the latter being split into two
subobjectives: obtain an uncontrolled network model, and then reinterpret it as
a stochastic team. For the first of these, and by arguing for the essentiality of
spike-based computations, we have arrived at a jump-diffusion formulation (in
both continuous and discrete time) which is not new, but is newly shown to unify
a broad scope of traditional modeling approaches. While this result does not
grant us confidence as to the biological validity of the model \textit{per se}
(although we do argue for this throughout chapter 2) it does provide some
assurance that any subsequent theoretical contributions will be compatible with
much of the existing computational neuroscience literature. At the same time, our model
can be more fluidly adjusted as we acquire further insights from existing work.\\[5pt]
\indent In our neural stochastic team formulation, we interpret the network topology
as the controlled variable, where each node regulates the weight of its locally incoming connections
under a decentralized, mean-field sharing information structure. Alternatively, and in view of our thinning formulation for defining the spike
process (along with the sole dependence of the cost function on the state evolution through the spike times) we may also
view the (multivariate) network stochastic intensity process as controlled, again under a decentralized mean-field sharing information structure.
That is, neural control has a dual role; simultaneously shaping the network structure and the statistics of the output (spiking) process.
On the other hand, the class of cost functions (spike dependent, exchangeable costs) we have proposed are without a clear biological (i.e., neurocomputational) interpretation. 
Instead, this choice was motivated by a set of recent results demonstrating that for exchangeable teams, symmetric, decentralized policies can approximate
an optimal one.\\[5pt]
\indent Demonstrating that a similar result holds for neuronal network models could be highly consequential. To wit, various brain structures exhibit structural motifs,
repetition, structural homogeneity or modules at some spatial resolution. For instance, cerebral cortex is composed as a tiling of columns (or modules) which exhibit a largely consistent cellular composition throughout much of its volume. 
In light of the dual role of controlled synapses noted above, policy symmetry could correspond to such regularities of brain structure, and at the same time relate it to a governing control objective. Then, by comparing optimal model structure
with biology, one could view the stagewise, exchangeable cost as an independent variable for probing network function. Decentralization, which we consider a facet of biological realism,
could then lend credence to any subsequent predictions. This vision will guide future research, and has motivated the contents/discussion of chapter 4.\\[5pt]
\indent Through our modeling efforts, we have gained some sense of the mathematical specifications that emerge from neuroscientific applications (e.g. point measure dependent dynamics, non-exchangeability). With the above vision in mind, these generate a set of open problems
whence we set about extending existing results/frameworks to suit the peculiarities of neural stochastic teams. Perhaps the most prominent are on the dynamics/structure of the large agent limit, which we outline with three questions:
\begin{enumerate}
    \item Can we apply a similar program as in \cite{Jabin_Zhou_2023}, \cite{Jabin_Schmutz_Zhou_2024} when the network topologies are controlled?
    \item With (1) in the affirmative, are the results of \cite{Sanjari_Saldi_Yüksel_2024} compatible with the graphon limit for non-exchangeable teams?
    \item Alternatively, can we construct a larger \textit{team-of-teams} with symmetric team-level dynamics, but non-exchangeable dynamics within each subteam for which some decentralization/symmetry results hold?
\end{enumerate}

\indent Let us conclude with some brief remarks and conjectures on these problems. First, we conjecture that the answer to (1) is affirmative, by simply viewing the finite population control as graphon-valued and considering the limit at each timestep.
On the other hand, we suspect that the answer to (2) is negative, since (as noted at the end of $\S$\ref{subsec1}) the dynamics of our neural stochastic team are inherently not symmetric, and the resulting non-exchangeability appears to be intractable. However, we suspect the answer to (3) is positive,
since exchangeability can be restored for the collection of subteams (pending the choice of an exchangeable cost function). Further, this could be interpretable as a model of cerebral cortex, where subteams represent columns, and the complete population as
a sheet of cortex. In this way, the transition from (2) to (3) may be regarded as a sort of spatial rescaling to a level where we expect the symmetry of optimal/near-optimal policies to hold. Models of this kind and their infinite population limits are an active area of research in the domain of \textit{graphon mean field games} \cite{Caines_Huang_2021}. As a purely mathematical contribution, we will thus need to consider how one might reconcile the spatially-structured
mean field dynamics of \cite{Jabin_Zhou_2023} and/or \cite{Jabin_Schmutz_Zhou_2024} with the graphon mean field game approach.
% \indent To elaborate on this network of networks idea, we shall briefly review the setup for graphon mean field games found in \cite[\S2]{Caines_Huang_2021}. Let $\mc{K}:=\{1,2,\dots,K\}$ for $K\geq 1$ and consider a finite graph $\mc{G}_K$, with nodes $\mc{K}$ and weights $w^K_{ij}$ for $i,j\in\mc{K}$.
% Each vertex will be occupied by a module -- a finite (but in principle large) population of controlled SgIF units, as were studied in \cite{Jabin_Zhou_2023}. Denote these modules with $\mc{M}_i$, the set of nodes from $\mc{N}$ occupying node $i$ for $i\in\mc{K}$, and let $M:\mc{N}\rightarrow\mc{K}$ return the module to which any unit belongs. In the limit where each module consists of infinitely many units,
% the state at each node is specified by a spatially extended measure $\mu^i_t(\cdot):[0,1]\rightarrow\mc{P}(\mbb{X})$ (and possibly a firing rate $r_t:[0,1]\rightarrow\mbb{R}_+$), by following a similar program as to find (\ref{eq92}). Then the typical agent in a module at another node $j$ is coupled to the set of modules through a function, possibly of the form
% \begin{align}
%     \frac{1}{K}\sum_{i=1}^Kw^k_{M(j)M(i)}\int g(x^j_t,u^j_t,x)\mu^i_t(dx,\xi)d\xi
% \end{align}
% which is inspired by \cite[equation 3.6]{Caines_Huang_2021}. Viewing the weights 







% Making the rather remote assumption
% that the neural team formulation could be made to satisfy the assumptions in $\S$\ref{subsec1}, theorem \ref{thm11} would certify that neurons need only attend to their local voltage, synaptic configuration, and the voltage mean-field
% to behave optimally. Furthermore, it says that neurons need not coordinate amongst one another, and that they respond identically given a fixed local information. Such a set of structural predictions could be of tremendous theoretical value,
% especially if the cost is truly representative of a putative computation. Of course, this is unlikely 




\appendix
\chapter{Basic Notions of Decentralized Stochastic Control}
\section{Markov Decision Processes}\label{app1}
Markov decision processes provide a well-studied and flexible platform for
representing dynamic decision-making processes. Over some time horizon, a
controller (viewed as an agent, or decision maker ($\DM$)) generates a sequence of decisions
while striving to minimize a cost objective. In general, the objective depends
on these decisions made, as well as a stochastic state process whose evolution is also influenced by the actions.
An MDP is summarized by a five-tuple
\begin{align}
    (\mbb{X},\mbb{U},\{\mbb{U}(x):x\in\mbb{X}\},\mc{T},c).\label{eq9}
\end{align}
Sets $\mbb{X}$ and $\mbb{U}$, called the state and action spaces
respectively, are taken to be Borel subsets of complete, separable metric
spaces. Each $\mbb{U}(x)\subseteq\mbb{U}$ is a nonempty, measurable
collection of admissable actions from the state $x$, and
$\mbb{D}:=\{(x,u)\in\mbb{X}\times\mbb{U}:x\in\mbb{X},u\in\mbb{U}(x)\}$ is
the (measurable) set of admissable state-action pairs. The transition kernel
$\mc{T}:\mbb{X}\times\mbb{U}\rightarrow\mc{P}(\mbb{X})$ encodes the state
dynamics so that for $x_t\in\mbb{X}$, $u_t\in\mbb{U}$ at some time $t\geq 0$
with $A\in\mc{B}(\mbb{X})$, $\mc{T}(A|\cdot,\cdot)$ is a measurable function
on $\mbb{X}\times\mbb{U}$, $\mc{T}(\cdot|x_t,u_t)$ is a probability measure on $\mbb{X}$ and
\begin{align}
    \mc{T}(A|x_t,u_t):=P(x_{t+1}\in A|x_t,u_t)=P(x_{t+1}\in A|x_{0:t},u_{0:t})\label{eq65}
\end{align}
where $(x_{0:t},u_{0:t})=\{(x_s,u_s)\}_{s=0}^t$ denotes the process realization
up to time $t$. The second equality in (\ref{eq65}) says that $\mc{T}$ is
Markovian. Finally, the stagewise (or incremental) cost function
$c:\mbb{D}\rightarrow\mbb{R}_+$ penalizes the $\DM$ according to its present
state and the most recent action taken.\\[5pt]
\indent Rather than the probabilistic characterization (\ref{eq65}), in most
situations it is preferable to work with a state space model for the dynamics:
\begin{align}
    x_{t+1}=f(x_t,u_t,z_t),\quad x_0\sim \nu_0\in\mc{P}(\mbb{X})\label{eq66}
\end{align}
where now $(z_t)_{t\geq 0}$ is some $\mc{Z}$-valued $\iid$ noise process defined on a probability space
$(\Omega,\mc{F},P)$ with $z_0\sim\nu$ and
$f:\mbb{X}\times\mbb{U}\times\mc{Z}\rightarrow\mbb{X}$. Classical results in
stochastic realization theory establish the existence of a representation
(\ref{eq66}) for any stochastic process $\{(x_t,u_t)\}_{t\geq 0}$ defined
according to (\ref{eq65}) \cite{Borkar_1993}. An equivalence between the two then
follows the observation that for $A\in\mc{B}(\mbb{X})$, $t\geq 0$, the Markovian
kernel in (\ref{eq65}) (commensurate with dynamics defined by $f$ and $\nu$) may
be recovered
\begin{align}
    P(x_{t+1}\in A|x_{0:t},u_{0:t})=P(f(x_t,u_t,z_t)\in A|x_t,u_t)
    &=\int_\Omega\1{f(x_t,u_t,z_t(\omega))\in A}P(d\omega)\\
    &=\int_\mc{Z}\1{f(x_t,u_t,z)\in A}\nu(dz)\label{eq68}
\end{align}
whereby the first equality, we could take $\mc{T}(\cdot|x,u)=\E(\1{f(x,u,z_k)\in
\cdot})$, for example.\\[5pt]
\indent A policy (or strategy) is a sequence of measurable functions
$\gamma=(\gamma_t)_{t\geq 0}$ so that, for $t\geq 0$, $\gamma_t$ maps the
information available to the $\DM$ at time $t$ to an admissable action in
$\mbb{U}(x_t)$. The maximal domain of information upon which policies are
permitted to depend determines the so-called information structure of the
problem, a precise definition of which is suspended until the following section.
Information structures are a critical construct for representing the information
flow of a system and the bounded rationality of its agents. This latter idea, as
suggested by the preceding informal definition, is codified by the set of
admissable policies, with more granular classifications following further
constraint on their informational dependencies.\\[5pt]
\indent To introduce these classes, let $\mbb{H}_0:=\mbb{X}$, and for $t\geq 1$,
$\mbb{H}_t:= \mbb{D}^t\times\mbb{X}=\mbb{D}\times \mbb{H}_{t-1}$, the set of
admissable MDP trajectories up to time $t$ -- noting that these exclude the
action at time $t$. The following definition is adapted from \cite[definition 2.3.2]{Hernandez-Lerma_Lasserre_1996}.
\begin{definition}[Policy Classes]\label{def26} A {\it randomized policy} is a
    sequence $\gamma=\{\gamma_t\}_{t\geq 0}$ so that $\forall t\geq 0$,
    $\gamma_t\in\mc{P}(\mbb{U}|\mbb{H}_t)$ and $\forall h_t\in\mbb{H}_t$,
    \[\gamma_t(\mbb{U}(x_t)|h_t)=1\] with the set of such sequences is denoted
    by $\Gamma$. If to each $\gamma_t$ one can find a
    $\gamma^\prime_t\in\mc{P}(\mbb{U}|\mbb{X})$ satisfying
    $\gamma^\prime_t(\mbb{U}(x)|x)=1$ $\forall x\in\mbb{X}$ and
    \begin{align}
        \gamma_t(A|h_t)=\gamma^\prime_t(A|x_t)\label{eq7}
    \end{align}
    $\forall h_t\in\mbb{H}_t$, $A\in\mc{B}(\mbb{U})$, then $\gamma$ is called
    {\it randomized Markov}, the set of which is denoted $\Gamma_{RM}$. If
    instead one has $\gamma_t(A|h_t)=\gamma^\prime(A|x_t)$ for $t\geq 0$,
    $A\in\mc{B}(\mbb{X})$ then $\gamma$ is called {\it randomized stationary},
    the set of which are denoted $\Gamma_{RS}$.\\[5pt]
    Alternatively, if there exists a sequence $\{f_t\}_{t\geq 0}$ of measurable
    functions $f_t:\mbb{H}_t\rightarrow\mbb{U}$ such that $\forall
    h_t\in\mbb{H}_t$, $f_t(h_t)\in\mbb{U}(x_t)$ and
    \[\gamma_t(A|h_t)=\1{f_t(h_t)\in A}\] holds for each $A\in\mc{B}(\mbb{U})$,
    $t\geq 0$, then $\gamma$ is called {\it deterministic}, with the set of
    these denoted $\Gamma_D$. The same policy is called {\it deterministic
    Markov} (or just Markov) if $f_t(h_t)=f_t(x_t)$ $\forall h_t\in\mbb{H}_t$,
    and {\it deterministic stationary} (or just stationary) if it is further the
    case that $f_t\equiv f_{t+1}$, for $t\geq 0$. The sets of these policies are
    denoted $\Gamma_M$, and $\Gamma_S$, respectively. In any case, controls
    $\{u_t\}_{t\geq 0}$ are chosen with $u_t\sim\gamma_t(\cdot|h_t)$.
\end{definition}
\begin{remark}
    From definition (\ref{def26}), it is easy to notice a partial ordering on
    these sets:
    \begin{align}
        \Gamma_S\subset\Gamma_M\subset\Gamma_D, \quad\text{and}\quad \Gamma_{RS}\subset\Gamma_{RM}\subset\Gamma
    \end{align}
    and further, $\Gamma_S\subset\Gamma_{RS}$ by the assignment
    $\gamma(A|x_t)=\1{f(x_t)\in A}$. Thus, assuming that $\mbb{D}$ contains the
    graph of a measurable function is sufficient to garuntee that each policy
    class is nonempty. This is called the measurable selection hypothesis, and
    it is a technical necessity to verify conditions under which it holds for
    dynamic programming recursions to be well-defined.
\end{remark}
Before introducing cost functions, it is useful to recall the fundamental
Ionescu-Tulcea extension theorem \cite{Ionescu-Tulcea_1949}, a proof of which can be
found in \cite[proposition 7.28]{Bertsekas_Shreve_2007}.
\begin{theorem}[Ionescu-Tulcea Extension \cite{Ionescu-Tulcea_1949}]\label{thm6}
    Define $\overline{\mbb{H}}_\infty:=\prod_{t=0}^\infty(\mbb{X}\times\mbb{U})$
    the space of all state-action sequences which contains the set of admissable
    trajectories $\mbb{H}_\infty:=\prod_{t=0}^\infty\mbb{D}$. Fixing
    $\gamma\in\Gamma$ and $\nu_0\in\mc{P}(\mbb{X})$ so that $x_0\sim\nu_0$,
    there exists a unique probability measure
    $P^\gamma_{\nu_0}\in\mc{P}(\overline{\mbb{H}}_\infty)$ such that
    $P^\gamma_{\nu_0}(\mbb{H}_\infty)=1$ and for $A\in\mc{B}(\mbb{X})$,
    $B\in\mc{B}(\mbb{U})$ and $h_t\in\mbb{H}_t$, each of
    \begin{align*}
        P^\gamma_{\nu_0}(x_0\in A)=\nu_0(A),\quad P^\gamma_{\nu_0}(u_t\in B|h_t)=\gamma_t(B|h_t),\quad\text{and}\quad P^\gamma_{\nu_0}(x_{t+1}\in A|h_t,u_t)=\mc{T}(A|x_t,u_t)
    \end{align*}
    hold. That is, $P^\gamma_{\nu_0}$ defines a stochastic process
    $\{(x_t,u_t)\}_{t\geq 0}$ with marginals consistent with $\gamma$, $\nu_0$ and $\mc{T}$
    almost surely.    
\end{theorem}
\begin{definition}[Cost and Loss Functions]\label{def6} For the MDP 
(\ref{eq9}), a {\it cost function}
$J:\mc{P}(\mbb{X})\times\Gamma\rightarrow\mbb{R}_+$ is a functional which for
$\nu_0\in\mc{P}(\mbb{X})$, $\gamma\in\Gamma$ satisfies
    \begin{align*}
        J(\nu_0,\gamma)=\E^\gamma_{\nu_0}\left(L(x_{0:\infty},u_{0:\infty})\right)
    \end{align*}
    where $L:\overline{\mbb{H}}_\infty\rightarrow\mbb{R}_+$ is called a {\it
    loss function} and is defined in terms of the stagewise cost $c$.
\end{definition}
\begin{remark}
    The expectation operator $\E^\gamma_{\nu_0}$ it is taken with respect to the
    law $P^\gamma_{\nu_0}$, and is needed to render policy preferences
    deterministic. 
\end{remark}
In accordance with definition (\ref{def6}), the optimal control objective for the
MDP (\ref{eq9}) is to find $\gamma^\ast\in\Gamma$ such that
$\gamma^\ast=\inf_{\gamma\in\Gamma}J(\nu_0,\gamma)$, assuming such a policy
exists. Much of the notions and notations presented here remain valid in the stochastic team setting.
Exceptionally, there will be more than one policy at play, and each may depend on different information
and even influence the information available to other $\DM$s. 
\section{Stochastic Dynamic Teams}\label{app2}
Stochastic teams are a class of problems embedded in the more general setting of
multi-agent decision making. In this setting, one seeks to jointly optimize the
policies of possibly many (even infinitely many) $\DM$s with respect to some
objective. The fundamental difference between this formulation and the more
familiar (single-agent) Markov decision process (MDP) framework is the potential
for vastly more complex, {\it decentralized} information structures, where
$\DM$s formulate their actions using private information variables. In this section,
the general construction of multi-agent decision problems is introduced, along with
a compatible state space model and the particularities of stochastic teams.\\[5pt]
\indent All multi-agent decision problems share five essential components. These
are the decision makers (along with their actions), the problem uncertainty, the
information structure, a cost criterion and some objective with respect to it.
In particular, teams are considered a generalization of single agent MDPs only through
their information structure.
The decision makers ($\DM$s) are indexed by a set $\mc{N}=\{1,2\dots,N\}$, with
their set of actions $\mbb{U}^i$ for $i\in\mc{N}$. Let $\xi$ be a $\Xi$-valued
random variable, defined on a subjacent probability space
$(\Omega,\mc{F},P)$, which contains all intrinsically random variables. To
elaborate, all random variables in the problem will either be contained in
$\xi$, or else are a function of it. For the sequel, let $\mbb{T}$ be an at most countable
set of time points and define both
$u_t=(u^i_t)_{i\in\mc{N}}\in\mbb{U}:=\prod_{i\in\mc{N}}\mbb{U}^i$ and
$u_{0:t}=\{u_0,u_1,\dots u_t\}$. 
\begin{definition}[Static and Dynamic Information Structures]\label{def1}
    Suppose the information available to $\DM^i$ at $t\in\mbb{T}$ is given by
    $y^i_t=\eta^i_t(\xi,u_{0:t-1})$ for some
    $\eta^i_t:\Xi\times\mbb{U}^{t}\rightarrow \mbb{Y}^i$, where
    $(\mbb{Y}^i,\mc{Y}^i)$ is a measurable space. Then the collection
    $\eta=(\eta^i_t)_{i\in\mc{N},t\in\mbb{T}}$ is called a {\it dynamic
    information structure}. In contrast, if $y^i_t=\eta^i_t(\xi)$ for all
    $i\in\mbb{T}$, $i\in\mc{N}$, then $\eta$ is called a {\it static information
    structure}.
\end{definition}
Fixing an IS $\eta$ implicitly fixes the policy (or strategy) spaces $\Gamma^i$
for $i\in\mc{N}$. In particular, for $t\in\mbb{T}$, one can view $\sigma(y^i_t)\subseteq\mc{B}(\Xi)\otimes\bigotimes_{t\in\mbb{T}}\mc{B}(\mbb{U})$,
whence $\Gamma^i$ is comprised of the admissable policies $\gamma^i=(\gamma^i)_{t\in\mbb{T}}$
by only requiring $\gamma^i_t:\mbb{Y}^i\rightarrow\mbb{U}^i$ to be measurable for $i\in\mc{N}$,
$t\in\mbb{T}$. Thus, $\eta$ determines the set of admissable strategies $\DM$s
can adopt when selecting their actions. In applications, these ideas are more
concretely deployed for state space models of the form
\begin{align}
    x_{t+1}=f(x_t,u^1_t,u^2_t,\dots,u^N_t,w_t),\quad t\in\mbb{T}\label{eq1}
\end{align}
where $(w_t)_{t\in\mbb{T}}$ and $x_0$ are intrinsically random variables, and
$x_t=(x^i_t)_{i\in\mc{N}}\in\mbb{X}=\prod_{i\in\mc{N}}\mbb{X}^i$ forms a
controlled, stochastic state trajectory with dynamics $f=(f^i)_{i\in\mc{N}}$.
Observations are generated according to an analogous functional form:
\begin{align}
    y^i_t=g^i(x_t,u^1_{t-1},u^2_{t-1}\dots,u^N_{t-1},w^i_t),\quad i\in\mc{N},t\in\mbb{T}\label{eq2}
\end{align}
where $(w^i_t)_{t\in\mbb{T}}$ is a private observation noise for $i\in\mc{N}$.
By recursively substituting (\ref{eq1}) for the state in (\ref{eq2}), one can
eventually express $y^i_t$ as function directly of
$(x_0,w_{0:t-1},w^i_t,u_{0:t-1})$. Defining
$\xi:=(x_0,w_t,w^i_t)_{t\in\mbb{T},i\in\mc{N}}$, any subset $\mc{I}^i_t$ of the
observable history $\{y_{0:t},u_{0:t-1}\}$ used to decide $u^i_t$ can thus be
expressed as a function $\eta^i_t(\xi,u_{0:t-1})$, defining a dynamic IS as per
definition (\ref{def1}). Notably, the dynamic nature of $\eta$ in this model is
a consequence of allowing $\DM$s to influence the dynamics and/or observations
of one another.\\[5pt]
\indent A cost criterion is a deterministic, real functional on the joint
strategy space $\Gamma=\prod_{i\in\mc{N}}\Gamma^i$, and is used to induce a
preference ordering on the joint policies. Many formulations are possible, but a common
approach is to define a loss function
$L:\mbb{X}^{|\mbb{T}|+1}\times\mbb{U}\rightarrow\mbb{R}$, so that, assuming
$\mbb{T}=\{1,2,\dots,T\}$,
\begin{align*}
    L(x_{0:T},u_{0:T-1})=\sum_{t=0}^{T-1}c_t(x_t,u_t)+c_T(x_T)=\sum_{t=0}^{T-1}c_t(x_t,\gamma_t(\eta_t(\xi,u_{0:t-1})))+c_T(x_T)
\end{align*}
where $\gamma=(\gamma_t)_{t\in\mbb{T}}\in\Gamma$ and
$\eta_t=(\eta^i_t)_{i\in\mc{N}}$. For $t\in\mbb{T}$,
$c_t:\mbb{X}\times\mbb{U}\rightarrow\mbb{R}$ is called a stagewise cost, and
$c_T:\mbb{X}\rightarrow\mbb{R}$ a terminal cost. Infinite horizon loss functions
are defined similarly. For instance, with $\mbb{T}$ countable, taking
$c_t\equiv\beta^t c$ for some $c:\mbb{X}\times\mbb{U}\rightarrow\mbb{R}$ and
$\beta\in(0,1)$ produces the popular discounted horizon loss function. Using $L$, a cost
criterion (or cost function) $J$ renders policy preferences deterministic.
Using the same recursive substitution as above, one can
write $L(x_{0:T},u_{0:T-1})=\tilde{L}(\xi,\gamma(\eta(\xi)))$ for some
$\tilde{L}$. Then $J$ can be defined as an expected loss:
\begin{align}
    J(\gamma)=\E_\xi\left(\tilde{L}(\xi,\gamma(\eta(\xi)))\right)=\E^\gamma\left(L((x_t)_{t\in\mbb{T}},(u_t)_{t\in\mbb{T}})\right)
\end{align}
where $\E_\xi$ is with respect to the law of $\xi$ on $\Xi$ and
$\E^\gamma$ is with respect to a probability measure $P^\gamma$
induced on the space of sample paths
$\mbb{X}^{|\mbb{T}|}\times\mbb{U}^{|\mbb{T}|-1}$ by a controlled Markovian
transition kernel (the existence and properties of $P^\gamma$ are stated
in the classical Ionescu-Tulcea extension theorem \cite{Ionescu-Tulcea_1949} (see also
\cite[proposition 7.28]{Bertsekas_Shreve_2007}). Finally, various objectives can
be associated to $J$ to complete the decision problem.
\begin{definition}[Team Optimality and Nash Equilibria]\label{def2}
    Let $\{J,\Gamma^i,\,i\in\mc{N}\}$ be given. With $\gamma\in\Gamma$, let
    $\gamma^{-i}=(\gamma^j)_{j\neq i}$ for $i\in\mc{N}$. Then a policy
    $\gamma^\ast$ is a {\it team optimal solution} with respect to $J$ iff
    \begin{align*}
        J(\gamma^\ast)=\inf_{\gamma\in\Gamma}J(\gamma).
    \end{align*}
    Alternatively, if for $i\in\mc{N}$ and any $\gamma^i\in\Gamma^i$,
    $\gamma^\ast$ satisfies the set of $N$ inequalities
    \begin{align*}
        J(\gamma^\ast)\leq J(\gamma^{\ast,-i},\gamma^i)
    \end{align*}
    then $\gamma^\ast$ is called a {\it Nash equilibrium}. In this case, $\DM$s
    have no incentive to unilaterally deviate from $\gamma^\ast$. Clearly, any
    team optimal $\gamma^\ast$ is also a Nash equilibrium.
\end{definition}
\indent Making the other cost dependencies explicit as $J(\gamma,\eta,P_\xi, L)$,
one can distinguish two classes of multi-agent decision problem. A stochastic
{\it game} (static or dynamic) is one where a cost $J^i$ is associated to each
$\DM^i$, where $J^i(\gamma,\eta)=J(\gamma,\eta,P^i_\xi,L^i)$ such that $\DM$s
are allowed to maintain subjective beliefs on the uncertainty $\xi$ and incur
local costs. In this case, a game can be totally noncooperative (called
zero-sum) or can require agents to balance local and collective costs. These
problems have alternative solution concepts and can be much more complex, but
their generality may be useful for modeling some higher-level, cognitive
processes such as indecision and behavioral planning. These use-cases shall be
left to future work. A special class of much simpler multi-agent decision
problems are stochastic teams, which shall be mainstay of our initial modeling
efforts.
\begin{definition}[Stochastic Team Decision Problem]\label{def3} Let
    $\mathfrak{T}=\{J^i,\Gamma^i,\eta^i,\,i\in\mc{N}\}$ be given, and suppose
    that $J^i\equiv J$ for all $i\in\mc{N}$. Then $\mathfrak{T}$ is called a
    {\it stochastic team decision problem} where, in particular, all $\DM$s have
    the same cost function, probabilistic description of the uncertainty but may
    differ in their local information. 
\end{definition}


% \chapter{Decentralized Stochastic Control}
% The main objective of this chapter is to introduce the essential notions of
% decentralized stochastic control, and in particular stochastic teams. Following
% closely the works of Y\"uksel \& Ba\c sar (2024), Hern\'andez-Lerma \& Lasserre
% (1996) and the lecture notes by S. Y\"uksel (2024), I will first present
% single-agent Markov decision processes (MDPs) as a more familiar setting in
% which to introduce the basic model ingredients before expanding into
% multi-agent systems theory
% \section{Markov Decision Processes}
% Markov decision processes provide a well-studied and flexible platform for
% representing dynamic decision-making processes. Over some time horizon, a
% controller (viewed as an agent, or decision maker ($\DM$)) generates a sequence of decisions
% while striving to minimize a cost objective. In general, the objective depends
% on these decisions made, as well as a stochastic state process whose evolution is also influenced by the actions.
% An MDP is summarized by a five-tuple
% \begin{align}
%     (\mbb{X},\mbb{U},\{\mbb{U}(x):x\in\mbb{X}\},\mc{T},c).\label{eq9}
% \end{align}
% Sets $\mbb{X}$ and $\mbb{U}$, called the state and action spaces
% respectively, are taken to be Borel subsets of complete, separable metric
% spaces. Each $\mbb{U}(x)\subseteq\mbb{U}$ is a nonempty, measurable
% collection of admissable actions from the state $x$, and
% $\mbb{D}:=\{(x,u)\in\mbb{X}\times\mbb{U}:x\in\mbb{X},u\in\mbb{U}(x)\}$ is
% the (measurable) set of admissable state-action pairs. Tthe he transition kernel
% $\mc{T}:\mbb{X}\times\mbb{U}\rightarrow\mc{P}(\mbb{X})$ encodes the state
% dynamics so that for $x_t\in\mbb{X}$, $u_t\in\mbb{U}$ at some time $t\geq 0$
% with $A\in\mc{B}(\mbb{X})$, $\mc{T}(A|\cdot,\cdot)$ is a measurable function
% on $\mbb{X}\times\mbb{U}$, $\mc{T}(\cdot|x_t,u_t)$ is a probability measure on $\mbb{X}$ and
% \begin{align}
%     \mc{T}(A|x_t,u_t):=P(x_{t+1}\in A|x_t,u_t)=P(x_{t+1}\in A|x_{0:t},u_{0:t})\label{eq65}
% \end{align}
% where $(x_{0:t},u_{0:t})=\{(x_s,u_s)\}_{s=0}^t$ denotes the process realization
% up to time $t$. The second equality in (\ref{eq65}) says that $\mc{T}$ is
% Markovian. Finally, the stagewise (or incremental) cost function
% $c:\mbb{D}\rightarrow\mbb{R}_+$ penalizes the $\DM$ according to its present
% state and the most recent action taken.\\[5pt]
% \indent Rather than the probabilistic characterization (\ref{eq65}), in most
% situations it is preferable to work with a state space model for the dynamics:
% \begin{align}
%     x_{t+1}=f(x_t,u_t,z_t),\quad x_0\sim \nu_0\in\mc{P}(\mbb{X})\label{eq66}
% \end{align}
% where now $(z_t)_{t\geq 0}$ is some $\mc{Z}$-valued $\iid$ noise process defined on a probability space
% $(\Omega,\mc{F},P)$ with $z_0\sim\nu$ and
% $f:\mbb{X}\times\mbb{U}\times\mc{Z}\rightarrow\mbb{X}$. Classical results in
% stochastic realization theory establish the existence of a representation
% (\ref{eq66}) for any stochastic process $\{(x_t,u_t)\}_{t\geq 0}$ defined
% according to (\ref{eq65}) \cite{Borkar_1993}. An equivalence between the two then
% follows the observation that for $A\in\mc{B}(\mbb{X})$, $t\geq 0$, the Markovian
% kernel in (\ref{eq65}) (commensurate with dynamics defined by $f$ and $\nu$) may
% be recovered
% \begin{align*}
%     P(x_{t+1}\in A|x_{0:t},u_{0:t})=P(f(x_t,u_t,z_t)\in A|x_t,u_t)
%     &=\int_\Omega\1{f(x_t,u_t,z_t(\omega))\in A}P(d\omega)\\
%     &=\int_\mc{Z}\1{f(x_t,u_t,z)\in A}\nu(dz)
% \end{align*}
% whereby the first equality, we could take $\mc{T}(\cdot|x,u)=\E(\1{f(x,u,z_k)\in
% \cdot})$, for example.\\[5pt]
% \indent A policy (or strategy) is a sequence of measurable functions
% $\gamma=(\gamma_t)_{t\geq 0}$ so that, for $t\geq 0$, $\gamma_t$ maps the
% information available to the $\DM$ at time $t$ to an admissable action in
% $\mbb{U}(x_t)$. The maximal domain of information upon which policies are
% permitted to depend determines the so-called information structure of the
% problem, a precise definition of which is suspended until the following section.
% Information structures are a critical construct for representing the information
% flow of a system and the bounded rationality of its agents. This latter idea, as
% suggested by the preceding informal definition, is codified by the set of
% admissable policies, with more granular classifications following further
% constraint on their informational dependencies.\\[5pt]
% \indent To introduce these classes, let $\mbb{H}_0:=\mbb{X}$, and for $t\geq 1$,
% $\mbb{H}_t:= \mbb{D}^t\times\mbb{X}=\mbb{D}\times \mbb{H}_{t-1}$, the set of
% admissable MDP trajectories up to time $t$ -- noting that these exclude the
% action at time $t$. The following definition is adapted from \cite[definition 2.3.2]{Hernandez-Lerma_Lasserre_1996}.
% \begin{definition}[Policy Classes]\label{def26} A {\it randomized policy} is a
%     sequence $\gamma=\{\gamma_t\}_{t\geq 0}$ so that $\forall t\geq 0$,
%     $\gamma_t\in\mc{P}(\mbb{U}|\mbb{H}_t)$ and $\forall h_t\in\mbb{H}_t$,
%     \[\gamma_t(\mbb{U}(x_t)|h_t)=1\] with the set of such sequences is denoted
%     by $\Gamma$. If to each $\gamma_t$ one can find a
%     $\gamma^\prime_t\in\mc{P}(\mbb{U}|\mbb{X})$ satisfying
%     $\gamma^\prime_t(\mbb{U}(x)|x)=1$ $\forall x\in\mbb{X}$ and
%     \begin{align}
%         \gamma_t(A|h_t)=\gamma^\prime_t(A|x_t)\label{eq7}
%     \end{align}
%     $\forall h_t\in\mbb{H}_t$, $A\in\mc{B}(\mbb{U})$, then $\gamma$ is called
%     {\it randomized Markov}, the set of which is denoted $\Gamma_{RM}$. If
%     instead one has $\gamma_t(A|h_t)=\gamma^\prime(A|x_t)$ for $t\geq 0$,
%     $A\in\mc{B}(\mbb{X})$ then $\gamma$ is called {\it randomized stationary},
%     the set of which are denoted $\Gamma_{RS}$.\\[5pt]
%     Alternatively, if there exists a sequence $\{f_t\}_{t\geq 0}$ of measurable
%     functions $f_t:\mbb{H}_t\rightarrow\mbb{U}$ such that $\forall
%     h_t\in\mbb{H}_t$, $f_t(h_t)\in\mbb{U}(x_t)$ and
%     \[\gamma_t(A|h_t)=\1{f_t(h_t)\in A}\] holds for each $A\in\mc{B}(\mbb{U})$,
%     $t\geq 0$, then $\gamma$ is called {\it deterministic}, with the set of
%     these denoted $\Gamma_D$. The same policy is called {\it deterministic
%     Markov} (or just Markov) if $f_t(h_t)=f_t(x_t)$ $\forall h_t\in\mbb{H}_t$,
%     and {\it deterministic stationary} (or just stationary) if it is further the
%     case that $f_t\equiv f_{t+1}$, for $t\geq 0$. The sets of these policies are
%     denoted $\Gamma_M$, and $\Gamma_S$, respectively. In any case, controls
%     $\{u_t\}_{t\geq 0}$ are chosen with $u_t\sim\gamma_t(\cdot|h_t)$.
% \end{definition}
% \begin{remark}
%     From definition (\ref{def26}), it is easy to notice a partial ordering on
%     these sets:
%     \begin{align}
%         \Gamma_S\subset\Gamma_M\subset\Gamma_D, \quad\text{and}\quad \Gamma_{RS}\subset\Gamma_{RM}\subset\Gamma
%     \end{align}
%     and further, $\Gamma_S\subset\Gamma_{RS}$ by the assignment
%     $\gamma(A|x_t)=\1{f(x_t)\in A}$. Thus, assuming that $\mbb{D}$ contains the
%     graph of a measurable function is sufficient to garuntee that each policy
%     class is nonempty. This is called the measurable selection hypothesis, and
%     it is a technical necessity to verify conditions under which it holds for
%     dynamic programming recursions to be well-defined.
% \end{remark}
% Before introducing cost functions, it is useful to recall the fundamental
% Ionescu-Tulcea extension theorem \cite{Ionescu-Tulcea_1949}, a proof of which can be
% found in \cite[proposition 7.28]{Bertsekas_Shreve_2007}.
% \begin{theorem}[Ionescu-Tulcea Extension \cite{Ionescu-Tulcea_1949}]
%     Define $\overline{\mbb{H}}_\infty:=\prod_{t=0}^\infty(\mbb{X}\times\mbb{U})$
%     the space of all state-action sequences which contains the set of admissable
%     trajectories $\mbb{H}_\infty:=\prod_{t=0}^\infty\mbb{D}$. Fixing
%     $\gamma\in\Gamma$ and $\nu_0\in\mc{P}(\mbb{X})$ so that $x_0\sim\nu_0$,
%     there exists a unique probability measure
%     $P^\gamma_{\nu_0}\in\mc{P}(\overline{\mbb{H}}_\infty)$ such that
%     $P^\gamma_{\nu_0}(\mbb{H}_\infty)=1$ and for $A\in\mc{B}(\mbb{X})$,
%     $B\in\mc{B}(\mbb{U})$ and $h_t\in\mbb{H}_t$, each of
%     \begin{align*}
%         P^\gamma_{\nu_0}(x_0\in A)=\nu_0(A),\quad P^\gamma_{\nu_0}(u_t\in B|h_t)=\gamma_t(B|h_t),\quad\text{and}\quad P^\gamma_{\nu_0}(x_{t+1}\in A|h_t,u_t)=\mc{T}(A|x_t,u_t)
%     \end{align*}
%     hold. That is, $P^\gamma_{\nu_0}$ defines a stochastic process
%     $\{(x_t,u_t)\}_{t\geq 0}$ with marginals consistent with $\gamma$, $\nu_0$ and $\mc{T}$
%     almost surely.    
% \end{theorem}
% \begin{definition}[Cost and Loss Functions]\label{def6} For the MDP 
% (\ref{eq9}), a {\it cost function}
% $J:\mc{P}(\mbb{X})\times\Gamma\rightarrow\mbb{R}_+$ is a functional which for
% $\nu_0\in\mc{P}(\mbb{X})$, $\gamma\in\Gamma$ satisfies
%     \begin{align*}
%         J(\nu_0,\gamma)=\E^\gamma_{\nu_0}\left(L(x_{0:\infty},u_{0:\infty})\right)
%     \end{align*}
%     where $L:\overline{\mbb{H}}_\infty\rightarrow\mbb{R}_+$ is called a {\it
%     loss function} and is defined in terms of the stagewise cost $c$.
% \end{definition}
% \begin{remark}
%     The expectation operator $\E^\gamma_{\nu_0}$ it is taken with respect to the
%     law $P^\gamma_{\nu_0}$, and is needed to render policy preferences
%     deterministic. 
% \end{remark}
% In accordance with definition (\ref{def6}), the optimal control objective for the
% MDP (\ref{eq9}) is to find $\gamma^\ast\in\Gamma$ such that
% $\gamma^\ast=\inf_{\gamma\in\Gamma}J(\nu_0,\gamma)$, assuming such a policy
% exists. Much of the notions and notations presented here remain valid in the stochastic team setting.
% Exceptionally, there will be more than one policy at play, and each may depend on different information
% and even influence the information available to other $\DM$s. 
% \section{Stochastic Dynamic Teams}
% Stochastic teams are a class of problems embedded in the more general setting of
% multi-agent decision making. In this setting, one seeks to jointly optimize the
% policies of possibly many (even infinitely many) $\DM$s with respect to some
% objective. The fundamental difference between this formulation and the more
% familiar (single-agent) Markov decision process (MDP) framework is the potential
% for vastly more complex, {\it decentralized} information structures, where
% $\DM$s formulate their actions using private information variables. In this section,
% the general construction of multi-agent decision problems is introduced, along with
% a compatible state space model and the particularities of stochastic teams.\\[5pt]
% \indent All multi-agent decision problems share five essential components. These
% are the decision makers (along with their actions), the problem uncertainty, the
% information structure, a cost criterion and some objective with respect to it.
% In particular, teams are considered a generalization of single agent MDPs only through
% their information structure.
% The decision makers ($\DM$s) are indexed by a set $\mc{N}=\{1,2\dots,N\}$, with
% their set of actions $\mbb{U}^i$ for $i\in\mc{N}$. Let $\xi$ be a $\Xi$-valued
% random variable, defined on a subjacent probability space
% $(\Omega,\mc{F},P)$, which contains all intrinsically random variables. To
% elaborate, all random variables in the problem will either be contained in
% $\xi$, or else are a function of it. For the sequel, let $\mbb{T}$ be an at most countable
% set of time points and define both
% $u_t=(u^i_t)_{i\in\mc{N}}\in\mbb{U}:=\prod_{i\in\mc{N}}\mbb{U}^i$ and
% $u_{0:t}=\{u_0,u_1,\dots u_t\}$. 
% \begin{definition}[Static and Dynamic Information Structures]\label{def1}
%     Suppose the information available to $\DM^i$ at $t\in\mbb{T}$ is given by
%     $y^i_t=\eta^i_t(\xi,u_{0:t-1})$ for some
%     $\eta^i_t:\Xi\times\mbb{U}^{t}\rightarrow \mbb{Y}^i$, where
%     $(\mbb{Y}^i,\mc{Y}^i)$ is a measurable space. Then the collection
%     $\eta=(\eta^i_t)_{i\in\mc{N},t\in\mbb{T}}$ is called a {\it dynamic
%     information structure}. In contrast, if $y^i_t=\eta^i_t(\xi)$ for all
%     $i\in\mbb{T}$, $i\in\mc{N}$, then $\eta$ is called a {\it static information
%     structure}.
% \end{definition}
% Fixing an IS $\eta$ implicitly fixes the policy (or strategy) spaces $\Gamma^i$
% for $i\in\mc{N}$. In particular, for $t\in\mbb{T}$, one can view $\sigma(y^i_t)\subseteq\mc{B}(\Xi)\otimes\bigotimes_{t\in\mbb{T}}\mc{B}(\mbb{U})$,
% whence $\Gamma^i$ is comprised of the admissable policies $\gamma^i=(\gamma^i)_{t\in\mbb{T}}$
% by only requiring $\gamma^i_t:\mbb{Y}^i\rightarrow\mbb{U}^i$ to be measurable for $i\in\mc{N}$,
% $t\in\mbb{T}$. Thus, $\eta$ determines the set of admissable strategies $\DM$s
% can adopt when selecting their actions. In applications, these ideas are more
% concretely deployed for state space models of the form
% \begin{align}
%     x_{t+1}=f(x_t,u^1_t,u^2_t,\dots,u^N_t,w_t),\quad t\in\mbb{T}\label{eq1}
% \end{align}
% where $(w_t)_{t\in\mbb{T}}$ and $x_0$ are intrinsically random variables, and
% $x_t=(x^i_t)_{i\in\mc{N}}\in\mbb{X}=\prod_{i\in\mc{N}}\mbb{X}^i$ forms a
% controlled, stochastic state trajectory with dynamics $f=(f^i)_{i\in\mc{N}}$.
% Observations are generated according to an analogous functional form:
% \begin{align}
%     y^i_t=g^i(x_t,u^1_{t-1},u^2_{t-1}\dots,u^N_{t-1},w^i_t),\quad i\in\mc{N},t\in\mbb{T}\label{eq2}
% \end{align}
% where $(w^i_t)_{t\in\mbb{T}}$ is a private observation noise for $i\in\mc{N}$.
% By recursively substituting (\ref{eq1}) for the state in (\ref{eq2}), one can
% eventually express $y^i_t$ as function directly of
% $(x_0,w_{0:t-1},w^i_t,u_{0:t-1})$. Defining
% $\xi:=(x_0,w_t,w^i_t)_{t\in\mbb{T},i\in\mc{N}}$, any subset $\mc{I}^i_t$ of the
% observable history $\{y_{0:t},u_{0:t-1}\}$ used to decide $u^i_t$ can thus be
% expressed as a function $\eta^i_t(\xi,u_{0:t-1})$, defining a dynamic IS as per
% definition (\ref{def1}). Notably, the dynamic nature of $\eta$ in this model is
% a consequence of allowing $\DM$s to influence the dynamics and/or observations
% of one another.\\[5pt]
% \indent A cost criterion is a deterministic, real functional on the joint
% strategy space $\Gamma=\prod_{i\in\mc{N}}\Gamma^i$, and is used to induce a
% preference ordering on the joint policies. Many formulations are possible, but a common
% approach is to define a loss function
% $L:\mbb{X}^{|\mbb{T}|+1}\times\mbb{U}\rightarrow\mbb{R}$, so that, assuming
% $\mbb{T}=\{1,2,\dots,T\}$,
% \begin{align*}
%     L(x_{0:T},u_{0:T-1})=\sum_{t=0}^{T-1}c_t(x_t,u_t)+c_T(x_T)=\sum_{t=0}^{T-1}c_t(x_t,\gamma_t(\eta_t(\xi,u_{0:t-1})))+c_T(x_T)
% \end{align*}
% where $\gamma=(\gamma_t)_{t\in\mbb{T}}\in\Gamma$ and
% $\eta_t=(\eta^i_t)_{i\in\mc{N}}$. For $t\in\mbb{T}$,
% $c_t:\mbb{X}\times\mbb{U}\rightarrow\mbb{R}$ is called a stagewise cost, and
% $c_T:\mbb{X}\rightarrow\mbb{R}$ a terminal cost. Infinite horizon loss functions
% are defined similarly. For instance, with $\mbb{T}$ countable, taking
% $c_t\equiv\beta^t c$ for some $c:\mbb{X}\times\mbb{U}\rightarrow\mbb{R}$ and
% $\beta\in(0,1)$ produces the popular discounted horizon loss function. Using $L$, a cost
% criterion (or cost function) $J$ renders policy preferences deterministic.
% Using the same recursive substitution as above, one can
% write $L(x_{0:T},u_{0:T-1})=\tilde{L}(\xi,\gamma(\eta(\xi)))$ for some
% $\tilde{L}$. Then $J$ can be defined as an expected loss:
% \begin{align}
%     J(\gamma)=\E_\xi\left(\tilde{L}(\xi,\gamma(\eta(\xi)))\right)=\E^\gamma\left(L((x_t)_{t\in\mbb{T}},(u_t)_{t\in\mbb{T}})\right)
% \end{align}
% where $\E_\xi$ is with respect to the law of $\xi$ on $\Xi$ and
% $\E^\gamma$ is with respect to a probability measure $P^\gamma$
% induced on the space of sample paths
% $\mbb{X}^{|\mbb{T}|}\times\mbb{U}^{|\mbb{T}|-1}$ by a controlled Markovian
% transition kernel (the existence and properties of $P^\gamma$ are stated
% in the classical Ionescu-Tulcea extension theorem \cite{Ionescu-Tulcea_1949} (see also
% \cite[proposition 7.28]{Bertsekas_Shreve_2007}). Finally, various objectives can
% be associated to $J$ to complete the decision problem.
% \begin{definition}[Team Optimality and Nash Equilibria]\label{def2}
%     Let $\{J,\Gamma^i,\,i\in\mc{N}\}$ be given. With $\gamma\in\Gamma$, let
%     $\gamma^{-i}=(\gamma^j)_{j\neq i}$ for $i\in\mc{N}$. Then a policy
%     $\gamma^\ast$ is a {\it team optimal solution} with respect to $J$ iff
%     \begin{align*}
%         J(\gamma^\ast)=\inf_{\gamma\in\Gamma}J(\gamma).
%     \end{align*}
%     Alternatively, if for $i\in\mc{N}$ and any $\gamma^i\in\Gamma^i$,
%     $\gamma^\ast$ satisfies the set of $N$ inequalities
%     \begin{align*}
%         J(\gamma^\ast)\leq J(\gamma^{\ast,-i},\gamma^i)
%     \end{align*}
%     then $\gamma^\ast$ is called a {\it Nash equilibrium}. In this case, $\DM$s
%     have no incentive to unilaterally deviate from $\gamma^\ast$. Clearly, any
%     team optimal $\gamma^\ast$ is also a Nash equilibrium.
% \end{definition}
% \indent Making the other cost dependencies explicit as $J(\gamma,\eta,P_\xi, L)$,
% one can distinguish two classes of multi-agent decision problem. A stochastic
% {\it game} (static or dynamic) is one where a cost $J^i$ is associated to each
% $\DM^i$, where $J^i(\gamma,\eta)=J(\gamma,\eta,P^i_\xi,L^i)$ such that $\DM$s
% are allowed to maintain subjective beliefs on the uncertainty $\xi$ and incur
% local costs. In this case, a game can be totally noncooperative (called
% zero-sum) or can require agents to balance local and collective costs. These
% problems have alternative solution concepts and can be much more complex, but
% their generality may be useful for modeling some higher-level, cognitive
% processes such as indecision and behavioral planning. These use-cases shall be
% left to future work. A special class of much simpler multi-agent decision
% problems are stochastic teams, which shall be mainstay of our initial modeling
% efforts.
% \begin{definition}[Stochastic Team Decision Problem]\label{def3} Let
%     $\mathfrak{T}=\{J^i,\Gamma^i,\eta^i,\,i\in\mc{N}\}$ be given, and suppose
%     that $J^i\equiv J$ for all $i\in\mc{N}$. Then $\mathfrak{T}$ is called a
%     {\it stochastic team decision problem} where, in particular, all $\DM$s have
%     the same cost function, probabilistic description of the uncertainty but may
%     differ in their local information. 
% \end{definition}
% Fully cooperative teams are appropriate to model neuronal networks under the
% assumption that every constituent contributes to the same computational purpose.
% However, the complexity of an IS, and in particular the kind encountered in
% neural systems, can pose a significant hurdle to analysis and obtaining
% solutions.
% \section{Witsenhausen's Intrinsic Model and Nonclassical Information}
% Wisenhausen \cite{Witsenhausen_1975}, \cite{Witsenhausen_1988} introduced a generic representation for multi-agent
% stochastic control problems, whereby a system of $N$ agents acting $T$ times
% each is transformed into a system of $N\times T$ agents, acting once. One of the main benefits of this representation is as a framework for characterizing
% information structures. According to definition (\ref{def3}), and the fact that
% $\Gamma$ depends on $\eta$, such a framework also classifies team problems at
% large. State space models like (\ref{eq1}, \ref{eq2}) described in the previous
% section are called {\it strictly causal}, since information depends only on
% actions taken at strictly preceding timesteps. More generally, one can consider
% {\it sequential} decision problems, where agents can act in a predefined
% (deterministic or random) order to avoid informational interdependence. Either
% condition is sufficient to ensure that the system is solvable, where
% $u^i_t=\gamma^i_t(y^i_t)$ does not (paradoxically) depend on any actions it
% might influence. All models here will be strictly causal, such that the order in
% which agents act at fixed $t\in\mbb{T}$ is arbitrary, and the team can be
% unravelled into $NT$ sequentially-acting $\DM$s.\\[5pt]
% \indent Consider such a strictly causal stochastic team of $N$ agents acting
% over time horizon $\mbb{T}$ of length $T\in\mbb{R}\cup\{\infty\}$. Then, let
% $\mc{N}=\{1,2,\dots,NT\}$ index a new set of $\DM$s, where each is associated
% with measurable observation $(\mbb{Y}^i,\mc{Y}^i)$ and action
% $(\mbb{U}^i,\mc{U}^i)$ spaces. The uncertainty is once again modeled with a
% probability space $(\Omega,\mc{F},P)$, and for each $i\in\mc{N}$, define the
% measurable spaces $(\mbb{H}^i,\mc{H}^i)$, $(\mbb{H},\mc{H})$ which encapsulate
% the partial and complete history (respectively) with
% $\mbb{H}^i:=\Omega\times\prod_{j=1}^{i}\mbb{U}^i$,
% $\mbb{H}:=\Omega\times\prod_{i\in\mc{N}}\mbb{U}^i$ and where $\mc{H}^i$,
% $\mc{H}$ are the corresponding Borel $\sigma$-fields. Then each $\DM$ is
% equipped with an information constraint, which is a measurable function
% $\eta^i:(\mbb{H}^{i-1},\mc{H}^{i-1})\rightarrow(\mbb{Y}^i,\mc{Y}^i)$, where
% $y^i=\eta^i(\omega,u^{1:i-1})$, and $u^{1:j}:=(u^k)_{1\leq k\leq j}$ for
% $j\in\mc{N}$. Thus, each $\eta^i$ induces a sub $\sigma$-field
% $\sigma(y^i)\subseteq\mc{H}^{i-1}$, which is the trace $\sigma$-algebra over the entire
% history $\mc{H}$. Finally, the set of policies $\Gamma$ is taken to be the set
% of all measurable functions $\gamma=(\gamma^i)_{i\in\mc{N}}$, where
% $\gamma^i:\mbb{Y}^i\rightarrow\mbb{U}^i$ for $i\in\mc{N}$. \\[5pt]
% \indent Just as in the previous section, the information structure
% $(\eta^i)_{i\in\mc{N}}$ is said to be dynamic when $\eta^i$ depends on an action
% $u^j$ with $j<i$ for some $i\in\mc{N}$, and is called static otherwise. This dependence
% can be indirect, which is perhaps most clearly characterized set-theoretically.
% For $i\in\mc{N}$, define
% \begin{align}
%     \mc{S}^i=\{A\in2^\mc{N}:\sigma(y^i)\subseteq\mc{F}\otimes\bigotimes_{j\in A}\mc{U}^j\otimes\bigotimes_{k\in\mc{N}\setminus A}\{\emptyset,\mbb{U}^k\}\}\label{eq5}
% \end{align}
% the set of subpopulations who influence the information available to $\DM^i$
% through their actions \cite{Barty_Chancelier_Cohen_Lara_Guilbaud_Carpentier_2006}. A subset $A$ is in $\mc{S}^i$ when
% some information on $u^k$ is needed to make $y^i$ measurable, for each $k\in A$.
% This set is closed under intersections \cite[proposition 3]{Barty_Chancelier_Cohen_Lara_Guilbaud_Carpentier_2006}, allowing one to
% define the precedence relation on $\mc{N}\times\mc{N}$:
% \begin{align}
%     j\rightsquigarrow i,\quad\longleftrightarrow\quad j\in\cap\mc{S}^i\label{eq4}
% \end{align}
% where $\cap\mc{S}^i$ is the smallest set of $\DM$s whose actions influence
% $y^i$. Equivalently, one can take a functional perspective and view
% $\cap\mc{S}^i$ as the smallest set of action indices appearing in the
% observation function $\eta^i_t$ (from the previous section) upon recursively
% eliminating any explicit state dependence. In either case, information
% structures can be classified by characterizing the set of such dependencies.
% \begin{definition}[Classification of Information Structures]\label{def4} An
%     information structure $(\eta^i)_{i\in\mc{N}}$ is 
%     \begin{enumerate}[(i)]
%         \item {\it Classical}, if for $i\in\mc{N}$,
%         $\sigma(y^j)\subseteq\sigma(y^i)$ whenever $j<i$.
%         \item {\it Quasi-classical}, if $j\rightsquigarrow i$ implies
%         $\sigma(y^j)\subseteq\sigma(y^i)$.
%         \item {\it Nonclassical}, if it is neither classical, nor
%         quasi-classical.
%     \end{enumerate}
% \end{definition}
% Since $\cap\mc{S}^i\subset\{1,2,\dots i-1\}$, every classical structure is
% quasi-classical. Additionally, definition \ref{def4} makes clear the sense in which team problems generalize MDPs: systems with classical information can be regarded as 
% a single agent decision problem by viewing the $NT$ agents of the intrinsic model as a single agent acting $NT$ times. However, nonclassical information is probably characteristic of neuronal
% networks since, under a nonclassical IS, the actions of $\DM$s have the
% additional role of transmitting information. That is, for quasi-classical
% structures, information fields are independent of strategies adopted by other agents \cite{Witsenhausen_1975}. The non-nestedness of information fields in nonclassical systems makes
% them exceedingly difficult to analyze and obtain solutions for. The only general
% approach for studying these is via static reduction (see \cite{Witsenhausen_1988}, \cite{Yuksel_2018}, \cite[ch.3]{Yuksel_Basar_2024}) whereby the dynamic problem is recast as a
% static one with independent measurements, while preserving the policies and the cost-induced preference on them.

% \section{Large Scale Decentralized Stochastic Control}


% \chapter{Mean Field MDPs and Non-Exchangeable Infinite Population Dynamics}
% The stochastic team formulation of the previous chapter inducts neuronal network models into the realm of multi-agent stochastic control. Owing to the large population, decentralized and nonclassical IS, and controlled Poissonian interactions,
% this class of models is rather idiosyncratic, and appears quite challenging to solve. 
\appendix
\bibliography{CBraun-MSc-report}
\bibliographystyle{abbrv}
\end{document}


% === Complete Description of Jump-Adapted JDSDE approximation ===== 
% \indent Jump-adapted discretization schemes were introduced by Platen (1982) and proceed as follows. Let $(\tau_k)_{k=1}^{\Pi_T((0,1))}$ be the set of $\as$ finitely-many jumps on $[0,T]$, and superimpose this
% with a deterministic, $\Delta$-equidistant grid to obtain the discretization $0=t_1<t_2<\cdots<t_M=T$. For the approximation $y^{i,\Delta}$, define
% $y^{i,\Delta}_{t_m-}=\lim_{s\nearrow t_m}y^{i,\Delta}_s$. Then the jump-adapted Euler scheme is given by the alternating updates
% \begin{align}
%     y^{i,\Delta}_{t_{m+1}-}&=y^{i,\Delta}_{t_m}+f^i(y^{i,\Delta}_{t_m})(t_{m+1}-t_m)+\sigma^i\sqrt{t_{m+1}-t_m}\xi_{t_m}\\
%     y^{i,\Delta}_{t_{m+1}}&=y^{i,\Delta}_{t_{m+1}-}+\int_0^1\widetilde{W}^i_{\mc{G}}(y,y^{i,\Delta}_{t_{m+1}-})\Pi(dy\times\{t_{m+1}\})
% \end{align}
% for $1\leq m<M$, and with $\xi_{t_m}$ $\iid$ standard Gaussian $\forall m$. Whenever a point $t_m$ is not a jump, $\int_0^1\widetilde{W}^i_\mc{G}(y,y^{i,\Delta}_{t_{m-}})\Pi(dy\times\{t_m\})=0$, and the process evolves as a pure diffusion
% under the usual Euler-Maruyama approximation. If instead $t_m$ is a jump, then $\int_0^1\widetilde{W}^i_\mc{G}(y,y^{i,\Delta}_{t_{m-}})\Pi(dy\times\{t_m\})=\widetilde{W}^i_\mc{G}(z_{t_m},y^{i,\Delta}_{t_m})$,
% and the mark $z_{t_m}$ determines if a spike is induced in any unit. Unsurprisingly, this scheme inherits the same uniform $\kappa=1/2$-strong convergence from the diffusive approximation.